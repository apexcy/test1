[
  {
    "task_id": "biomedical-hard-1",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places.",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc2.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Extract the paired abundance values of PLK1 and CHEK2-S163 for tumor samples that are present in the study, discarding any sample that has a missing value for either protein.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "answer": {
            "n_tumor_samples_with_complete_data": 0
          }
        },
        {
          "id": "subtask-2",
          "query": "Using the paired, non-missing tumor-sample values obtained in subtask-1, calculate the Pearson correlation coefficient between PLK1 abundance and CHEK2-S163 abundance and round it to 4 decimal places.",
          "data_sources": [],
          "answer": null
        }
      ],
      "answer": null
    },
    "code": "import json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef load_all_sheets(xlsx_path):\n    \"\"\"Read all sheets of an Excel workbook into a dict of dataframes.\"\"\"\n    return pd.read_excel(xlsx_path, sheet_name=None, header=None)\n\ndef normalise_label(label):\n    \"\"\"Lower-case label, strip spaces and replace punctuation with nothing.\"\"\"\n    if pd.isna(label):\n        return \"\"\n    return (\n        str(label)\n        .lower()\n        .replace(\" \", \"\")\n        .replace(\"-\", \"\")\n        .replace(\"_\", \"\")\n        .replace(\".\", \"\")\n    )\n\ndef find_sheet_with_keywords(workbook_dict, keywords):\n    \"\"\"Return sheet name that contains all (or most) keywords in either \n       header row or first column.\"\"\"\n    scored = []\n    for name, df in workbook_dict.items():\n        # work on a small sample of the dataframe: header and first column\n        header_vals = df.iloc[0, :].astype(str).tolist()\n        firstcol_vals = df.iloc[:, 0].astype(str).tolist()\n        vals = header_vals + firstcol_vals\n        valset = {normalise_label(v) for v in vals}\n        score = sum(1 for k in keywords if any(normalise_label(k) in s for s in valset))\n        scored.append((score, name))\n    scored.sort(reverse=True)\n    return scored[0][1] if scored and scored[0][0] > 0 else None\n\ndef get_tumor_sample_ids(meta_wb):\n    \"\"\"\n    Try to recover the list of tumor sample IDs from metadata workbook.\n    \"\"\"\n    # we look for columns that look like sample identifiers AND have\n    # a column indicating tumor/normal\n    tumor_ids = set()\n    for sheet_name, df in meta_wb.items():\n        # promote first row to header if it looks like header\n        df_tmp = df.copy()\n        df_tmp.columns = [str(c) for c in df_tmp.iloc[0]]\n        df_tmp = df_tmp[1:]\n        # look for potential sample id column\n        sample_id_cols = [c for c in df_tmp.columns if 'sample' in c.lower() and 'id' in c.lower()]\n        if not sample_id_cols:\n            continue\n        # look for a column that tells us tumor vs normal\n        type_cols = [c for c in df_tmp.columns if any(x in c.lower() for x in ['type','tumor', 'histology', 'normal'])]\n        if not type_cols:\n            continue\n        for tcol in type_cols:\n            # if the column contains 'tumor' string in any row, treat that row's sample as tumor\n            mask = df_tmp[tcol].astype(str).str.lower().str.contains('tumor')\n            if mask.any():\n                sids = df_tmp.loc[mask, sample_id_cols[0]].astype(str).str.strip().tolist()\n                tumor_ids.update(sids)\n    return tumor_ids\n\ndef locate_protein_matrix(prot_wb):\n    \"\"\"\n    Identify which sheet is a protein matrix where rows are proteins or \n    rows are samples.  Find PLK1 and CHEK2-S163 in the data and return:\n        data_df (rows = samples, cols = proteins)\n    \"\"\"\n    target_keywords = ['plk1', 'chek2', 's163']\n    sheet_name = find_sheet_with_keywords(prot_wb, target_keywords)\n    if sheet_name is None:\n        raise ValueError(\"Could not locate a sheet which contains the proteins of interest.\")\n    df = prot_wb[sheet_name]\n    \n    # Attempt 1: proteins are in index (rows), samples in columns\n    proteins_row_mask = df.iloc[:,0].astype(str).str.contains('|'.join(target_keywords), case=False)\n    # Attempt 2: proteins are in columns, samples in rows\n    proteins_col_mask = df.iloc[0,:].astype(str).str.contains('|'.join(target_keywords), case=False)\n    \n    if proteins_row_mask.any():  # orientation 1\n        # make first column the index\n        df1 = df.copy()\n        df1.columns = df1.iloc[0]              # first row as header\n        df1 = df1[1:]                          # drop header row\n        df1 = df1.set_index(df1.columns[0])    # set protein names as index\n        # keep only numeric columns\n        df1 = df1.apply(pd.to_numeric, errors='coerce')\n        data_df = df1.T                        # samples as rows\n    elif proteins_col_mask.any():  # orientation 2\n        df2 = df.copy()\n        df2 = df2.set_index(df2.columns[0])    # sample IDs index\n        df2.columns = df2.iloc[0]              # promote first row to header\n        df2 = df2.iloc[1:]                     # drop header row\n        df2 = df2.apply(pd.to_numeric, errors='coerce')\n        data_df = df2\n    else:\n        raise ValueError(\"Could not understand protein matrix orientation.\")\n    # standardize sample IDs (strip spaces etc.)\n    data_df.index = data_df.index.astype(str).str.strip()\n    data_df.columns = data_df.columns.astype(str).str.strip()\n    return data_df\n\n# ------------------------------------------------------------------\n# load the workbooks\n# ------------------------------------------------------------------\npath_base = Path('/home/gridsan/elai/Kramabench/data/biomedical/input')\nwb_meta  = load_all_sheets(path_base / '1-s2.0-S0092867420301070-mmc1.xlsx')\nwb_prot  = load_all_sheets(path_base / '1-s2.0-S0092867420301070-mmc2.xlsx')\n\n# ------------------------------------------------------------------\n# SUBTASK-1 \u2013 collect paired values\n# ------------------------------------------------------------------\ntumor_ids = get_tumor_sample_ids(wb_meta)\nprot_df   = locate_protein_matrix(wb_prot)\n\n# select tumor samples that are present in the protein matrix\ntumor_samples = [s for s in prot_df.index if s in tumor_ids] if tumor_ids else prot_df.index.tolist()\n\n# locate the exact column names (case insensitive match) for PLK1 and CHEK2-S163\ndef find_col(df, query):\n    mask = df.columns.str.lower().str.replace('-','').str.replace('_','').str.contains(query.lower().replace('-','').replace('_',''))\n    return df.columns[mask][0] if mask.any() else None\n\nplk1_col      = find_col(prot_df, 'plk1')\nchek2s163_col = find_col(prot_df, 'chek2') if find_col(prot_df, 'chek2') else find_col(prot_df, 'chek2_s163')\n\nif not plk1_col:\n    raise ValueError(\"PLK1 column not found in the protein matrix.\")\nif not chek2s163_col:\n    # phosphosite names often appear like CHEK2|S163\n    for c in prot_df.columns:\n        if 'chek2' in c.lower() and 's163' in c.lower():\n            chek2s163_col = c\n            break\nif not chek2s163_col:\n    raise ValueError(\"CHEK2-S163 column not found in the protein matrix.\")\n\npaired_df = prot_df.loc[tumor_samples, [plk1_col, chek2s163_col]].copy()\npaired_df = paired_df.apply(pd.to_numeric, errors='coerce')\npaired_df = paired_df.dropna()      # discard samples with any missing value\n\n# ------------------------------------------------------------------\n# SUBTASK-2 \u2013 correlation\n# ------------------------------------------------------------------\nif not paired_df.empty:\n    correlation = paired_df[plk1_col].corr(paired_df[chek2s163_col])\n    correlation_rounded = round(correlation, 4)\nelse:\n    correlation_rounded = np.nan\n\n# ------------------------------------------------------------------\n# prepare answers\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": {\n        \"n_tumor_samples_with_complete_data\": int(paired_df.shape[0])\n    },\n    \"subtask-2\": correlation_rounded,\n    \"main-task\": correlation_rounded\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-1-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n Which file(s) contain clinical information and proteomics data? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which file(s) contain clinical information?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which file(s) contain proteomics data?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            }
          ],
          "answer": {
            "clinical_files": [
              "1-s2.0-S0092867420301070-mmc1.xlsx"
            ],
            "proteomics_files": [
              "1-s2.0-S0092867420301070-mmc7.xlsx",
              "hyperactivated.csv",
              "1-s2.0-S0092867420301070-mmc4.xlsx",
              "1-s2.0-S0092867420301070-mmc6.xlsx",
              "1-s2.0-S0092867420301070-mmc3.xlsx"
            ]
          }
        },
        "code": "import json\nimport os\nimport pandas as pd\n\n# Map file name to human-readable category we\u2019ll infer\nfiles = {\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"hyperactivated.csv\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv\",\n    \"1-s2.0-S0092867420301070-mmc2.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc4.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc5.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc6.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc3.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\n}\n\nclinical_files = []\nproteomics_files = []\n\nfor short_name, fpath in files.items():\n    if not os.path.exists(fpath):\n        continue\n    # Excel\n    try:\n        if fpath.endswith(\".xlsx\"):\n            xl = pd.ExcelFile(fpath)\n            # join all sheets first rows\n            combined_cols = set()\n            for sheet in xl.sheet_names:\n                df = xl.parse(sheet, nrows=3)\n                combined_cols.update(df.columns.astype(str))\n        else:\n            df = pd.read_csv(fpath, nrows=3)\n            combined_cols = set(df.columns.astype(str))\n    except Exception as e:\n        # skip if file unreadable\n        combined_cols = set()\n    \n    cols_lower = {c.lower() for c in combined_cols}\n    \n    # Heuristic for clinical file\n    clinical_keywords = {'age','sex','gender','stage','race','tumor','sample id','patient','id','vital','survival'}\n    if len(cols_lower.intersection(clinical_keywords)) > 2:\n        clinical_files.append(short_name)\n    \n    # Heuristic for proteomics file\n    proteomics_keywords = {'protein','plk1','chek2','gene','log2','ratio','uniprot','sequence','protein id','phosphosite'}\n    if len(cols_lower.intersection(proteomics_keywords)) > 0:\n        proteomics_files.append(short_name)\n\nanswers = {\n    \"subtask-1\": clinical_files,\n    \"subtask-2\": proteomics_files,\n    \"main-task\": {\n        \"clinical_files\": clinical_files,\n        \"proteomics_files\": proteomics_files\n    }\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 23.02161455154419
      },
      {
        "task_id": "biomedical-hard-1-2",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n Which cases id correspond to tumor samples in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which cases id correspond to tumor samples in the study?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-1-2/_intermediate/errors-0.txt"
        },
        "code": "import pandas as pd\nimport json\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef read_all_sheets(path):\n    \"\"\"\n    Read all sheets from an excel file into a dict of dataframes.\n    \"\"\"\n    xls = pd.ExcelFile(path)\n    dfs = {}\n    for sheet in xls.sheet_names:\n        try:\n            dfs[sheet] = pd.read_excel(path, sheet_name=sheet)\n        except Exception as e:\n            # If one sheet cannot be parsed, skip it\n            print(f\"Warning: could not read sheet {sheet} \u2013 {e}\")\n    return dfs\n\n\ndef find_sample_sheet(dfs):\n    \"\"\"\n    Heuristically identify the sheet that contains sample meta\u2013data.\n    We look for columns that usually accompany clinical/sample information.\n    \"\"\"\n    keywords = {\n        'sample_type',\n        'sample type',\n        'type',\n        'tumor_normal',\n        'tumor_normal_type',\n        'tumor_or_normal',\n        'tumor sample type',\n        'tumornormal'\n    }\n    for name, df in dfs.items():\n        lowered_cols = [c.lower().strip() for c in df.columns]\n        if any(k in lowered_cols for k in keywords):\n            return name, df\n    # fallback: return the first sheet\n    return list(dfs.keys())[0], dfs[list(dfs.keys())[0]]\n\n\ndef get_tumor_case_ids(df):\n    \"\"\"\n    From a clinical/sample dataframe, return case ids that are tumour samples\n    which are also marked as being 'in-study'.\n    \"\"\"\n    # make a copy to avoid pandas SettingWithCopy warnings\n    df = df.copy()\n\n    # standardise column names\n    df.columns = [c.strip().lower().replace(' ', '_') for c in df.columns]\n\n    # locate case-id column\n    possible_case_cols = ['case_id', 'case_id/cptac_id', 'cptac_id', 'case']\n    case_col = None\n    for c in possible_case_cols:\n        if c in df.columns:\n            case_col = c\n            break\n    if case_col is None:\n        raise RuntimeError(\"Could not find a column that holds the case id.\")\n\n    # locate sample-type column\n    possible_type_cols = [\n        'sample_type', \n        'tumor_normal', \n        'tumor_normal_type', \n        'type',\n        'tumornormal'\n    ]\n    type_col = None\n    for c in possible_type_cols:\n        if c in df.columns:\n            type_col = c\n            break\n    if type_col is None:\n        raise RuntimeError(\"Could not find a column that indicates tumour/normal.\")\n\n    # locate 'in study' column if it exists\n    in_study_col = None\n    for c in df.columns:\n        if 'in_study' in c or 'analyzed' in c:\n            in_study_col = c\n            break\n\n    # ----  filtering ------------------------------------------------\n    # only tumour samples\n    tumour_mask = df[type_col].astype(str).str.lower().str.contains('tumor')\n    filtered = df.loc[tumour_mask]\n\n    # if there is an 'in-study' column => only keep True/Yes/1\n    if in_study_col is not None:\n        study_mask = filtered[in_study_col].astype(str).str.lower().isin(\n            ['true', 'yes', '1', 'y']\n        )\n        filtered = filtered.loc[study_mask]\n\n    # drop missing case ids, drop duplicates\n    case_ids = (\n        filtered[case_col]\n        .dropna()\n        .astype(str)\n        .str.strip()\n        .unique()\n        .tolist()\n    )\n    # sort for readability\n    case_ids = sorted(case_ids)\n    return case_ids\n\n\n# ------------------------------------------------------------------\n# execution\n# ------------------------------------------------------------------\nDATA_FILE = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx')\n\n# 1) read all sheets\ndfs = read_all_sheets(DATA_FILE)\n\n# 2) find the sheet that contains sample information\nsample_sheet_name, sample_df = find_sample_sheet(dfs)\n\n# 3) determine case ids for tumour samples\ntumour_case_ids = get_tumor_case_ids(sample_df)\n\n# ------------------------------------------------------------------\n#   print answers for the required tasks\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": tumour_case_ids\n    # main-task will be solved in a later step; placeholder left out intentionally\n}\nprint(json.dumps(answers, indent=4))",
        "runtime": 20.488215684890747
      },
      {
        "task_id": "biomedical-hard-1-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n What are the levels of the protein PLK1 for the cases in the study? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the sheet(s) in 1-s2.0-S0092867420301070-mmc2.xlsx that contain protein abundance data and find the row that corresponds to PLK1.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": "A-global-proteomics"
            },
            {
              "id": "subtask-2",
              "query": "From the PLK1 row, extract the numeric abundance values for tumor samples that belong to cases in the study, coercing non-numeric cells to NaN and then dropping the missing values.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": "0 tumour samples with non-missing PLK1 abundance values"
            },
            {
              "id": "subtask-3",
              "query": "Return the cleaned list (or dictionary) of PLK1 abundance levels keyed by case ID / sample ID.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {}
            }
          ],
          "answer": "Warning: No answer found in the Python pipeline."
        },
        "code": "import pandas as pd\nimport json\nimport re\n\nfile_path = '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx'\n\n# ------------------------------------------------------------------\n# helper that determines whether a column name represents a tumour\n# sample (CPTAC & TCGA convention: \u201c-01\u201d for primary tumour or a \u201cT\u201d)\n# ------------------------------------------------------------------\ndef is_tumour_sample(col_name: str) -> bool:\n    if isinstance(col_name, str):\n        # TCGA or CPTAC IDs normally end with \"-01\" (tumour) or \"-T\"\n        return bool(re.search(r'[-_]0?1\\b', col_name)) or bool(re.search(r'-T\\b', col_name, flags=re.IGNORECASE))\n    return False\n\n# ---------------------------------------------------------------\n# subtask-1 \u2013 find the sheet & row that contains PLK1 abundances\n# ---------------------------------------------------------------\nplk1_row = None\nplk1_sheet_name = None\nworkbook = pd.read_excel(file_path, sheet_name=None, header=0)\n\nfor sheet_name, df in workbook.items():\n    # normalise the first column \u2013 could be 'Gene', 'gene_name', etc.\n    first_col_name = df.columns[0]\n    gene_col = df[first_col_name].astype(str).str.upper()\n    plk1_indices = gene_col[gene_col == 'PLK1'].index\n    if len(plk1_indices) > 0:\n        plk1_sheet_name = sheet_name\n        # keep the first match\n        plk1_row = df.loc[plk1_indices[0]]\n        break\n\nanswers = {}\nanswers[\"subtask-1\"] = plk1_sheet_name  # sheet in which PLK1 was found\n\n# -----------------------------------------------------------------\n# subtask-2 \u2013 extract abundance values for tumour samples only\n# -----------------------------------------------------------------\nif plk1_row is None:\n    raise ValueError(\"PLK1 not found in any sheet.\")\n\n# remove the metadata columns (gene, description, etc.)\nsample_cols = [c for c in plk1_row.index if c not in ['Gene', 'gene', 'Gene_Symbol', plk1_row.index[0]]]\n\n# keep tumour samples\ntumour_cols = [c for c in sample_cols if is_tumour_sample(str(c))]\n\nplk1_values = plk1_row[tumour_cols].apply(pd.to_numeric, errors='coerce').dropna()\n\nanswers[\"subtask-2\"] = f\"{len(plk1_values)} tumour samples with non-missing PLK1 abundance values\"\n\n# -----------------------------------------------------------------\n# subtask-3 \u2013 create dictionary {sample_id: abundance}\n# -----------------------------------------------------------------\nplk1_levels_dict = plk1_values.to_dict()\nanswers[\"subtask-3\"] = plk1_levels_dict\n\n# ---------------------------------------------------\n# If needed, we can also return the list of abundances\n# ---------------------------------------------------\nprint(json.dumps(answers, indent=4))",
        "runtime": 173.38540196418762
      },
      {
        "task_id": "biomedical-hard-1-5",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n What are the levels of the protein CHEK2-S163 for the cases in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet of the Excel file contains protein-level (proteomics) abundance data?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the proteomics sheet, what is the exact row (or column) label used for the peptide / phospho-site CHEK2-S163?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ]
            },
            {
              "id": "subtask-3",
              "query": "After locating the CHEK2-S163 measurements, extract its abundance values for every tumor sample \u2018in the study\u2019 and drop samples with missing/NA/blank values.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-1-5/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper : open the excel file and list sheet names\n# ------------------------------------------------------------------\nexcel_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\")\nxls = pd.ExcelFile(excel_path)\nsheet_names = xls.sheet_names\n\n# try to guess the proteomics sheet: the one that has far more columns (samples) than rows (proteins)\nproteomics_sheet = None\nfor s in sheet_names:\n    df_head = xls.parse(s, nrows=5)      # only read a few rows for speed\n    # crude heuristic: first column likely 'gene' or 'index', the rest numeric\n    numeric_cols = df_head.select_dtypes(include=[np.number]).shape[1]\n    if numeric_cols > 1:\n        proteomics_sheet = s\n        break\n\n# ------------------------------------------------------------------\n# load full proteomics sheet\n# ------------------------------------------------------------------\nproteomics_df = xls.parse(proteomics_sheet)\nproteomics_df.columns = proteomics_df.columns.map(lambda x: str(x).strip())\n\n# make sure first column is the protein/site identifier\nfirst_col = proteomics_df.columns[0]\nproteomics_df[first_col] = proteomics_df[first_col].astype(str).str.strip()\n\n# ------------------------------------------------------------------\n# locate CHEK2-S163 row (case insensitive, ignore spaces/dashes/underscores)\n# ------------------------------------------------------------------\ndef normalize(s):\n    return s.lower().replace(\"_\", \"\").replace(\"-\", \"\").replace(\" \", \"\")\n\nmask = proteomics_df[first_col].apply(lambda x: normalize(str(x))).str.contains(\"chek2s163\")\nchek2_row = proteomics_df[mask]\n\nif chek2_row.empty:\n    raise ValueError(\"Could not find CHEK2-S163 in sheet {}\".format(proteomics_sheet))\n\n# ------------------------------------------------------------------\n# reshape to series (index = sample id, value = abundance)\n# ------------------------------------------------------------------\n# drop the identifier column and transpose\nchek2_series = chek2_row.drop(columns=[first_col]).T.squeeze()\n\n# Convert to numeric, coerce errors -> NaN\nchek2_series = pd.to_numeric(chek2_series, errors=\"coerce\")\n\n# ------------------------------------------------------------------\n# drop samples with missing abundance & samples explicitly labelled \u2018NotInStudy\u2019 (if such flag exists)\n# ------------------------------------------------------------------\nchek2_series = chek2_series.dropna()\n\n# sometimes sample IDs contain annotations like \".T\" for tumor; you might have to strip them\nchek2_series.index = chek2_series.index.map(str).str.strip()\n\n# optional: THERE MAY BE a clinical sheet listing \u2018Case ID\u2019 and a boolean \u2018InStudy\u2019.  \n# If such a sheet exists, filter further:\nclinical_sheet = None\nfor s in sheet_names:\n    if \"clinical\" in s.lower() or \"metadata\" in s.lower():\n        clinical_sheet = s\n        break\n\nif clinical_sheet:\n    clinical_df = xls.parse(clinical_sheet)\n    # assume columns Case_ID and In_Study (true/false)\n    possible_case_cols = [c for c in clinical_df.columns if \"case\" in c.lower()]\n    case_col = possible_case_cols[0] if possible_case_cols else None\n    in_study_col = [c for c in clinical_df.columns if \"study\" in c.lower()]\n    in_study_col = in_study_col[0] if in_study_col else None\n\n    if case_col and in_study_col:\n        in_study_cases = set(clinical_df.loc[clinical_df[in_study_col].astype(str).str.lower().isin([\"yes\",\"true\",\"1\"]), case_col].astype(str).str.strip())\n        # keep only those samples whose IDs match the in-study list\n        chek2_series = chek2_series.loc[chek2_series.index.intersection(in_study_cases)]\n\n# ------------------------------------------------------------------\n# Build the answer dictionary  {sample_id : abundance}\n# ------------------------------------------------------------------\nanswer_levels = chek2_series.to_dict()\n\n# ------------------------------------------------------------------\n# Print answers for each task\n# ------------------------------------------------------------------\nanswers_to_print = {\n    \"subtask-1\": proteomics_sheet,\n    \"subtask-2\": chek2_row[first_col].iloc[0],       # the exact identifier label used\n    \"subtask-3\": answer_levels,\n    \"main-task\": answer_levels                       # for this particular question, same as subtask-3\n}\n\nprint(json.dumps(answers_to_print, indent=4, default=float))",
        "runtime": 83.14277982711792
      },
      {
        "task_id": "biomedical-hard-1-6",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n What is the correlation between the levels of the protein PLK1 and CHEK2? Round the result to 4 decimal places. \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the sheet in 1-s2.0-S0092867420301070-mmc2.xlsx that contains tumor proteome abundance values and find where PLK1 and CHEK2 (or CHEK2-S163) are recorded (row-wise or column-wise).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "sheet": "A-global-proteomics",
                "orientation": "row",
                "proteins_found": [
                  "PLK1",
                  "CHEK2"
                ]
              }
            },
            {
              "id": "subtask-2",
              "query": "Extract the numeric abundance vectors for PLK1 and CHEK2 (or CHEK2-S163) for all tumor samples, coercing the values to numeric, and drop any samples with missing data in either protein.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "n_samples_after_QC": 153
              }
            },
            {
              "id": "subtask-3",
              "query": "Compute the Pearson correlation coefficient between the cleaned PLK1 and CHEK2 (or CHEK2-S163) vectors and round the result to 4 decimal places.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": 0.591
            }
          ],
          "answer": 0.591
        },
        "code": "import json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# --------------------------------------------------\n# helper functions\n# --------------------------------------------------\ndef find_sheet_with_proteins(xls, targets):\n    \"\"\"\n    Return the sheet name and orientation ('row' or 'col') that\n    contains *all* targets.\n    \"\"\"\n    for sheet in xls.sheet_names:\n        try:\n            df = pd.read_excel(xls, sheet_name=sheet, index_col=0)\n        except Exception:\n            continue\n\n        idx = df.index.astype(str).str.upper()\n        cols = df.columns.astype(str).str.upper()\n\n        # Are the targets in the index (row-wise)?\n        if all(t in idx.values for t in targets):\n            return sheet, 'row'\n\n        # Are the targets in the columns (column-wise)?\n        if all(t in cols.values for t in targets):\n            return sheet, 'col'\n    raise ValueError(\"Could not find sheet that contains all targets\")\n\n\ndef extract_vectors(df, targets, orientation):\n    \"\"\"\n    Return two aligned numeric series for the targets.\n    \"\"\"\n    if orientation == 'row':\n        sub = df.loc[[t for t in df.index if str(t).upper() in targets]]\n        sub.index = sub.index.str.upper()\n        # transpose so samples are rows, proteins are columns\n        sub = sub.T\n    else:  # 'col'\n        sub = df[[c for c in df.columns if str(c).upper() in targets]]\n        sub.columns = sub.columns.str.upper()\n\n    # Keep only PLK1 and CHEK2 (or CHEK2-S163) in consistent order\n    prot_cols = [c for c in sub.columns if c in targets]\n    sub = sub[prot_cols]\n\n    # Coerce to numeric\n    sub = sub.apply(pd.to_numeric, errors='coerce')\n\n    # Drop rows with any missing value\n    sub = sub.dropna(axis=0, how='any')\n\n    return sub.iloc[:, 0], sub.iloc[:, 1]  # returns PLK1, CHEK2 vectors\n\n\n# --------------------------------------------------\n# main workflow\n# --------------------------------------------------\nanswers = {}\n\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\")\n\n# Subtask-1: locate sheet & orientation\ntargets_primary = {'PLK1', 'CHEK2'}         # try protein level first\ntargets_phospho = {'PLK1', 'CHEK2-S163'}    # fall back to phosphosite if needed\n\nxls = pd.ExcelFile(file_path)\n\ntry:\n    sheet_name, orient = find_sheet_with_proteins(xls, {t.upper() for t in targets_primary})\n    used_targets = targets_primary\nexcept ValueError:\n    sheet_name, orient = find_sheet_with_proteins(xls, {t.upper() for t in targets_phospho})\n    used_targets = targets_phospho\n\nanswers[\"subtask-1\"] = {\n    \"sheet\": sheet_name,\n    \"orientation\": orient,\n    \"proteins_found\": list(used_targets)\n}\n\n# Subtask-2: extract numeric abundance vectors\ndf_full = pd.read_excel(xls, sheet_name=sheet_name, index_col=0)\nvec1, vec2 = extract_vectors(df_full, {t.upper() for t in used_targets}, orient)\n\nanswers[\"subtask-2\"] = {\n    \"n_samples_after_QC\": len(vec1)\n}\n\n# Subtask-3: correlation\nif len(vec1) == 0:\n    corr_val = np.nan\nelse:\n    corr_val = np.corrcoef(vec1, vec2)[0, 1]\n\ncorr_rounded = np.round(corr_val, 4) if pd.notnull(corr_val) else None\nanswers[\"subtask-3\"] = corr_rounded\nanswers[\"main-task\"] = corr_rounded\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 119.25528454780579
      }
    ],
    "runtime": 224.78838634490967
  },
  {
    "task_id": "biomedical-easy-2",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the average age of patients with serous tumor samples analyzed in the study?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Locate the sheet(s) and column names that hold (a) patient age and (b) tumor subtype / histology information in 1-s2.0-S0092867420301070-mmc1.xlsx.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": [
            {
              "sheet": "UCEC_CPTAC3_meta_table_V2.1",
              "age_cols": [
                "Path_Stage_Primary_Tumor-pT",
                "Path_Stage_Reg_Lymph_Nodes-pN",
                "Clin_Stage_Dist_Mets-cM",
                "Path_Stage_Dist_Mets-pM",
                "tumor_Stage-Pathological",
                "FIGO_stage",
                "Age",
                "EPIC_Macrophages",
                "CIBERSORT_Macrophages _M0",
                "CIBERSORT_Macrophages _M1",
                "CIBERSORT_Macrophages _M2"
              ],
              "subtype_cols": [
                "Proteomics_Tumor_Normal",
                "Tumor_purity",
                "Path_Stage_Primary_Tumor-pT",
                "tumor_Stage-Pathological",
                "Tumor_Site",
                "Tumor_Site_Other",
                "Tumor_Focality",
                "Tumor_Size_cm",
                "POLE_subtype",
                "Genomics_subtype",
                "WXS_tumor_sample_type",
                "WXS_tumor_filename",
                "WXS_tumor_UUID",
                "WGS_tumor_sample_type",
                "WGS_tumor_UUID"
              ]
            }
          ]
        },
        {
          "id": "subtask-2",
          "query": "From the identified sheet(s), extract all rows whose tumor subtype is serous (case-insensitive, allows values such as \u2018serous\u2019, \u2018high-grade serous\u2019, \u2018serous carcinoma\u2019, etc.), clean the age column (cast to numeric, drop missing or malformed entries) and calculate the mean age.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": null
        }
      ],
      "answer": null
    },
    "code": "import pandas as pd\nimport numpy as np\nimport json\nfrom pathlib import Path\n\n# path to data file\ndata_file = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# --------------------  subtask-1  --------------------\n# Inspect every sheet and record which ones have columns that look like \"age\" and \"tumor subtype\"\nsheets = pd.read_excel(data_file, sheet_name=None)  # read all sheets\nage_sheets = []\n\nfor sheet_name, df in sheets.items():\n    lower_cols = [c.lower() for c in df.columns]\n    has_age = any(\"age\" == c or c.endswith(\"_age\") or \"age \" in c for c in lower_cols)\n    has_subtype = any(\n        any(key in c for key in [\"histology\", \"subtype\", \"tumor\", \"sub-type\"])\n        for c in lower_cols\n    )\n    if has_age and has_subtype:\n        age_sheets.append(\n            {\n                \"sheet\": sheet_name,\n                \"age_cols\": [df.columns[i] for i, c in enumerate(lower_cols) if \"age\" in c],\n                \"subtype_cols\": [\n                    df.columns[i]\n                    for i, c in enumerate(lower_cols)\n                    if any(key in c for key in [\"histology\", \"subtype\", \"tumor\", \"sub-type\"])\n                ],\n            }\n        )\n\n# --------------------  subtask-2  --------------------\ndef compute_mean_from_sheet(sheet_dict):\n    \"\"\"Compute mean age of serous tumors for a sheet specification produced above\"\"\"\n    sheet_name = sheet_dict[\"sheet\"]\n    age_col = sheet_dict[\"age_cols\"][0]          # take the first candidate\n    subtype_col = sheet_dict[\"subtype_cols\"][0]  # take the first candidate\n\n    df = sheets[sheet_name].copy()\n\n    # Clean age column\n    df[age_col] = pd.to_numeric(df[age_col], errors=\"coerce\")\n\n    # Clean subtype column and filter for serous\n    df[subtype_col] = df[subtype_col].astype(str).str.lower()\n    serous_mask = df[subtype_col].str.contains(\"serous\", na=False)\n\n    serous_ages = df.loc[serous_mask, age_col].dropna()\n\n    if len(serous_ages) == 0:\n        return np.nan\n    return serous_ages.mean()\n\nall_means = [\n    compute_mean_from_sheet(sheet_info) for sheet_info in age_sheets\n    if compute_mean_from_sheet(sheet_info) == compute_mean_from_sheet(sheet_info)\n]\n\n# If multiple sheets contribute, pool the patients together instead of averaging the means\nif all_means:\n    # Gather pooled data\n    pooled_ages = []\n    for sheet_info in age_sheets:\n        sheet_name = sheet_info[\"sheet\"]\n        age_col = sheet_info[\"age_cols\"][0]\n        subtype_col = sheet_info[\"subtype_cols\"][0]\n        df = sheets[sheet_name].copy()\n        df[age_col] = pd.to_numeric(df[age_col], errors=\"coerce\")\n        df[subtype_col] = df[subtype_col].astype(str).str.lower()\n        serous_mask = df[subtype_col].str.contains(\"serous\", na=False)\n        pooled_ages.extend(df.loc[serous_mask, age_col].dropna().tolist())\n\n    final_mean_age = float(np.mean(pooled_ages)) if pooled_ages else np.nan\nelse:\n    final_mean_age = np.nan\n\n# --------------------  PRINT ANSWERS  --------------------\nanswers = {\n    \"subtask-1\": age_sheets,         # list of dicts with sheet and column information\n    \"subtask-2\": final_mean_age,     # numeric mean\n    \"main-task\": final_mean_age      # same as subtask-2\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-easy-2-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n Which file(s) contains clinical information? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Does the file '1-s2.0-S0092867420301070-mmc7.xlsx' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "Yes"
            },
            {
              "id": "subtask-2",
              "query": "Does the file 'hyperactivated.csv' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "hyperactivated.csv"
              ],
              "answer": "Yes"
            },
            {
              "id": "subtask-3",
              "query": "Does the file '1-s2.0-S0092867420301070-mmc2.xlsx' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": "No"
            },
            {
              "id": "subtask-4",
              "query": "Does the file '1-s2.0-S0092867420301070-mmc4.xlsx' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc4.xlsx"
              ],
              "answer": "Yes"
            },
            {
              "id": "subtask-5",
              "query": "Does the file '1-s2.0-S0092867420301070-mmc5.xlsx' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc5.xlsx"
              ],
              "answer": "Yes"
            },
            {
              "id": "subtask-6",
              "query": "Does the file '1-s2.0-S0092867420301070-mmc1.xlsx' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Yes"
            },
            {
              "id": "subtask-7",
              "query": "Does the file '1-s2.0-S0092867420301070-mmc6.xlsx' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": "No"
            },
            {
              "id": "subtask-8",
              "query": "Does the file '1-s2.0-S0092867420301070-mmc3.xlsx' contain clinical-level information (e.g. patient age, sex, histology, stage, grade, ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "Yes"
            }
          ],
          "answer": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ]
        },
        "code": "import json\nimport os\nimport re\nimport pandas as pd\n\n# path list provided\npaths = [\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx'\n]\n\nclinical_keywords = {\n    'age', 'sex', 'gender', 'histology', 'histological', 'grade',\n    'stage', 'tumor', 'tumour', 'diagnosis', 'sample_id', 'patient',\n    'case', 'platinum', 'response', 'site', 'status', 'survival',\n}\n\ndef has_clinical_info(df):\n    dcol = [str(c).strip().lower() for c in df.columns]\n    return any(any(kw in col for kw in clinical_keywords) for col in dcol)\n\nclinical_files = []\nper_file_answer = {}\n\nfor p in paths:\n    fname = os.path.basename(p)\n    try:\n        if p.endswith('.csv'):\n            df = pd.read_csv(p, nrows=5)\n            flag = has_clinical_info(df)\n        else:  # excel\n            xl = pd.ExcelFile(p)\n            flag = False\n            for sh in xl.sheet_names:\n                try:\n                    df = xl.parse(sh, nrows=5)\n                    if has_clinical_info(df):\n                        flag = True\n                        break\n                except Exception:\n                    continue\n        per_file_answer[fname] = \"Yes\" if flag else \"No\"\n        if flag:\n            clinical_files.append(fname)\n    except Exception as e:\n        per_file_answer[fname] = f\"Could not read file ({e})\"\n\n# Compile answers for json printing\nanswers = {\"main-task\": clinical_files}\n# individual subtasks\nfor idx, fname in enumerate(paths, 1):\n    answers[f\"subtask-{idx}\"] = per_file_answer[os.path.basename(fname)]\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 22.09329581260681
      },
      {
        "task_id": "biomedical-easy-2-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n Which cases id correspond to serous tumour samples included in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in 1-s2.0-S0092867420301070-mmc1.xlsx contain information about tumor samples and their histological subtype?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the relevant sheet(s), what is/are the column name(s) that capture histological subtype (e.g., contain the string 'histolog', 'subtype', or 'tumor type')?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "UCEC_CPTAC3_meta_table_V2.1": [
                  "Histologic_Grade_FIGO",
                  "Histologic_type",
                  "POLE_subtype",
                  "Genomics_subtype"
                ]
              }
            },
            {
              "id": "subtask-3",
              "query": "What is the column name that uniquely identifies each case (e.g., contains the string 'case' and 'id') in those sheet(s)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "UCEC_CPTAC3_meta_table_V2.1": []
              }
            },
            {
              "id": "subtask-4",
              "query": "Filter the rows where the histological subtype contains the word 'serous' (case-insensitive, allowing for leading/trailing spaces, different capitalisation, or NaN) and return the unique case-id values for these rows.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import pandas as pd\nimport json\nimport re\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Data source\n# ------------------------------------------------------------------\nFILE = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# ------------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------------\ndef locate_histology_columns(df):\n    \"\"\"Return a list of columns whose names suggest they store histology / subtype info.\"\"\"\n    return [\n        col\n        for col in df.columns\n        if re.search(r\"(histolog|subtype|tumou?r\\s*type|histotype)\", str(col), re.I)\n    ]\n\n\ndef locate_caseid_columns(df):\n    \"\"\"Return columns whose name suggests they store a case ID.\"\"\"\n    return [\n        col\n        for col in df.columns\n        if re.search(r\"case.*id\", str(col), re.I)\n    ]\n\n\ndef extract_serous_case_ids(xls_path):\n    \"\"\"Return a set of case IDs whose histology / subtype contains 'serous'.\"\"\"\n    xls = pd.ExcelFile(xls_path)\n    serous_cases = set()\n    sheet_debug_info = {}\n\n    for sheet_name in xls.sheet_names:\n        df = xls.parse(sheet_name, dtype=str)  # read as strings to avoid messy dtypes\n        df.columns = df.columns.map(lambda x: str(x).strip())  # normalise column names\n\n        hist_cols = locate_histology_columns(df)\n        case_cols = locate_caseid_columns(df)\n\n        # Store information for debugging / subtask answers\n        if hist_cols or case_cols:\n            sheet_debug_info[sheet_name] = {\n                \"histology_columns\": hist_cols,\n                \"case_id_columns\": case_cols\n            }\n\n        if not hist_cols or not case_cols:\n            continue  # Cannot process if either missing\n\n        # Build a boolean mask across all histology columns\n        hist_mask = False\n        for hcol in hist_cols:\n            col_serous = df[hcol].str.contains(r\"serous\", case=False, na=False)\n            hist_mask = hist_mask | col_serous\n\n        # Collect IDs\n        for cid_col in case_cols:\n            serous_ids = (\n                df.loc[hist_mask, cid_col]\n                .dropna()                      # remove missing\n                .map(str.strip)               # trim spaces\n                .tolist()\n            )\n            serous_cases.update(serous_ids)\n\n    return serous_cases, sheet_debug_info\n\n\n# ------------------------------------------------------------------\n# Execute extraction\n# ------------------------------------------------------------------\nserous_case_ids, debug_info = extract_serous_case_ids(FILE)\n\n# ------------------------------------------------------------------\n# Prepare answers for each subtask\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": list(debug_info.keys()),  # sheets containing relevant columns\n    \"subtask-2\": {\n        sheet: info[\"histology_columns\"] for sheet, info in debug_info.items()\n    },\n    \"subtask-3\": {\n        sheet: info[\"case_id_columns\"] for sheet, info in debug_info.items()\n    },\n    \"subtask-4\": sorted(serous_case_ids),  # final desired list\n    \"main-task\": sorted(serous_case_ids)\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 37.35213756561279
      },
      {
        "task_id": "biomedical-easy-2-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n Which is the age of patients with serous tumour samples in the study? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the sheet(s) in the Excel file that contain patient/sample-level meta-data, and identify the column names that correspond to (a) the histological tumour subtype (e.g. \u201cSerous\u201d) and (b) patient age.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "sheet_name_found": null,
                "age_column": null,
                "histology_column": null
              }
            },
            {
              "id": "subtask-2",
              "query": "After locating the correct sheet and columns, extract all rows where the histology / tumour-type column contains the word \u201cserous\u201d (case-insensitive, ignoring surrounding white-space, possible mixed annotations such as \u201cHigh-grade serous\u201d, etc.).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "n_serous_rows": 0
              }
            },
            {
              "id": "subtask-3",
              "query": "Clean the Age values: strip white-space, coerce to numeric (float), drop rows where age is missing or non-numeric).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "cleaned_age_values_preview": [],
                "total_valid_age_values": 0
              }
            },
            {
              "id": "subtask-4",
              "query": "Compute the mean (average) of the cleaned age values for the serous tumour rows.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": null
            }
          ],
          "answer": null
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n\n# ---------------- helper functions -----------------\ndef locate_age_and_histology_columns(df):\n    \"\"\"\n    Try to guess which two columns correspond to age and histology.\n    Returns (age_col, hist_col) or (None, None) if not found.\n    \"\"\"\n    cols_lower = {c.lower(): c for c in df.columns}\n    \n    # Possible keywords for age & histology\n    age_keywords = ['age', 'patient age', 'age (years)', 'age_years']\n    hist_keywords = ['histology', 'histological subtype', 'tumour type', \n                     'tumor type', 'tumour subtype', 'tumor subtype', \n                     'subtype', 'pathology']\n    \n    age_col = None\n    for kw in age_keywords:\n        if kw in cols_lower:\n            age_col = cols_lower[kw]\n            break\n    \n    hist_col = None\n    for kw in hist_keywords:\n        if kw in cols_lower:\n            hist_col = cols_lower[kw]\n            break\n    \n    return age_col, hist_col\n\n# ---------------- main extraction ------------------\n\nxlsx_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# Read all sheets (some files are large; sheet_name=None returns a dict)\nall_sheets = pd.read_excel(xlsx_path, sheet_name=None, engine='openpyxl')\n\nserous_ages = []\n\nsheet_with_meta = None\nage_column_name = None\nhist_column_name = None\n\nfor sheet_name, df in all_sheets.items():\n    age_col, hist_col = locate_age_and_histology_columns(df)\n    if age_col and hist_col:\n        # We found a candidate sheet\n        sheet_with_meta = sheet_name\n        age_column_name = age_col\n        hist_column_name = hist_col\n        \n        # Standardise histology values to string & check for \"serous\"\n        serous_mask = df[hist_col].astype(str).str.lower().str.contains(\"serous\", na=False)\n        if serous_mask.any():\n            # Coerce age to numeric\n            age_values = pd.to_numeric(df.loc[serous_mask, age_col], errors='coerce')\n            serous_ages.extend(age_values.dropna().tolist())\n\n# ---- answers to each subtask ------------------------------------------\n\nanswers = {}\n\n# subtask-1 answer: sheet name and column names discovered\nanswers[\"subtask-1\"] = {\n    \"sheet_name_found\": sheet_with_meta,\n    \"age_column\": age_column_name,\n    \"histology_column\": hist_column_name\n}\n\n# subtask-2 answer: number of serous rows extracted\nanswers[\"subtask-2\"] = {\n    \"n_serous_rows\": len(serous_ages)\n}\n\n# subtask-3 answer: list (or preview) of cleaned age values\nanswers[\"subtask-3\"] = {\n    \"cleaned_age_values_preview\": serous_ages[:10],   # show first 10 values\n    \"total_valid_age_values\": len(serous_ages)\n}\n\n# subtask-4 (and main-task) answer: average age\nif serous_ages:\n    average_age = float(np.mean(serous_ages))\nelse:\n    average_age = None\n\nanswers[\"subtask-4\"] = average_age\nanswers[\"main-task\"] = average_age\n\n# pretty-print all answers\nprint(json.dumps(answers, indent=4))",
        "runtime": 31.722041845321655
      },
      {
        "task_id": "biomedical-easy-2-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n What is the average the age of patients with serous tumour samples in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheets are present in the Excel file and might contain patient-level metadata (e.g. age, tumour type)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which of the worksheets identified in subtask-1 has both an Age column (patient age) and a column that records histological subtype / tumour type?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "sheet_with_metadata": "UCEC_CPTAC3_meta_table_V2.1",
                "age_column": "Path_Stage_Primary_Tumor-pT",
                "tumour_column": "Proteomics_Tumor_Normal"
              }
            },
            {
              "id": "subtask-3",
              "query": "From that worksheet, extract all rows whose tumour type / histology indicates a serous tumour (\u2018serous\u2019, case-insensitive) and compile a clean numeric list of their ages (convert strings to numbers, drop NA / non-numeric).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-4",
              "query": "Compute (and round to a sensible number of decimals, e.g. one) the mean/average age of the serous tumour patients obtained in subtask-3.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": null
            }
          ],
          "answer": null
        },
        "code": "import json\nimport pandas as pd\nimport numpy as np\n\n# ----------------------\n# Path to the Excel file\n# ----------------------\nFILE = \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\"\n\n# ----------------------\n# subtask-1: list sheets\n# ----------------------\nsheets = pd.ExcelFile(FILE).sheet_names\n\n# ----------------------\n# subtask-2: find sheet with both 'Age' and tumour/histology column\n#            We look for a column that contains 'age'\n#            AND one that contains 'histolog', 'tumor', or 'tumour'\n# ----------------------\ncandidate_sheet = None\nage_col = None\ntumour_col = None\n\nfor sh in sheets:\n    df_head = pd.read_excel(FILE, sheet_name=sh, nrows=0)  # just header\n    cols = [c.strip() for c in df_head.columns]\n    # search for an age column\n    potential_age = [c for c in cols if 'age' in c.lower()]\n    # search for a tumour / histology column\n    potential_tumour = [c for c in cols if any(x in c.lower() for x in ['histolog', 'tumour', 'tumor', 'subtype', 'type'])]\n    if potential_age and potential_tumour:\n        candidate_sheet = sh\n        age_col = potential_age[0]\n        tumour_col = potential_tumour[0]\n        break\n\n# ----------------------\n# subtask-3: extract serous rows and clean ages\n# ----------------------\nserous_ages = []\nif candidate_sheet is not None:\n    df = pd.read_excel(FILE, sheet_name=candidate_sheet)\n    # ensure column names stripped\n    df.columns = [c.strip() for c in df.columns]\n    \n    # cast age to numeric, coerce errors\n    df[age_col] = pd.to_numeric(df[age_col], errors='coerce')\n    \n    # filter for serous tumour\n    serous_mask = df[tumour_col].astype(str).str.contains('serous', case=False, na=False)\n    serous_ages = df.loc[serous_mask, age_col].dropna().tolist()\n\n# ----------------------\n# subtask-4: compute average\n# ----------------------\naverage_age = float(np.nan)   # default if no data\nif serous_ages:\n    average_age = round(float(np.mean(serous_ages)), 1)\n\n# ----------------------\n# prepare answers\n# ----------------------\nanswers = {\n    \"subtask-1\": sheets,\n    \"subtask-2\": {\n        \"sheet_with_metadata\": candidate_sheet,\n        \"age_column\": age_col,\n        \"tumour_column\": tumour_col\n    },\n    \"subtask-3\": serous_ages,\n    \"subtask-4\": average_age,\n    \"main-task\": average_age\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 20.783908367156982
      }
    ],
    "runtime": 28.102372884750366
  },
  {
    "task_id": "biomedical-hard-3",
    "model_output": {
      "id": "main-task",
      "query": "What is the age of the patient with the lowest APP-Z score?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc7.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "In which workbook-sheet is the APP-Z score stored, and what is the column name?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ]
        },
        {
          "id": "subtask-2",
          "query": "After locating the APP-Z column, read it and coerce the values to numeric, handling non-numeric and missing entries.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ]
        },
        {
          "id": "subtask-3",
          "query": "Locate the corresponding Age column for the same patient rows and coerce it to numeric, again handling bad data.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ]
        },
        {
          "id": "subtask-4",
          "query": "Identify the row(s) with the minimum valid APP-Z value and return the patient age(s).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ]
        }
      ],
      "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-3/_intermediate/errors-0.txt"
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper : scan every sheet in every workbook for a column containing \n# the keyword (case\u2013insensitive, ignoring punctuation/white-space)\ndef locate_column(workbook_paths, keyword):\n    locations = []  # list of (workbook_path, sheet_name, column_name)\n    for wb in workbook_paths:\n        xls = pd.ExcelFile(wb)\n        for sheet in xls.sheet_names:\n            df = xls.parse(sheet, nrows=2)     # only need headers\n            for col in df.columns:\n                if keyword.lower() in str(col).lower():\n                    locations.append((wb, sheet, col))\n    return locations\n\ndef coerce_numeric(series):\n    \"\"\"Return series as numeric (floats), coercing errors to NaN.\"\"\"\n    return pd.to_numeric(series, errors=\"coerce\")\n\n# ------------------------------------------------------------------\nexcel_paths = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\"\n]\n\n# ================= SUBTASK-1  : where is APP-Z?\nappz_locations = locate_column(excel_paths, \"APP-Z\")  # list of matches\n# For this dataset there should be exactly one logical location; \n# if there are more, prefer the one that also has an Age column.\nage_locations = locate_column(excel_paths, \"age\")\n\nchosen = None\nfor loc in appz_locations:\n    wb, sheet, col = loc\n    # Does this sheet also contain an age column?\n    has_age = any((wb == a[0] and sheet == a[1]) for a in age_locations)\n    if has_age:\n        chosen = loc\n        break\n# Fallback to the first if none found with age\nif chosen is None and appz_locations:\n    chosen = appz_locations[0]\n\n# unpack\nwb_path, sheet_name, appz_col = chosen\n# Find exact Age column in same sheet\nxls = pd.ExcelFile(wb_path)\ntmp_df = xls.parse(sheet_name, nrows=2)\nage_col = None\nfor col in tmp_df.columns:\n    if \"age\" in str(col).lower():\n        age_col = col\n        break\n\n# ================= SUBTASK-2 & 3  : load numeric columns\ndf = pd.read_excel(wb_path, sheet_name=sheet_name)\ndf[appz_col] = coerce_numeric(df[appz_col])\ndf[age_col]  = coerce_numeric(df[age_col])\n\n# ================= SUBTASK-4  : find min APP-Z\nvalid = df.dropna(subset=[appz_col, age_col])\nmin_val = valid[appz_col].min()\nmin_rows = valid[valid[appz_col] == min_val]\n\n# Extract age(s) as python types (could be more than one patient)\nages_of_min = min_rows[age_col].tolist()\n\n# ------------------------------------------------------------------\n# Assemble the answers\nanswers = {\n    \"subtask-1\": {\n        \"workbook\": Path(wb_path).name,\n        \"sheet\": sheet_name,\n        \"appz_column\": appz_col,\n        \"age_column\": age_col\n    },\n    \"subtask-2\": \"APP-Z column successfully coerced to numeric; {} non-NaN values retained\".format(valid[appz_col].notna().sum()),\n    \"subtask-3\": \"Age column successfully coerced to numeric; {} non-NaN values retained\".format(valid[age_col].notna().sum()),\n    \"subtask-4\": {\n        \"lowest_APP-Z_score\": min_val,\n        \"age_of_patient(s)\": ages_of_min\n    },\n    \"main-task\": ages_of_min[0] if len(ages_of_min)==1 else ages_of_min\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-3-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What files contain clinical data and data about the APP-Z? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which files (and sheets, if applicable) contain any variable whose name includes the string \"APP\" AND \"Z\" (e.g., APP_Z, APP-Z score, APP Z\u2010Score, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which files (and sheets, if applicable) contain obvious clinical variables (e.g., Age, Sex, Gender, Diagnosis, Patient_ID, Subject_ID, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv"
              ]
            }
          ],
          "answer": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ]
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef list_columns_from_excel(path):\n    \"\"\"\n    Return a dict   {sheet_name: [list_of_column_names]} for an Excel file.\n    If loading a sheet fails (e.g., weird format), catch the error and continue.\n    \"\"\"\n    cols = {}\n    try:\n        xl = pd.ExcelFile(path)\n        for sheet in xl.sheet_names:\n            try:\n                df = xl.parse(sheet, nrows=1)  # only need header row\n                cols[sheet] = [str(c) for c in df.columns]\n            except Exception:\n                pass\n    except Exception:\n        pass\n    return cols\n\n\ndef list_columns_from_csv(path):\n    \"\"\"\n    Load just the header of the CSV file.\n    \"\"\"\n    try:\n        df = pd.read_csv(path, nrows=0)\n        return {Path(path).stem: [str(c) for c in df.columns]}\n    except Exception:\n        return {}\n\n\ndef get_all_columns(path):\n    \"\"\"\n    Wrapper that chooses the right reader based on extension.\n    \"\"\"\n    path = str(path)\n    if path.lower().endswith(\".csv\"):\n        return list_columns_from_csv(path)\n    else:\n        return list_columns_from_excel(path)\n\n\ndef contains_app_z(col_name):\n    \"\"\"\n    Does the column name contain both 'app' and 'z'?\n    Make it case-insensitive and ignore non-alphanumeric chars.\n    \"\"\"\n    if not isinstance(col_name, str):\n        return False\n    name = col_name.lower()\n    return (\"app\" in name) and (\"z\" in name)\n\n\ndef is_clinical(col_name):\n    \"\"\"\n    Heuristically decide if a column is a clinical variable.\n    \"\"\"\n    if not isinstance(col_name, str):\n        return False\n    name = col_name.lower()\n    clinical_keywords = [\n        \"age\", \"sex\", \"gender\",\n        \"diagnosis\", \"dx\",\n        \"subject\", \"patient\", \"id\", \"cohort\"\n    ]\n    return any(k in name for k in clinical_keywords)\n\n\n# ------------------------------------------------------------------\n# inspect every file\n# ------------------------------------------------------------------\ndata_dir = \"/home/gridsan/elai/Kramabench/data/biomedical/input\"\nfile_names = [\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"hyperactivated.csv\",\n    \"1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc3.xlsx\",\n]\nfile_paths = [Path(data_dir) / f for f in file_names]\n\nfiles_with_appz = set()\nfiles_with_clinical = set()\n\n# keep a record of which sheet had what\ndetails_appz = []\ndetails_clinical = []\n\nfor f in file_paths:\n    column_dict = get_all_columns(f)\n    for sheet, cols in column_dict.items():\n        # standardize missing / nan names\n        cols = [c if isinstance(c, str) else \"\" for c in cols]\n        if any(contains_app_z(c) for c in cols):\n            files_with_appz.add(f.name)\n            details_appz.append((f.name, sheet))\n        if any(is_clinical(c) for c in cols):\n            files_with_clinical.add(f.name)\n            details_clinical.append((f.name, sheet))\n\nfiles_with_both = sorted(list(files_with_appz & files_with_clinical))\n\n# ------------------------------------------------------------------\n# answers for each step\n# ------------------------------------------------------------------\nanswer_subtask1 = sorted(list(files_with_appz))\nanswer_subtask2 = sorted(list(files_with_clinical))\nanswer_main = files_with_both  # files that have BOTH APP-Z and clinical variables\n\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": answer_subtask1,\n            \"subtask-2\": answer_subtask2,\n            \"main-task\": answer_main,\n        },\n        indent=4,\n    )\n)",
        "runtime": 67.45657086372375
      },
      {
        "task_id": "biomedical-hard-3-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What are the case ids included in the study with endometrioid or serous histological samples? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheet(s) in 1-s2.0-S0092867420301070-mmc1.xlsx contain both histology information and case identifiers?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the identified worksheet, what are the exact column names that store (a) histological subtype and (b) case identifiers?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "histology_column": "Histologic_Grade_FIGO",
                "caseid_column": "Proteomics_Parent_Sample_IDs",
                "sheet": "UCEC_CPTAC3_meta_table_V2.1"
              }
            },
            {
              "id": "subtask-3",
              "query": "Clean/standardise the histological-subtype column (e.g., lowercase, strip whitespace, fill NA with empty string) and filter the rows whose histology contains either the substring \"endometrioid\" or \"serous\".",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "0 rows match (endometrioid or serous)"
            },
            {
              "id": "subtask-4",
              "query": "Return the unique case identifiers from the filtered rows (endometrioid or serous).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# helper: robustly read all sheets into dataframes\n# -------------------------------------------------\nexcel_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx')\nxls = pd.ExcelFile(excel_path)\nsheet_names = xls.sheet_names\n\n# ------------------------------------------------------------------\n# SUBTASK-1  \u2500\u2500 Which sheet(s) contain both histology & case id info?\n# ------------------------------------------------------------------\ncandidate_sheets = []\nfor sheet in sheet_names:\n    df = xls.parse(sheet, nrows=5)  # just peek first rows\n    cols_lower = [c.lower().strip() for c in df.columns]\n    if any('histolog' in c or 'histotype' in c or 'histology' in c for c in cols_lower) and \\\n       any('case' in c and 'id' in c or 'sample' in c for c in cols_lower):\n        candidate_sheets.append(sheet)\n\n# ------------------------------------------------------------------\n# SUBTASK-2  \u2500\u2500 Identify exact column names\n# ------------------------------------------------------------------\nhistology_col, caseid_col = None, None\ntarget_sheet = None\n\nfor sheet in candidate_sheets:\n    df = xls.parse(sheet)\n    original_cols = df.columns\n    lower_cols = [c.lower().strip() for c in original_cols]\n\n    # find histology column\n    for idx, cname in enumerate(lower_cols):\n        if 'histolog' in cname or 'histotype' in cname or 'histology' in cname:\n            histology_col = original_cols[idx]\n            break\n    # find case id column\n    for idx, cname in enumerate(lower_cols):\n        if ('case' in cname and 'id' in cname) or cname == 'case' or 'sample id' in cname or \\\n           (('case' in cname or 'sample' in cname) and 'id' in cname):\n            caseid_col = original_cols[idx]\n            break\n    if histology_col is not None and caseid_col is not None:\n        target_sheet = sheet\n        break\n\n# guard\nif target_sheet is None:\n    raise ValueError(\"Could not locate worksheet with required columns.\")\n\n# ------------------------------------------------------------------\n# SUBTASK-3  \u2500\u2500 Clean & filter rows\n# ------------------------------------------------------------------\ndf_full = xls.parse(target_sheet)\n\n# convert caseid to string because IDs can be numeric or mixed\ndf_full[caseid_col] = df_full[caseid_col].astype(str).str.strip()\n\n# clean histology column\ndf_full[histology_col] = df_full[histology_col].astype(str).str.lower().str.strip()\n\nmask = df_full[histology_col].str.contains('endometrioid', na=False) | \\\n       df_full[histology_col].str.contains('serous', na=False)\n\nfiltered_df = df_full[mask]\n\n# ------------------------------------------------------------------\n# SUBTASK-4  \u2500\u2500 Unique case identifiers\n# ------------------------------------------------------------------\nunique_case_ids = sorted(filtered_df[caseid_col].dropna().unique().tolist())\n\n# ------------------------------------------------------------------\n# PREPARE ANSWERS DICT\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": candidate_sheets,                             # list[str]\n    \"subtask-2\": {\"histology_column\": histology_col, \"caseid_column\": caseid_col, \"sheet\": target_sheet},\n    \"subtask-3\": f\"{len(filtered_df)} rows match (endometrioid or serous)\",\n    \"subtask-4\": unique_case_ids,\n    \"main-task\": unique_case_ids                               # final answer is same as subtask-4\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 54.540993213653564
      },
      {
        "task_id": "biomedical-hard-3-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What is the value of the 'idx' column corresponding to the row with the minimum 'APP_Z_score'? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in the Excel file contain a column named 'APP_Z_score'?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "B-APM subtypes"
              ]
            },
            {
              "id": "subtask-2",
              "query": "What is the minimum numeric value of the 'APP_Z_score' across all sheets that contain that column (after coercing non-numeric values to NaN and dropping them)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": -2.15300777
            },
            {
              "id": "subtask-3",
              "query": "What is the value in the 'idx' column for the row that holds this minimum 'APP_Z_score'?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "S019"
            }
          ],
          "answer": "S019"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# Helper variables & containers\n# -------------------------------------------------\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\")\nexcel_book   = pd.ExcelFile(file_path)\n\n# -------------------------------------------------\n# subtask-1 \u2013 find sheets that contain APP_Z_score\n# -------------------------------------------------\nsheets_with_appz = []\nfor sheet in excel_book.sheet_names:\n    # read only first 0 rows just to get columns \u2013 faster than full load\n    df_head = excel_book.parse(sheet, nrows=0)\n    if any(col.strip().lower() == \"app_z_score\" for col in df_head.columns.astype(str)):\n        sheets_with_appz.append(sheet)\n\n# -------------------------------------------------\n# subtask-2 \u2013 find the overall minimum APP_Z_score\n#              and remember its sheet & row position\n# -------------------------------------------------\nmin_appz_value  = None\nmin_row_details = None     # (sheet_name, row_index, row_series)\n\nfor sheet in sheets_with_appz:\n    df = excel_book.parse(sheet)\n    \n    # Standardise column names (strip / lower) so we don\u2019t care about spaces/case\n    df.columns = [c.strip() for c in df.columns.astype(str)]\n    \n    # Coerce APP_Z_score column to numeric\n    df[\"APP_Z_score\"] = pd.to_numeric(df[\"APP_Z_score\"], errors=\"coerce\")\n    \n    # Drop rows with missing scores\n    df_valid = df.dropna(subset=[\"APP_Z_score\"]).copy()\n    if df_valid.empty:\n        continue\n    \n    # Current sheet minimum\n    sheet_min_idx   = df_valid[\"APP_Z_score\"].idxmin()\n    sheet_min_value = df_valid.loc[sheet_min_idx, \"APP_Z_score\"]\n    \n    # Update global minimum\n    if (min_appz_value is None) or (sheet_min_value < min_appz_value):\n        min_appz_value  = sheet_min_value\n        min_row_details = (sheet, sheet_min_idx, df_valid.loc[sheet_min_idx])\n\n# -------------------------------------------------\n# subtask-3 \u2013 obtain the corresponding idx value\n# -------------------------------------------------\nif min_row_details is None:\n    raise ValueError(\"Could not find any numeric APP_Z_score in the workbook.\")\n\nsheet_name, row_idx, row_series = min_row_details\n\n# Make sure 'idx' column exists and is properly typed\nif \"idx\" not in row_series:\n    raise KeyError(f\"The column 'idx' was not found in sheet '{sheet_name}' for the row with the minimum APP_Z_score.\")\n\nidx_value = row_series[\"idx\"]\n\n# In case idx is a string that looks like an int, convert to int where possible\ntry:\n    idx_value_clean = int(idx_value)\nexcept (ValueError, TypeError):\n    idx_value_clean = idx_value  # keep original if it can't be coerced\n\n# -------------------------------------------------\n# Print answers for each task\n# -------------------------------------------------\nprint(json.dumps(\n    {\n        \"subtask-1\": sheets_with_appz,\n        \"subtask-2\": min_appz_value,\n        \"subtask-3\": idx_value_clean,\n        \"main-task\": idx_value_clean\n    },\n    indent=4))",
        "runtime": 26.93485164642334
      },
      {
        "task_id": "biomedical-hard-3-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What is the age of the patient with the identifier 'S019'? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in the Excel file contain both a patient identifier column (e.g., the value 'S019') and an age column?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "In the sheet identified in subtask-1, what is the age of the patient whose identifier equals 'S019' (after cleaning and type-casting, if necessary)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": 2
            }
          ],
          "answer": 2
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper parameters\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\ntarget_id  = \"S019\"\n\n# ------------------------------------------------------------------\n# Containers for answers\nanswer_sheet_candidates = []\nage_value = None\n\n# ------------------------------------------------------------------\n# 1. Load the Excel file and iterate through every sheet\nxls = pd.ExcelFile(file_path)\n\nfor sheet_name in xls.sheet_names:\n    try:\n        df = pd.read_excel(xls, sheet_name=sheet_name)\n    except Exception:\n        # skip unreadable sheets\n        continue\n\n    # Skip completely empty sheets\n    if df.shape[1] == 0:\n        continue\n\n    # ------------------------------------------------------------------\n    # 2. Find columns that might hold patient identifiers\n    #    (look for 'id', 'identifier', 'patient' in the column name)\n    possible_id_cols = [\n        col for col in df.columns\n        if any(key in str(col).lower() for key in [\"id\", \"identifier\", \"patient\"])\n    ]\n    # If no \u201cpossible\u201d columns were found, just search every column\n    if not possible_id_cols:\n        possible_id_cols = df.columns\n\n    # ------------------------------------------------------------------\n    # 3. Look for the target identifier in any of those columns\n    mask = None\n    for col in possible_id_cols:\n        # Treat everything as string for comparison\n        tmp_mask = df[col].astype(str).str.strip() == target_id\n        if tmp_mask.any():\n            mask = tmp_mask\n            id_col_chosen = col\n            break\n\n    # ------------------------------------------------------------------\n    # 4. If we found the row, look for a column that contains the age\n    if mask is not None:\n        age_cols = [c for c in df.columns if \"age\" in str(c).lower()]\n        if age_cols:\n            age_col = age_cols[0]               # take the first matching column\n            raw_age = df.loc[mask, age_col].iloc[0]\n\n            # -------------------------\n            #   Data-type cleaning\n            # -------------------------\n            # Remove any trailing non-numeric characters (e.g., '34*' -> '34')\n            cleaned = str(raw_age).strip()\n            cleaned = \"\".join(ch for ch in cleaned if (ch.isdigit() or ch == \".\"))\n\n            try:\n                # Cast to numeric; prefer int if integral\n                num_age = pd.to_numeric(cleaned, errors=\"coerce\")\n                if pd.notnull(num_age):\n                    age_value = int(num_age) if float(num_age).is_integer() else float(num_age)\n                else:\n                    age_value = raw_age                   # fallback to original value\n            except Exception:\n                age_value = raw_age\n\n            # keep track of which sheets contained the ID\n            answer_sheet_candidates.append(sheet_name)\n        else:\n            # Identifier found but no age column in this sheet\n            continue\n\n# ------------------------------------------------------------------\n# Build answers for each subtask\nanswers = {\n    \"subtask-1\": answer_sheet_candidates,\n    \"subtask-2\": age_value,\n    \"main-task\": age_value\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 21.452499628067017
      }
    ],
    "runtime": 19.04114580154419
  },
  {
    "task_id": "biomedical-hard-4",
    "model_output": {
      "id": "main-task",
      "query": "What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc4.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Which tumor-sample IDs contain the peptide sequence HPKPEVLGSSADGALLVSLDGLR?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc4.xlsx"
          ]
        },
        {
          "id": "subtask-2",
          "query": "What are the histological grades of the tumor-sample IDs returned in subtask-1?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        }
      ],
      "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-4/_intermediate/errors-0.txt"
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper: read every sheet of an excel file into one dataframe\ndef read_all_sheets(xlsx_path):\n    \"\"\"\n    Return a dictionary {sheet_name: dataframe} for all sheets in an\n    excel file.  Cells are left as strings where possible.\n    \"\"\"\n    return pd.read_excel(\n        xlsx_path,\n        sheet_name=None,          # read all sheets\n        dtype=str,                # keep everything as string, we\u2019ll cast later\n        engine=\"openpyxl\"         # fallback reader\n    )\n\n# ---------------------------------------------------------------\n# 1) SUBTASK-1  \u2013 which samples carry the peptide?\nfile_mmc4 = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\")\nsheets_mmc4 = read_all_sheets(file_mmc4)\n\ntarget_peptide = \"HPKPEVLGSSADGALLVSLDGLR\"\nsamples_with_peptide = set()\n\nfor sheet_name, df in sheets_mmc4.items():\n    # dataframe could be gigantic \u2013 work column-wise\n    for col in df.columns:\n        # quick heuristic \u2013 only look at columns that could hold peptides\n        if \"peptide\" in col.lower() or \"sequence\" in col.lower():\n            # standardise casing & strip spaces before comparing\n            mask = df[col].astype(str).str.strip().str.upper() == target_peptide.upper()\n            if mask.any():\n                # figure out which column in this sheet looks like sample ID\n                # common labels: 'Sample', 'SampleID', 'Patient', etc.\n                possible_id_cols = [c for c in df.columns if any(k in c.lower()\n                                       for k in [\"sample\", \"tumor\", \"patient\", \"specimen\", \"id\"])]\n                if not possible_id_cols:\n                    # if nothing obvious, we\u2019ll take the index label instead\n                    ids = df.index[mask].astype(str).tolist()\n                else:\n                    # pick the first candidate id column\n                    ids = df.loc[mask, possible_id_cols[0]].astype(str).tolist()\n                samples_with_peptide.update([i.strip() for i in ids if pd.notna(i)])\n\n# ---------------------------------------------------------------\n# 2) SUBTASK-2  \u2013 look up histological grade for those samples\nfile_mmc1 = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\nsheets_mmc1 = read_all_sheets(file_mmc1)\n\ngrades = set()\nfor sheet_name, df in sheets_mmc1.items():\n    # find a column that contains the sample ID\n    id_cols = [c for c in df.columns if any(k in c.lower()\n                for k in [\"sample\", \"tumor\", \"patient\", \"specimen\", \"id\"])]\n    grade_cols = [c for c in df.columns if \"grade\" in c.lower()]\n    if not id_cols or not grade_cols:\n        continue\n\n    # narrow dataframe down to the samples of interest\n    for id_col in id_cols:\n        # normalise IDs\n        df['__ID__'] = df[id_col].astype(str).str.strip()\n        matches = df['__ID__'].isin(samples_with_peptide)\n        if matches.any():\n            for g_col in grade_cols:\n                g_series = df.loc[matches, g_col].astype(str).str.strip()\n                grades.update(g_series.tolist())\n\n# tidy the answer (remove obvious NaNs / empties / \u2018nan\u2019 strings)\ngrades = {g for g in grades if g and g.lower() != \"nan\"}\n\n# ----------------------------------------------------------------\n# package answers\nanswer_dict = {\n    \"subtask-1\": sorted(samples_with_peptide),\n    \"subtask-2\": sorted(grades),\n    \"main-task\": sorted(grades)  # for this question, main answer == subtask-2 answer\n}\n\nprint(json.dumps(answer_dict, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-4-1",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found?, please answer the following question:\n What file(s) contains clinical information and peptides found per tumor samples? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Inspect 1-s2.0-S0092867420301070-mmc7.xlsx for columns that describe (i) clinical/phenotypic information (e.g., grade, stage, age, sex, histology, tumor id) AND (ii) peptide-level columns (e.g., \u2018peptide\u2019, \u2018sequence\u2019, \u2018PeptideSequence\u2019).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Inspect hyperactivated.csv for columns that describe clinical/phenotypic information AND peptide-level columns.",
              "data_sources": [
                "hyperactivated.csv"
              ]
            },
            {
              "id": "subtask-3",
              "query": "Inspect 1-s2.0-S0092867420301070-mmc2.xlsx for clinical and peptide columns.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ]
            },
            {
              "id": "subtask-4",
              "query": "Inspect 1-s2.0-S0092867420301070-mmc4.xlsx for clinical and peptide columns.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc4.xlsx"
              ]
            },
            {
              "id": "subtask-5",
              "query": "Inspect 1-s2.0-S0092867420301070-mmc5.xlsx for clinical and peptide columns.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc5.xlsx"
              ]
            },
            {
              "id": "subtask-6",
              "query": "Inspect 1-s2.0-S0092867420301070-mmc1.xlsx for clinical and peptide columns.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            },
            {
              "id": "subtask-7",
              "query": "Inspect 1-s2.0-S0092867420301070-mmc6.xlsx for clinical and peptide columns.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ]
            },
            {
              "id": "subtask-8",
              "query": "Inspect 1-s2.0-S0092867420301070-mmc3.xlsx for clinical and peptide columns.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-4-1/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------------\ndef sheet_iterator(excel_path):\n    \"\"\"Yield (sheet_name, DataFrame) pairs for every sheet in the file.\"\"\"\n    try:\n        xls = pd.ExcelFile(excel_path)\n        for sheet in xls.sheet_names:\n            try:\n                df = pd.read_excel(excel_path, sheet_name=sheet, nrows=200)  # read a sample\n            except ValueError:\n                # empty sheet\n                continue\n            yield sheet, df\n    except Exception:\n        pass\n\n\ndef contains_keywords(columns, keywords):\n    \"\"\"Return True if ANY column header contains ANY keyword (case-insensitive).\"\"\"\n    cols_lower = [str(c).lower() for c in columns]\n    return any(any(k in col for k in cols_lower) for k in keywords)\n\n\n# ------------------------------------------------------------------\n# Main inspection loop\n# ------------------------------------------------------------------\nclinical_kw = [\n    \"grade\", \"histologic\", \"histology\", \"stage\", \"age\", \"sex\",\n    \"gender\", \"tumor\", \"sample\", \"id\", \"patient\", \"subtype\"\n]\npeptide_kw = [\n    \"peptide\", \"sequence\", \"peptidesequence\", \"aa sequence\",\n    \"epitope\", \"protein\", \"peptide id\"\n]\n\nfile_paths = {\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"hyperactivated.csv\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv\",\n    \"1-s2.0-S0092867420301070-mmc2.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc4.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc5.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc6.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc3.xlsx\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\n}\n\nsatisfying_files = []            # final answer\nsubtask_answers = {}             # store True/False for each file\n\nfor fname, path in file_paths.items():\n    has_clinical = False\n    has_peptide  = False\n\n    if Path(path).suffix.lower() == \".csv\":\n        try:\n            df = pd.read_csv(path, nrows=200)   # peek only first 200 rows\n            has_clinical = contains_keywords(df.columns, clinical_kw)\n            has_peptide  = contains_keywords(df.columns, peptide_kw)\n        except Exception:\n            pass\n    else:  # Excel\n        for _, df in sheet_iterator(path):\n            # evaluate each sheet; stop early if both True\n            has_clinical = has_clinical or contains_keywords(df.columns, clinical_kw)\n            has_peptide  = has_peptide  or contains_keywords(df.columns, peptide_kw)\n            if has_clinical and has_peptide:\n                break\n\n    subtask_answers[fname] = has_clinical and has_peptide\n    if has_clinical and has_peptide:\n        satisfying_files.append(fname)\n\n# ------------------------------------------------------------------\n# Compose output\n# ------------------------------------------------------------------\noutput = {f\"subtask-{i+1}\": subtask_answers[fname]\n          for i, fname in enumerate(file_paths.keys())}\n\noutput[\"main-task\"] = satisfying_files\n\nprint(json.dumps(output, indent=4))",
        "runtime": 44.77890753746033
      },
      {
        "task_id": "biomedical-hard-4-2",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found?, please answer the following question:\n What samples are found with the peptide HPKPEVLGSSADGALLVSLDGLR? ",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate which of the eight provided data sources (Excel/CSV) contain at least one row where any peptide-sequence column exactly matches \u2018HPKPEVLGSSADGALLVSLDGLR\u2019.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "For every file that does contain the peptide, extract the sample identifier(s) from the corresponding rows. Because column names may differ (e.g. \u201cSample\u201d, \u201cSample_ID\u201d, \u201cTissue\u201d, \u201cRaw file\u201d), inspect the header row and retrieve all non-empty values that plausibly identify the sample of origin.",
              "data_sources": [
                "Any file(s) returned by subtask-1"
              ]
            },
            {
              "id": "subtask-3",
              "query": "Standardise and de-duplicate the sample identifiers collected in subtask-2 (trim white-space, cast to string, make case-consistent) and output the final unique list of samples in which peptide \u2018HPKPEVLGSSADGALLVSLDGLR\u2019 was detected.",
              "data_sources": [
                "Output of subtask-2"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-4-2/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper ------------------------------------------------------------\n# ------------------------------------------------------------------\ndef load_any(path):\n    \"\"\"Load excel (any sheet) or csv safely into one big dataframe.\"\"\"\n    suffix = Path(path).suffix.lower()\n    try:\n        if suffix == \".csv\":\n            df = pd.read_csv(path, low_memory=False)\n            return {\"\": df}          # single unnamed sheet\n        else:                       # excel\n            xls = pd.ExcelFile(path)\n            dct = {}\n            for sheet in xls.sheet_names:\n                try:\n                    dct[sheet] = xls.parse(sheet)\n                except Exception:\n                    pass\n            return dct\n    except Exception as e:\n        print(f\"Could not read {path}: {e}\")\n        return {}\n\ndef standardise_columns(df):\n    \"\"\"Lower case col names and strip spaces for easier matching.\"\"\"\n    df = df.copy()\n    df.columns = [str(c).strip().lower() for c in df.columns]\n    return df\n\n# ------------------------------------------------------------------\n# subtask-1 : locate files/sheets containing the peptide ------------\n# ------------------------------------------------------------------\nPEPTIDE = \"HPKPEVLGSSADGALLVSLDGLR\"\nfiles = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\n]\n\ncontains_peptide = {}  # {(file, sheet): dataframe_rows}\n\nfor f in files:\n    sheets = load_any(f)\n    for sheet_name, df in sheets.items():\n        if df is None or df.empty:\n            continue\n        df = standardise_columns(df)\n\n        # find peptide columns (those with 'peptide' or 'sequence' in header)\n        peptide_cols = [c for c in df.columns if any(x in c for x in [\"peptide\", \"sequence\"])]\n        if not peptide_cols:\n            continue\n        # look for exact match (case-sensitive as given)\n        mask = pd.Series([False]*len(df))\n        for col in peptide_cols:\n            # ensure column is string\n            mask = mask | (df[col].astype(str).str.strip() == PEPTIDE)\n        rows = df[mask]\n        if not rows.empty:\n            contains_peptide[(f, sheet_name)] = rows\n\n# ------------------------------------------------------------------\n# subtask-2 : extract sample identifiers ---------------------------\n# ------------------------------------------------------------------\ncandidate_sample_cols_keywords = [\"sample\", \"sample_id\", \"tissue\", \"raw file\", \"raw_file\",\n                                  \"ms_file\", \"tumor id\", \"tumor\", \"patient\", \"specimen\"]\nsamples_found = set()\n\nfor (f, sheet), rows in contains_peptide.items():\n    df = standardise_columns(rows)\n    for col in df.columns:\n        if any(k in col for k in candidate_sample_cols_keywords):\n            vals = df[col].dropna().astype(str).str.strip()\n            samples_found.update(vals)\n\n# fall-back: sometimes the sample id may be embedded in column names (wide format)\n# e.g., intensities across samples.  Scan column names too.\nfor (f, sheet), rows in contains_peptide.items():\n    cols = rows.columns\n    for c in cols:\n        # ignore typical non-sample prefixes\n        if any(p in c for p in [\"peptide\", \"sequence\", \"score\", \"protein\", \"gene\", \"accession\"]):\n            continue\n        if PEPTIDE in rows.iloc[0].astype(str).to_string():  # placeholder\n            pass\n        # heuristically, long names with P-numbers etc. might be sample names\n        # here we skip this advanced detection for brevity.\n\n# clean\nsamples_found = {s for s in samples_found if s}  # remove empty strings\nsamples_found = sorted(samples_found)\n\n# ------------------------------------------------------------------\n# subtask-3 : done \u2013 now print answers -----------------------------\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": sorted({Path(k[0]).name + (\"::\" + k[1] if k[1] else \"\") for k in contains_peptide}),\n    \"subtask-2\": samples_found,\n    \"main-task\": samples_found\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 192.3639895915985
      },
      {
        "task_id": "biomedical-hard-4-3",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found?, please answer the following question:\n What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found? ",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which tumor/sample IDs contain the peptide sequence HPKPEVLGSSADGALLVSLDGLR?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "hyperactivated.csv"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Given the tumor/sample IDs that contain HPKPEVLGSSADGALLVSLDGLR, what are their histological grades?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-4-3/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# CONFIG\n# ------------------------------------------------------------------\nPEPTIDE = \"HPKPEVLGSSADGALLVSLDGLR\"\nDATA_DIR = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input\")\nFILE_NAMES = [\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"hyperactivated.csv\",\n    \"1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc3.xlsx\"\n]\n# ------------------------------------------------------------------\n# HELPERS\n# ------------------------------------------------------------------\ndef load_any(path):\n    \"\"\"\n    Load a file into a pandas DataFrame.\n    Supports .csv and .xlsx with multiple sheets (concatenated).\n    \"\"\"\n    if path.suffix == \".csv\":\n        try:\n            return {\"Sheet1\": pd.read_csv(path, low_memory=False)}\n        except UnicodeDecodeError:\n            return {\"Sheet1\": pd.read_csv(path, low_memory=False, encoding=\"latin1\")}\n    elif path.suffix in [\".xls\", \".xlsx\"]:\n        sheets_dict = pd.read_excel(path, sheet_name=None, engine=\"openpyxl\")\n        return sheets_dict\n    else:\n        raise ValueError(f\"Unsupported file type: {path}\")\n\ndef find_sequence_rows(df, peptide):\n    \"\"\"\n    Return rows where any column that looks like a peptide/sequence column\n    (i.e. contains 'peptide', 'sequence', or 'seq' in the column name)\n    matches the peptide of interest (case-insensitive).\n    \"\"\"\n    peptide_upper = peptide.upper()\n    candidate_cols = [\n        c for c in df.columns\n        if any(key in str(c).lower() for key in [\"peptide\", \"sequence\", \"seq\"])\n    ]\n    if not candidate_cols:\n        return pd.DataFrame(columns=df.columns)  # empty\n    mask = False\n    for c in candidate_cols:\n        mask = mask | (df[c].astype(str).str.upper() == peptide_upper)\n    return df[mask]\n\ndef extract_id_cols(df):\n    \"\"\"\n    Return a Series of sample/tumor ids from columns that look like IDs.\n    (column names containing `sample`, `tumor`, `id`, or `barcode`)\n    \"\"\"\n    id_cols = [\n        c for c in df.columns\n        if any(key in str(c).lower() for key in [\"sample\", \"tumor\", \"id\", \"barcode\"])\n    ]\n    if not id_cols:\n        return pd.Series(dtype=str)\n    # concatenate all ids into a single series\n    ids = pd.Series(dtype=str)\n    for c in id_cols:\n        ids = pd.concat([ids, df[c].astype(str)])\n    ids = ids[ids != \"nan\"].unique()\n    return pd.Series(ids, dtype=str)\n\ndef clean_id(x):\n    \"\"\"basic cleanup to facilitate merges\"\"\"\n    return str(x).strip().upper()\n\n# ------------------------------------------------------------------\n# SUBTASK-1 \u2013 find all tumor/sample IDs containing the peptide\n# ------------------------------------------------------------------\nsample_ids = set()\n\nfor file_name in FILE_NAMES:\n    file_path = DATA_DIR / file_name\n    try:\n        sheets = load_any(file_path)\n    except Exception as e:\n        print(f\"Failed to load {file_name}: {e}\")\n        continue\n    \n    for sheet_name, df in sheets.items():\n        # Defensive: make sure df is actually a DataFrame\n        if not isinstance(df, pd.DataFrame) or df.empty:\n            continue\n        \n        rows_with_peptide = find_sequence_rows(df, PEPTIDE)\n        if rows_with_peptide.empty:\n            continue\n        \n        ids = extract_id_cols(rows_with_peptide)\n        sample_ids.update(map(clean_id, ids))\n\nsample_ids = {sid for sid in sample_ids if sid}  # remove empty strings\n\n# ------------------------------------------------------------------\n# SUBTASK-2 \u2013 lookup histological grades for those sample IDs\n# ------------------------------------------------------------------\nhistological_grades = set()\nfor file_name in FILE_NAMES:\n    file_path = DATA_DIR / file_name\n    try:\n        sheets = load_any(file_path)\n    except Exception:\n        continue\n    \n    for sheet_name, df in sheets.items():\n        if not isinstance(df, pd.DataFrame) or df.empty:\n            continue\n        \n        # Identify possible ID columns and Grade columns\n        id_cols = [\n            c for c in df.columns\n            if any(key in str(c).lower() for key in [\"sample\", \"tumor\", \"id\", \"barcode\"])\n        ]\n        grade_cols = [\n            c for c in df.columns\n            if any(key in str(c).lower() for key in [\"histological grade\", \"grade\", \"histology grade\"])\n        ]\n        if not id_cols or not grade_cols:\n            continue\n        \n        # Standardise IDs for comparison\n        for col in id_cols:\n            df[col] = df[col].astype(str).apply(clean_id)\n        \n        # Any row with an ID of interest?\n        mask = df[id_cols].apply(lambda row: any(x in sample_ids for x in row), axis=1)\n        if not mask.any():\n            continue\n        \n        candidate_grades = df.loc[mask, grade_cols].values.flatten()\n        clean_grades = [str(g).strip() for g in candidate_grades if str(g).strip() and str(g).strip().lower() != \"nan\"]\n        histological_grades.update(clean_grades)\n\n# ------------------------------------------------------------------\n# PRINT ANSWERS\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": sorted(sample_ids),\n    \"subtask-2\": sorted(histological_grades),\n    \"main-task\": sorted(histological_grades)  # final answer is the same as subtask-2 list\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 356.4904487133026
      }
    ],
    "runtime": 37.52733039855957
  },
  {
    "task_id": "biomedical-hard-5",
    "model_output": {
      "id": "main-task",
      "query": "What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places.",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc7.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Locate the sheet(s) that contain, at the sample level, both a per-sample mutation-burden column (e.g. \u201cvariants per Mbp\u201d, \u201cSNVs/Mbp\u201d, \u201cmutations per Mb\u201d, etc.) and a column describing the tumour histology/sub-type (e.g. containing the word \u201cserous\u201d).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ]
        },
        {
          "id": "subtask-2",
          "query": "From the sheet(s) identified in subtask-1, keep only those rows whose histology / tumour-type column contains the string \u201cserous\u201d (case-insensitive, leading/trailing white-space stripped).",
          "data_sources": [
            "same-sheet(s) as returned by subtask-1"
          ]
        },
        {
          "id": "subtask-3",
          "query": "Cast the mutation-burden column selected in subtask-1 to numeric (coercing non-convertible values to NaN) and drop rows where that numeric value is missing.",
          "data_sources": [
            "subset of the sheet(s) coming from subtask-2"
          ]
        },
        {
          "id": "subtask-4",
          "query": "Compute the median of the cleaned numeric mutation-burden values obtained in subtask-3 and round the result to 4 decimal places.",
          "data_sources": [
            "numeric series produced in subtask-3"
          ]
        }
      ],
      "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-5/_intermediate/errors-0.txt"
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\nimport numpy as np\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef load_all_sheets(xlsx_path):\n    \"\"\"Return a dict {sheet_name: dataframe} for *all* sheets in an xlsx.\"\"\"\n    return pd.read_excel(xlsx_path, sheet_name=None, engine=\"openpyxl\")\n\ndef find_candidate_columns(df, burden_keywords, hist_keywords):\n    \"\"\"\n    Return (mutation_burden_column, histology_column) tuples that satisfy\n    keyword criteria.  Empty list if none found in the dataframe.\n    \"\"\"\n    burden_col = None\n    hist_col = None\n    \n    for col in df.columns:\n        col_lc = col.lower()\n        if any(kw in col_lc for kw in burden_keywords):\n            burden_col = col\n        if any(kw in col_lc for kw in hist_keywords):\n            hist_col = col\n    if burden_col and hist_col:\n        return (burden_col, hist_col)\n    return None\n\n# ------------------------------------------------------------------\n# constants\n# ------------------------------------------------------------------\nDATA_FILES = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\"\n]\n\nBURDEN_KEYWORDS = [\"variant\", \"mutation\", \"snv\", \"mutations\", \"variants\", \"mutburden\"]\nBURDEN_KEYWORDS = [f\"{kw} per\" for kw in BURDEN_KEYWORDS] + [\"per mb\", \"per mbp\", \"/mb\", \"/mbp\"]\nHIST_KEYWORDS = [\"histology\", \"tumor subtype\", \"tumour subtype\", \"subtype\", \"tumor type\", \"tumour type\", \"histologic\", \"histopathology\", \"histological\", \"tumour\", \"tumor\"]\n\n# ------------------------------------------------------------------\n# subtask-1 \u2013 locate sheet(s) and the relevant columns\n# ------------------------------------------------------------------\ncandidate_info = []   # list of (file, sheet, burden_col, hist_col)\n\nfor xlsx in DATA_FILES:\n    sheets = load_all_sheets(xlsx)\n    for sh_name, df in sheets.items():\n        cand = find_candidate_columns(df, BURDEN_KEYWORDS, HIST_KEYWORDS)\n        if cand:\n            candidate_info.append({\n                \"file\": Path(xlsx).name,\n                \"sheet\": sh_name,\n                \"burden_col\": cand[0],\n                \"hist_col\": cand[1]\n            })\n\n# Keep only the *first* plausible sheet (usually there will be just one), but\n# collect all if multiple\nif not candidate_info:\n    raise RuntimeError(\"Could not locate a sheet containing both histology and variants/Mb columns.\")\n\n# ------------------------------------------------------------------\n# subtask-2 \u2013 filter to serous\n# ------------------------------------------------------------------\nserous_frames = []\nfor info in candidate_info:\n    df = pd.read_excel(\n        f\"/home/gridsan/elai/Kramabench/data/biomedical/input/{info['file']}\",\n        sheet_name=info['sheet'],\n        engine=\"openpyxl\"\n    )\n    hist_series = df[info['hist_col']].astype(str).str.strip().str.lower()\n    serous_mask = hist_series.str.contains(\"serous\", na=False)\n    serous_df = df.loc[serous_mask, [info['burden_col'], info['hist_col']]]\n    if not serous_df.empty:\n        serous_df[\"burden_col_name\"] = info[\"burden_col\"]   # keep track\n        serous_frames.append(serous_df)\n\nif not serous_frames:\n    raise RuntimeError(\"No rows with 'serous' histology were found.\")\n\nserous_all = pd.concat(serous_frames, ignore_index=True)\n\n# ------------------------------------------------------------------\n# subtask-3 \u2013 cast the burden column to numeric\n# ------------------------------------------------------------------\n# because our dataframe might come from different sheets with different column names,\n# we normalise it to a single numeric column called 'variants_per_mbp'\ndef to_numeric_clean(s):\n    # remove potential commas or other thousands separators\n    return pd.to_numeric(s.astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n\nserous_all[\"variants_per_mbp\"] = to_numeric_clean(serous_all[serous_all[\"burden_col_name\"].iloc[0]])\n# If more than one distinct burden column present, handle them separately\nif serous_all[\"burden_col_name\"].nunique() > 1:\n    numeric_vals = []\n    for name, grp in serous_all.groupby(\"burden_col_name\"):\n        numeric_vals.append(to_numeric_clean(grp[name]))\n    serous_all[\"variants_per_mbp\"] = pd.concat(numeric_vals, ignore_index=True)\n\nserous_clean = serous_all.dropna(subset=[\"variants_per_mbp\"])\n\n# ------------------------------------------------------------------\n# subtask-4 \u2013 compute the median\n# ------------------------------------------------------------------\nmedian_val = float(serous_clean[\"variants_per_mbp\"].median())\nmedian_rounded = round(median_val, 4)\n\n# ------------------------------------------------------------------\n# print answers\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": [   # sheet(s) found\n        {\n            \"file\": info[\"file\"],\n            \"sheet\": info[\"sheet\"],\n            \"mutation_burden_column\": info[\"burden_col\"],\n            \"histology_column\": info[\"hist_col\"]\n        } for info in candidate_info\n    ],\n    \"subtask-2\": {\n        \"n_serous_rows_kept\": int(serous_all.shape[0])\n    },\n    \"subtask-3\": {\n        \"n_serous_rows_with_numeric_burden\": int(serous_clean.shape[0])\n    },\n    \"main-task\": median_rounded\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-5-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What files contain clinical data and mutation burden data? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which files among the provided data sources contain clinical data?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "hyperactivated.csv"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which files among the provided data sources contain mutation-burden (variants per Mbp) data?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "hyperactivated.csv"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            }
          ],
          "answer": {
            "clinical_data_files": [
              "1-s2.0-S0092867420301070-mmc1.xlsx"
            ],
            "mutation_burden_files": [
              "1-s2.0-S0092867420301070-mmc1.xlsx"
            ]
          }
        },
        "code": "import pandas as pd\nimport json\nimport os\nimport re\n\n# -------------------------------------------------------------\n# helper functions\n# -------------------------------------------------------------\ndef classify_file(path):\n    \"\"\"\n    Inspect the first sheet (or the CSV) of the file and decide whether\n    it contains clinical information and/or mutation-burden information.\n    Returns two booleans: (has_clinical, has_mut_burden)\n    \"\"\"\n    clinical_keywords = [\n        'age', 'stage', 'histology', 'grade', 'sex', 'gender',\n        'tumor', 'clinical', 'sample id', 'patient id'\n    ]\n    burden_keywords = [\n        'mutation burden', 'mutational burden', 'mutations per',\n        'variants per', 'mbp', 'mutations/mb', 'tmb'\n    ]\n\n    has_clinical = False\n    has_mut_burden = False\n\n    name = os.path.basename(path)\n    try:\n        if path.endswith('.csv'):\n            df = pd.read_csv(path, nrows=10)\n            colnames = [c.lower() for c in df.columns.astype(str)]\n        else:                       # Excel\n            xls = pd.ExcelFile(path)\n            # read only the first sheet for speed\n            df = pd.read_excel(path, sheet_name=xls.sheet_names[0], nrows=10)\n            colnames = [c.lower() for c in df.columns.astype(str)]\n    except Exception as e:\n        # If the file cannot be parsed, return False, False\n        print(f'Could not read {name}: {e}')\n        return has_clinical, has_mut_burden\n\n    joined_cols = \" \".join(colnames)   # convenient for substring search\n\n    # clinical?\n    for kw in clinical_keywords:\n        if re.search(r'\\b{}\\b'.format(re.escape(kw)), joined_cols):\n            has_clinical = True\n            break\n\n    # mutation burden?\n    for kw in burden_keywords:\n        if kw in joined_cols:\n            has_mut_burden = True\n            break\n\n    return has_clinical, has_mut_burden\n\n\n# -------------------------------------------------------------\n# main routine\n# -------------------------------------------------------------\ndata_paths = [\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx'\n]\n\nclinical_files = []\nburden_files  = []\n\nfor p in data_paths:\n    clinical, burden = classify_file(p)\n    if clinical:\n        clinical_files.append(os.path.basename(p))\n    if burden:\n        burden_files.append(os.path.basename(p))\n\n# -------------------------------------------------------------\n# Prepare answers\n# -------------------------------------------------------------\nanswers = {\n    \"subtask-1\": clinical_files,\n    \"subtask-2\": burden_files,\n    \"main-task\": {\n        \"clinical_data_files\": clinical_files,\n        \"mutation_burden_files\": burden_files\n    }\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 55.14240550994873
      },
      {
        "task_id": "biomedical-hard-5-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What cases are included in the study and have a serous histologic type? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Identify the sheet(s) in the Excel workbook that contain sample-level metadata (case ID, histologic type, etc.).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "From the identified sheet(s), locate the column that stores the histologic type information, taking care of potential variations in column name spelling or capitalisation.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "UCEC_CPTAC3_meta_table_V2.1": "Histologic_Grade_FIGO"
              }
            },
            {
              "id": "subtask-3",
              "query": "Standardise the histologic-type strings (e.g., lower-case, strip spaces) and filter the rows whose histologic type contains the word \u201cserous\u201d.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": 0
            },
            {
              "id": "subtask-4",
              "query": "Return the unique case identifiers (e.g., Case ID or Sample ID column) corresponding to the filtered (serous) rows.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# Utility functions\n# -------------------------------------------------\ndef load_excel_sheets(path):\n    \"\"\"\n    Return a dict mapping sheet name -> DataFrame.\n    \"\"\"\n    xl = pd.ExcelFile(path)\n    sheets = {}\n    for sh in xl.sheet_names:\n        try:\n            sheets[sh] = xl.parse(sh, header=0)  # keep first row as header\n        except Exception as e:\n            # Some sheets may raise due to unsupported formats; skip them\n            print(f\"Could not parse sheet '{sh}': {e}\")\n    return sheets\n\n\ndef find_histology_column(df):\n    \"\"\"\n    Return the column name that most likely stores the histologic type.\n    \"\"\"\n    for col in df.columns:\n        if isinstance(col, str) and (\"histologic\" in col.lower() or \"histology\" in col.lower() or \"histotype\" in col.lower()):\n            return col\n    return None\n\n\ndef find_case_id_column(df):\n    \"\"\"\n    Heuristically find a column with case / sample identifiers.\n    \"\"\"\n    for col in df.columns:\n        if isinstance(col, str):\n            lowered = col.lower()\n            if any(key in lowered for key in [\"case\", \"sample\", \"tumor\", \"id\"]):\n                return col\n    return None\n\n\n# -------------------------------------------------\n# Main logic following the subtasks\n# -------------------------------------------------\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# -------- subtask-1: read sheets and pick the ones with metadata --------\nsheets = load_excel_sheets(file_path)\n\nmetadata_sheets = {}\nfor name, df in sheets.items():\n    if df.empty:\n        continue\n    hist_col = find_histology_column(df)\n    if hist_col is not None:\n        metadata_sheets[name] = df\n\n# -------- subtask-2 & subtask-3: locate histology column, standardise, filter --------\nserous_rows = []\nfor name, df in metadata_sheets.items():\n    hist_col = find_histology_column(df)\n    if hist_col is None:\n        continue\n\n    # normalise histology strings\n    df[hist_col] = df[hist_col].astype(str).str.strip().str.lower()\n\n    # filter serous rows (substring match)\n    mask = df[hist_col].str.contains(\"serous\", na=False)\n    if mask.any():\n        serous_rows.append(df.loc[mask].copy())\n\n# Combine all serous rows\nif serous_rows:\n    serous_df = pd.concat(serous_rows, ignore_index=True, sort=False)\nelse:\n    serous_df = pd.DataFrame()  # empty placeholder\n\n# -------- subtask-4: extract unique case IDs --------\ncase_ids = []\nif not serous_df.empty:\n    cid_col = find_case_id_column(serous_df)\n    if cid_col is not None:\n        case_ids = (\n            serous_df[cid_col]\n            .astype(str)\n            .str.strip()\n            .dropna()\n            .unique()\n            .tolist()\n        )\n\n# -------------------------------------------------\n# Output answers in the required JSON mapping\n# -------------------------------------------------\nanswers = {\n    \"subtask-1\": list(metadata_sheets.keys()),\n    \"subtask-2\": {name: find_histology_column(df) for name, df in metadata_sheets.items()},\n    \"subtask-3\": len(serous_df),             # number of rows classified as serous\n    \"subtask-4\": case_ids,\n    \"main-task\": case_ids                    # for this intermediate question, same as subtask-4\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 25.65594458580017
      },
      {
        "task_id": "biomedical-hard-5-3",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What are the 'Log2_variant_per_Mbp' values for the serous cases? ",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Load every sheet in 1-s2.0-S0092867420301070-mmc7.xlsx and inspect the header rows so we know which sheet contains the column \u2018Log2_variant_per_Mbp\u2019 (case-insensitive) and a histology column that can identify the serous cases. Return the sheet name(s) and the exact column names.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the identified sheet, standardise the data types: \n  \u2022 Trim surrounding white-spaces from all string values.\n  \u2022 Convert the histology column to lower-case strings so we can match \u2018serous\u2019 robustly.\n  \u2022 Coerce the \u2018Log2_variant_per_Mbp\u2019 column to numeric, forcing non-numeric entries to NaN.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-3",
              "query": "Filter the rows whose histology value is exactly \u2018serous\u2019. Extract the cleaned numeric vector of \u2018Log2_variant_per_Mbp\u2019 values (dropping NaNs).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-4",
              "query": "Return that vector so the next step (outside the scope of this question) can be used to compute the median variants per Mbp. (For this question we only need the vector itself).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-5-3/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# helper: read every sheet and look for the target\n# -------------------------------------------------\nxlsx_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx')\nxls = pd.ExcelFile(xlsx_path)\n\ntarget_sheet    = None\nhistology_col   = None\nlog2_var_col    = None\n\nfor sheet_name in xls.sheet_names:\n    df = pd.read_excel(xlsx_path, sheet_name=sheet_name, nrows=5)  # read a few rows just to inspect\n    cols_normalised = [c.strip().lower() for c in df.columns]\n    \n    # look for columns that look like histology and log2_variant_per_mbp\n    possible_histology = [c for c in cols_normalised if 'histology' in c or 'tumor_type' in c or 'histotype' in c]\n    possible_log2      = [c for c in cols_normalised if 'log2' in c and 'variant' in c and 'mbp' in c]\n    \n    if possible_histology and possible_log2:\n        target_sheet  = sheet_name\n        histology_col = df.columns[cols_normalised.index(possible_histology[0])]\n        log2_var_col  = df.columns[cols_normalised.index(possible_log2[0])]\n        break\n\nif target_sheet is None:\n    raise RuntimeError(\"Could not locate the required columns in any sheet\")\n\n# -------------------------------------------------\n# full read of the identified sheet\n# -------------------------------------------------\ndf = pd.read_excel(xlsx_path, sheet_name=target_sheet)\n\n# standardise strings in the histology column\ndf[histology_col] = df[histology_col].astype(str).str.strip().str.lower()\n\n# coerce the log2 variant column to numeric\ndf[log2_var_col] = pd.to_numeric(df[log2_var_col], errors='coerce')\n\n# filter serous cases\nserous_df = df[df[histology_col] == 'serous']\n\n# drop missing values and get the vector\nserous_log2_values = serous_df[log2_var_col].dropna().tolist()\n\n# prepare answers for each subtask ------------------------------------------\nanswers = {\n    \"subtask-1\": {\n        \"sheet\": target_sheet,\n        \"histology_col\": histology_col,\n        \"log2_variant_per_mbp_col\": log2_var_col\n    },\n    \"subtask-2\": \"Data types standardised and coercions applied\",\n    \"subtask-3\": f\"{len(serous_log2_values)} serous rows extracted\",\n    \"subtask-4\": serous_log2_values,\n    \"main-task\": serous_log2_values\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 70.58619904518127
      },
      {
        "task_id": "biomedical-hard-5-7",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What are the linearized values for the variantper Mbp ? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) and column name(s) in 1-s2.0-S0092867420301070-mmc7.xlsx contain the information on histological subtype (e.g., \u201cSerous\u201d) and the number of variants per Mbp (e.g., \u201cVariants/Mbp\u201d, \u201cVariants_per_Mbp\u201d, or similar)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "After locating the correct sheet and columns, extract all rows where the histological subtype equals \"Serous\" (case-insensitive, stripping leading/trailing space, handling missing values).  What are the cleaned numeric values of variants per Mbp for these rows?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-3",
              "query": "Provide the list (linearized vector) of the variants-per-Mbp values obtained in subtask-2.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-4",
              "query": "What is the median of the variants-per-Mbp values from subtask-2, rounded to 4 decimal places?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-5-7/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------------\ndef find_relevant_sheet_and_columns(xls_path):\n    \"\"\"\n    Return (sheet_name, subtype_col, vpm_col)\n    where:\n        sheet_name  \u2013 sheet containing both subtype and variant/Mbp info\n        subtype_col \u2013 column that holds histological subtype / tumor type\n        vpm_col     \u2013 column holding 'variants per Mbp'\n    \"\"\"\n    xl = pd.ExcelFile(xls_path)\n    for sheet in xl.sheet_names:\n        df = xl.parse(sheet, nrows=5)     # peek first few rows\n        lower_cols = [c.lower() if isinstance(c, str) else \"\" for c in df.columns]\n        # candidate columns\n        subtype_candidates = [c for c in df.columns if isinstance(c, str) and any(k in c.lower() for k in [\"histology\", \"subtype\", \"tumour type\", \"tumor type\", \"histological subtype\"])]\n        vpm_candidates     = [c for c in df.columns if isinstance(c, str) and (\"mbp\" in c.lower() and \"variant\" in c.lower())]\n        if subtype_candidates and vpm_candidates:\n            return sheet, subtype_candidates[0], vpm_candidates[0]\n    raise RuntimeError(\"Could not locate required sheet/columns.\")\n\ndef clean_series_to_numeric(s):\n    \"\"\"\n    Convert a pandas Series to numeric, coercing errors, stripping \n    commas, etc., and dropping missing values.\n    \"\"\"\n    # remove commas, spaces\n    s = s.astype(str).str.replace(\",\", \"\").str.strip()\n    return pd.to_numeric(s, errors=\"coerce\").dropna()\n\n# ------------------------------------------------------------------\n# File path\n# ------------------------------------------------------------------\nxls_file = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\")\n\n# ------------------------------------------------------------------\n# subtask-1 \u2013 find sheet & columns\n# ------------------------------------------------------------------\nsheet, subtype_col, vpm_col = find_relevant_sheet_and_columns(xls_file)\n\n# ------------------------------------------------------------------\n# subtask-2 \u2013 extract rows with subtype == 'Serous'\n# ------------------------------------------------------------------\ndf_full = pd.read_excel(xls_file, sheet_name=sheet)\n# Standardise column names lookup\ndf_full.columns = [c.strip() if isinstance(c, str) else c for c in df_full.columns]\nserous_mask = df_full[subtype_col].astype(str).str.strip().str.lower() == \"serous\"\nvpm_values_raw = df_full.loc[serous_mask, vpm_col]\n\n# ------------------------------------------------------------------\n# subtask-3 \u2013 clean and linearise\n# ------------------------------------------------------------------\nvpm_values = clean_series_to_numeric(vpm_values_raw).tolist()\n\n# ------------------------------------------------------------------\n# subtask-4 \u2013 compute median, round to 4 d.p.\n# ------------------------------------------------------------------\nmedian_vpm = round(pd.Series(vpm_values).median(), 4) if vpm_values else None\n\n# ------------------------------------------------------------------\n# Print answers for each task\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": {\n        \"sheet\": sheet,\n        \"histology_column\": subtype_col,\n        \"variants_per_mbp_column\": vpm_col\n    },\n    \"subtask-2\": vpm_values,          # cleaned list already corresponds to serous\n    \"subtask-3\": vpm_values,          # identical output expected (linearised list)\n    \"subtask-4\": median_vpm,\n    \"main-task\": {                    # both linearised list + median (rounded)\n        \"variants_per_mbp_values\": vpm_values,\n        \"median_variants_per_mbp\": median_vpm\n    }\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 50.63565111160278
      },
      {
        "task_id": "biomedical-hard-5-8",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What is the median number of variant per Mbp for serous samples? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet (or sheets) in 1-s2.0-S0092867420301070-mmc7.xlsx contains a column that looks like variants per Mbp?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "B-APM subtypes"
              ]
            },
            {
              "id": "subtask-2",
              "query": "What is the exact column name that stores the value \u2018variants per Mbp\u2019 (case-insensitive) in the identified sheet(s)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "Log2_variant_per_Mbp"
            },
            {
              "id": "subtask-3",
              "query": "After loading the appropriate sheet, what are the rows whose histology / subtype equals \u2018serous\u2019 (case-insensitive, strip surrounding white-space)? Return their values in the variants-per-Mbp column as a clean numeric Series (drop NaN, cast to float).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-4",
              "query": "Compute the median of the cleaned numeric Series from subtask-3 and round it to 4 decimal places.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": null
            }
          ],
          "answer": null
        },
        "code": "import json\nimport pandas as pd\nimport re\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef find_sheets_with_variants_per_mbp(xls_file):\n    \"\"\"\n    Return a list of sheet names that contain a column with a name\n    matching 'variant' and 'mbp' (case-insensitive).\n    \"\"\"\n    xls = pd.ExcelFile(xls_file)\n    target_sheets = []\n    for sheet in xls.sheet_names:\n        df_head = xls.parse(sheet, nrows=5)    # read only head to save time\n        col_candidates = [\n            c for c in df_head.columns\n            if isinstance(c, str)\n            and re.search(r'variant', c, flags=re.I)\n            and re.search(r'mbp', c, flags=re.I)\n        ]\n        if col_candidates:\n            target_sheets.append(sheet)\n    return target_sheets\n\ndef find_variants_per_mbp_column(df):\n    \"\"\"\n    Return the exact column name within df that holds 'variants per Mbp'.\n    \"\"\"\n    for c in df.columns:\n        if isinstance(c, str) and re.search(r'variant', c, flags=re.I) and re.search(r'mbp', c, flags=re.I):\n            return c\n    raise ValueError(\"No variants/Mbp column found\")\n\ndef find_histology_column(df):\n    \"\"\"\n    Return the best-guess column name describing histology / subtype.\n    \"\"\"\n    for pattern in ['histology', 'subtype', 'tumour', 'tumor', 'type']:\n        for c in df.columns:\n            if isinstance(c, str) and re.search(pattern, c, flags=re.I):\n                return c\n    raise ValueError(\"No histology column found\")\n\n# ------------------------------------------------------------------\n# locate the Excel file\n# ------------------------------------------------------------------\nxls_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\")\n\n# ----------------------------- subtask-1 ---------------------------\nsheets_with_var_mbp = find_sheets_with_variants_per_mbp(xls_path)\n\n# For this data set, there should be only one such sheet, but we keep the logic generic.\n# Treat the first matched sheet as the primary sheet.\nprimary_sheet = sheets_with_var_mbp[0] if sheets_with_var_mbp else None\n\n# ----------------------------- subtask-2 ---------------------------\n# load the full sheet now\ndf_full = pd.read_excel(xls_path, sheet_name=primary_sheet)\n\nvar_mbp_col = find_variants_per_mbp_column(df_full)\nhistology_col = find_histology_column(df_full)\n\n# ----------------------------- subtask-3 ---------------------------\n# Clean both columns: strip, lower-case for comparison\nhistology_series = (\n    df_full[histology_col]\n    .astype(str)\n    .str.strip()\n    .str.lower()\n)\n\nserous_mask = histology_series == \"serous\"\nvariants_serous = (\n    df_full.loc[serous_mask, var_mbp_col]\n    .apply(pd.to_numeric, errors=\"coerce\")\n    .dropna()\n    .astype(float)\n)\n\n# ----------------------------- subtask-4 ---------------------------\nmedian_val = round(variants_serous.median(), 4)\n\n# ----------------------------- printing all answers ---------------\nanswers = {\n    \"subtask-1\": sheets_with_var_mbp,\n    \"subtask-2\": var_mbp_col,\n    \"subtask-3\": variants_serous.tolist(),   # raw values after cleaning\n    \"subtask-4\": median_val,\n    \"main-task\": median_val\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 24.054540157318115
      }
    ],
    "runtime": 51.754643201828
  },
  {
    "task_id": "biomedical-easy-6",
    "model_output": {
      "id": "main-task",
      "query": "What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Which sheet(s) in 1-s2.0-S0092867420301070-mmc1.xlsx contain the columns needed to answer the main question (i.e., have both an age-related column and a FIGO_stage column)?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        },
        {
          "id": "subtask-2",
          "query": "Load the sheet(s) identified in subtask-1 and standardise the column names so that (a) the column representing patient age is called Age and (b) the histology grade column is called FIGO_stage.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        },
        {
          "id": "subtask-3",
          "query": "Clean the Age column: convert values to numeric, coerce errors to NaN, and drop rows with missing ages.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        },
        {
          "id": "subtask-4",
          "query": "Filter the dataframe for Age > 70, drop rows with missing FIGO_stage, and compute the most frequent (mode) FIGO_stage value among the remaining rows.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        }
      ],
      "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-easy-6/_intermediate/errors-0.txt"
    },
    "code": "import json\nimport pandas as pd\nfrom collections import Counter\nfrom pathlib import Path\n\n# -------------------------------------------------\n# Helper functions\n# -------------------------------------------------\ndef standardise_columns(df):\n    \"\"\"Return dataframe with unified column names Age and FIGO_stage if possible.\"\"\"\n    rename_map = {}\n    for col in df.columns:\n        lc = col.lower()\n        if 'age' in lc and 'median' not in lc:   # crude but works for most age cols\n            rename_map[col] = 'Age'\n        if 'figo' in lc and 'stage' in lc:\n            rename_map[col] = 'FIGO_stage'\n        if 'histology' in lc and 'grade' in lc:\n            rename_map[col] = 'FIGO_stage'\n    df = df.rename(columns=rename_map)\n    return df\n\n# -------------------------------------------------\n# Subtask-1: find sheet(s) that have both Age and FIGO_stage columns\n# -------------------------------------------------\nexcel_path = '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx'\nxls = pd.ExcelFile(excel_path)\ncandidate_sheets = []\n\nfor sheet in xls.sheet_names:\n    try:\n        tmp = xls.parse(sheet, nrows=5)     # peek the first few rows\n        tmp = standardise_columns(tmp)\n        if {'Age', 'FIGO_stage'}.issubset(tmp.columns):\n            candidate_sheets.append(sheet)\n    except Exception:\n        # Some sheets may not be tabular, just skip\n        continue\n\n# -------------------------------------------------\n# Subtask-2: load and standardise columns\n# -------------------------------------------------\ndf_list = []\nfor sheet in candidate_sheets:\n    try:\n        df = xls.parse(sheet)\n        df = standardise_columns(df)\n        df_list.append(df[['Age', 'FIGO_stage']])  # keep only the columns we need\n    except Exception:\n        continue\n\ndf_combined = pd.concat(df_list, ignore_index=True)\n\n# -------------------------------------------------\n# Subtask-3: clean Age column\n# -------------------------------------------------\ndf_combined['Age'] = pd.to_numeric(df_combined['Age'], errors='coerce')\ndf_clean_age = df_combined.dropna(subset=['Age'])\n\n# -------------------------------------------------\n# Subtask-4: filter Age>70 and compute mode of FIGO_stage\n# -------------------------------------------------\ndf_over70 = df_clean_age[df_clean_age['Age'] > 70]\ndf_over70 = df_over70.dropna(subset=['FIGO_stage'])\n\n# Handle case-sensitivity / stray spaces in FIGO_stage values\ndf_over70['FIGO_stage'] = df_over70['FIGO_stage'].astype(str).str.strip()\n\nif df_over70.empty:\n    most_common_figo = None\nelse:\n    counts = Counter(df_over70['FIGO_stage'])\n    most_common_figo = counts.most_common(1)[0][0]\n\n# ---------------------------------\n# Collect answers for printing\n# ---------------------------------\nanswers = {\n    \"subtask-1\": candidate_sheets,\n    \"subtask-2\": df_combined.columns.tolist(),\n    \"subtask-3\": len(df_over70),   # number of patients over 70 with valid data\n    \"subtask-4\": most_common_figo,\n    \"main-task\": most_common_figo\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-easy-6-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n What is the total number of cases in the loaded clinical dataset? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Load the Excel workbook, list all available sheets, and visually inspect (programmatically) which sheet(s) contain clinical-level information (e.g., a column that mentions \u201cFIGO\u201d or a patient/sample identifier).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "sheet_names": [
                  "UCEC_CPTAC3_meta_table_V2.1"
                ],
                "chosen_clinical_sheet": "UCEC_CPTAC3_meta_table_V2.1"
              }
            },
            {
              "id": "subtask-2",
              "query": "Within the identified clinical sheet, determine the most appropriate column that uniquely represents each case (e.g., `case_id`, `Sample`, `Patient_ID`).  If no obvious unique identifier exists, fall back to counting the number of rows after dropping exact-duplicate rows.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "chosen_id_column": null
              }
            },
            {
              "id": "subtask-3",
              "query": "After cleaning (casting column names to string, handling missing values, dropping duplicates), compute the total number of cases in the clinical data set.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "total_cases": 153
              }
            }
          ],
          "answer": 153
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# CONFIG\n# ------------------------------------------------------------------\nDATA_FILE = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# ------------------------------------------------------------------\n# SUBTASK-1 \u2013 Load workbook & list sheets\n# ------------------------------------------------------------------\nxls = pd.ExcelFile(DATA_FILE)\nsheet_names = xls.sheet_names  # list of sheet names\n\n# Attempt to locate the clinical sheet by searching for FIGO/clinical-type columns\nclinical_df = None\nclinical_sheet_name = None\nfor name in sheet_names:\n    df_tmp = pd.read_excel(xls, sheet_name=name)\n    cols_lower = [str(c).lower() for c in df_tmp.columns]\n    if any(\"figo\" in c for c in cols_lower) or any(\"stage\" in c for c in cols_lower):\n        clinical_df = df_tmp.copy()\n        clinical_sheet_name = name\n        break\n\n# If none were found, default to the first sheet\nif clinical_df is None:\n    clinical_sheet_name = sheet_names[0]\n    clinical_df = pd.read_excel(xls, sheet_name=clinical_sheet_name)\n\n# ------------------------------------------------------------------\n# SUBTASK-2 \u2013 Identify unique-case column\n# ------------------------------------------------------------------\n# A few candidate column names that typically uniquely identify cases\ncandidate_cols = [\"case\", \"case_id\", \"patient_id\", \"sample_id\", \"sample\", \"id\"]\ncandidate_cols_lower = [c.lower() for c in candidate_cols]\n\nchosen_id_col = None\nfor col in clinical_df.columns:\n    if str(col).lower() in candidate_cols_lower:\n        chosen_id_col = col\n        break\n\n# If an id column exists, coerce to string for reliability\nif chosen_id_col:\n    # Cast to string because mixture of numeric/string is common\n    clinical_df[chosen_id_col] = clinical_df[chosen_id_col].astype(str)\n\n# ------------------------------------------------------------------\n# SUBTASK-3 \u2013 Total number of cases\n# ------------------------------------------------------------------\nif chosen_id_col:\n    # Drop rows where id is NA or blank\n    df_clean = clinical_df[clinical_df[chosen_id_col].notna() & (clinical_df[chosen_id_col] != \"\")]\n    total_cases = df_clean[chosen_id_col].nunique()\nelse:\n    # If no id column, treat each unique row as a case\n    total_cases = clinical_df.drop_duplicates().shape[0]\n\n# ------------------------------------------------------------------\n# PRINT ANSWERS\n# ------------------------------------------------------------------\nprint(json.dumps(\n    {\n        \"subtask-1\": {\n            \"sheet_names\": sheet_names,\n            \"chosen_clinical_sheet\": clinical_sheet_name\n        },\n        \"subtask-2\": {\n            \"chosen_id_column\": chosen_id_col\n        },\n        \"subtask-3\": {\n            \"total_cases\": int(total_cases)\n        },\n        \"main-task\": int(total_cases)\n    },\n    indent=4\n))",
        "runtime": 50.31717872619629
      },
      {
        "task_id": "biomedical-easy-6-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n Keep only samples from case in the study. How many cases are included in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the sheet(s) that contain individual-level (sample / patient) metadata, including a column that indicates whether the sample is a \u2018case\u2019 or \u2018control\u2019.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "UCEC_CPTAC3_meta_table_V2.1"
            },
            {
              "id": "subtask-2",
              "query": "Within the identified metadata sheet(s), determine the exact column name that flags a sample as a \u2018case\u2019 (e.g., \u201cCase_Control\u201d, \u201cStatus\u201d, \u201cGroup\u201d, etc.). Standardise the values to upper-case strings so that values such as \u201ccase\u201d, \u201cCase\u201d, \u201cCASE\u201d are treated the same.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Case_excluded"
            },
            {
              "id": "subtask-3",
              "query": "Filter the metadata to retain only rows whose case/control flag equals \u201cCASE\u201d. After filtering, identify the column that uniquely identifies a patient / case (e.g., \u201cCase_ID\u201d, \u201cPatient_ID\u201d, \u201cSample_ID\u201d). Remove rows that have a null value in this identifier column.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": 0
            },
            {
              "id": "subtask-4",
              "query": "Count the number of distinct case identifiers that remain after the filter. This count is the number of cases included in the study.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": 0
            }
          ],
          "answer": "0 cases"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef read_all_sheets(xlsx_path):\n    \"\"\"\n    Read every sheet in an Excel workbook into a dict of DataFrames.\n    Any sheet that cannot be read as a table is skipped.\n    \"\"\"\n    xlsx = pd.ExcelFile(xlsx_path)\n    sheets = {}\n    for name in xlsx.sheet_names:\n        try:\n            df = xlsx.parse(name, dtype=str)          # read as strings first (safest)\n            # drop completely empty rows / columns\n            df.dropna(axis=0, how=\"all\", inplace=True)\n            df.dropna(axis=1, how=\"all\", inplace=True)\n            if not df.empty:\n                sheets[name] = df\n        except Exception as e:\n            # ignore sheets that are not tabular\n            print(f\"Skipping sheet {name}: {e}\")\n    return sheets\n\ndef find_case_column(df):\n    \"\"\"\n    Try to find the column that labels case vs control.\n    A simple heuristic: look for column names containing case, control, status, group.\n    \"\"\"\n    candidates = [c for c in df.columns if any(key in c.lower() for key in [\"case\", \"control\", \"status\", \"group\"])]\n    if not candidates:\n        return None\n    # pick the first candidate that actually has case/control like text in its values\n    for c in candidates:\n        vals = df[c].dropna().astype(str).str.lower()\n        if vals.str.contains(\"case\").any() and vals.str.contains(\"control\").any():\n            return c\n        if vals.str.contains(\"case\").any():\n            return c\n    # fallback to first candidate\n    return candidates[0]\n\ndef find_id_column(df):\n    \"\"\"\n    Try to find the column that looks like a patient or case identifier.\n    \"\"\"\n    for key in [\"case_id\", \"patient_id\", \"sample_id\", \"id\"]:\n        hits = [c for c in df.columns if key in c.lower()]\n        if hits:\n            return hits[0]\n    # fallback to very first column\n    return df.columns[0]\n\n\n# ------------------------------------------------------------------\n# Step-by-step execution following the plan\n# ------------------------------------------------------------------\nxlsx_file = \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\"\nassert Path(xlsx_file).is_file(), f\"Data file {xlsx_file} not found\"\n\n# subtask-1: load every sheet\nsheets = read_all_sheets(xlsx_file)\n\n# heuristically pick the sheet that contains case/control information\nmetadata_sheet_name = None\nmetadata_df = None\ncase_column_name = None\n\nfor name, df in sheets.items():\n    col = find_case_column(df)\n    if col:\n        metadata_sheet_name = name\n        metadata_df = df.copy()\n        case_column_name = col\n        break\n\nif metadata_df is None:\n    raise RuntimeError(\"Could not locate a sheet with Case / Control information\")\n\n# clean and standardise the case column\nmetadata_df[case_column_name] = (\n    metadata_df[case_column_name]\n    .astype(str)\n    .str.strip()\n    .str.upper()\n)\n\n# subtask-3: filter only CASE rows\ncases_df = metadata_df[metadata_df[case_column_name] == \"CASE\"].copy()\n\n# identify the column that uniquely identifies a case / patient\nid_col = find_id_column(cases_df)\n\ncases_df = cases_df[~cases_df[id_col].isna()]\ncases_df[id_col] = cases_df[id_col].astype(str).str.strip()\n\n# subtask-4: count distinct cases\nn_cases = cases_df[id_col].nunique()\n\n# ------------------------------------------------------------------\n# Print answers for every (sub)task\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": metadata_sheet_name,                 # sheet that holds metadata\n    \"subtask-2\": case_column_name,                    # column used to label CASE\n    \"subtask-3\": len(cases_df),                       # number of rows after filtering\n    \"subtask-4\": int(n_cases),                        # distinct case identifiers\n    \"main-task\": f\"{n_cases} cases\"                   # formatted main answer\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 38.77392029762268
      },
      {
        "task_id": "biomedical-easy-6-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n What are the indices of patients aged 70 or older? ",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the column that stores patient age in the spreadsheet (it may be called Age, age_years, age (y), etc.).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Age"
            },
            {
              "id": "subtask-2",
              "query": "Read the age column, coerce the values to numeric (handle strings, missing values, or non-numeric tokens).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Number of valid numeric ages parsed: 101"
            },
            {
              "id": "subtask-3",
              "query": "Return the row indices of the records where age \u2265 70 (after cleaning).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                4,
                8,
                14,
                15,
                23,
                26,
                27,
                37,
                45,
                46,
                47,
                48,
                53,
                59,
                61,
                62,
                78,
                81,
                86,
                88,
                89,
                96,
                99,
                100,
                101,
                102
              ]
            }
          ],
          "answer": [
            4,
            8,
            14,
            15,
            23,
            26,
            27,
            37,
            45,
            46,
            47,
            48,
            53,
            59,
            61,
            62,
            78,
            81,
            86,
            88,
            89,
            96,
            99,
            100,
            101,
            102
          ]
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper: load every sheet and try to find the column that contains age\ndef find_age_column(df):\n    \"\"\"Return the first column name that looks like an age column.\"\"\"\n    age_like = ['age', 'age_years', 'age (y)', 'Age', 'age_year', 'Age_years']\n    for col in df.columns:\n        # normalise: remove spaces, lower case\n        col_clean = col.replace(' ', '').replace('_', '').lower()\n        for candidate in age_like:\n            if col_clean == candidate.replace(' ', '').replace('_', '').lower():\n                return col\n    # also fallback to the first numeric-looking column with plausible range\n    numeric_cols = df.select_dtypes(include='number').columns\n    for col in numeric_cols:\n        series = df[col]\n        if series.dropna().between(0, 120).all():\n            return col\n    return None\n# ------------------------------------------------------------------\n\nfile_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx')\n\nxlsx = pd.ExcelFile(file_path)\nage_column = None\nage_df = None\nsheet_used = None\n\n# iterate over sheets to find age column\nfor sheet in xlsx.sheet_names:\n    df = xlsx.parse(sheet)\n    col = find_age_column(df)\n    if col:\n        age_column = col\n        age_df = df\n        sheet_used = sheet\n        break\n\nif age_column is None:\n    raise ValueError(\"Could not locate an age column in any sheet.\")\n\n# ------------------------------------------------------------------\n# Clean the age column\nages = pd.to_numeric(age_df[age_column], errors='coerce')\n\n# Identify indices with age >= 70\nindices_over_70 = ages[ages >= 70].index.tolist()\n\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": age_column,\n    \"subtask-2\": f\"Number of valid numeric ages parsed: {ages.notna().sum()}\",\n    \"subtask-3\": indices_over_70,\n    \"main-task\": indices_over_70\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 12.948894500732422
      },
      {
        "task_id": "biomedical-easy-6-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n What is the most common 'FIGO' stage in samples from patients above 70? ",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the worksheet(s) that contain patient-level information with the columns for age and FIGO/FIGO_stage. Identify the exact column names and the sheet name(s).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                {
                  "sheet": "UCEC_CPTAC3_meta_table_V2.1",
                  "columns": [
                    "idx",
                    "Proteomics_Participant_ID",
                    "Case_excluded",
                    "Proteomics_TMT_batch",
                    "Proteomics_TMT_plex",
                    "Proteomics_TMT_channel",
                    "Proteomics_Parent_Sample_IDs",
                    "Proteomics_Aliquot_ID",
                    "Proteomics_Tumor_Normal",
                    "Proteomics_OCT",
                    "Country",
                    "Histologic_Grade_FIGO",
                    "Myometrial_invasion_Specify",
                    "Histologic_type",
                    "Treatment_naive",
                    "Tumor_purity",
                    "Path_Stage_Primary_Tumor-pT",
                    "Path_Stage_Reg_Lymph_Nodes-pN",
                    "Clin_Stage_Dist_Mets-cM",
                    "Path_Stage_Dist_Mets-pM",
                    "tumor_Stage-Pathological",
                    "FIGO_stage",
                    "LVSI",
                    "BMI",
                    "Age",
                    "Diabetes",
                    "Race",
                    "Ethnicity",
                    "Gender",
                    "Tumor_Site",
                    "Tumor_Site_Other",
                    "Tumor_Focality",
                    "Tumor_Size_cm",
                    "Estrogen_Receptor",
                    "Estrogen_Receptor_%",
                    "Progesterone_Receptor",
                    "Progesterone_Receptor_%",
                    "MLH1",
                    "MLH2",
                    "MSH6",
                    "PMS2",
                    "p53",
                    "Other_IHC_specify",
                    "MLH1_Promoter_Hypermethylation",
                    "Num_full_term_pregnancies",
                    "EPIC_Bcells",
                    "EPIC_CAFs",
                    "EPIC_CD4_Tcells",
                    "EPIC_CD8_Tcells",
                    "EPIC_Endothelial",
                    "EPIC_Macrophages",
                    "EPIC_NKcells",
                    "EPIC_otherCells",
                    "CIBERSORT_B _cells _naive",
                    "CIBERSORT_B _cells _memory",
                    "CIBERSORT_Plasma _cells",
                    "CIBERSORT_T _cells _CD8",
                    "CIBERSORT_T _cells _CD4 _naive",
                    "CIBERSORT_T _cells _CD4 _memory _resting",
                    "CIBERSORT_T _cells _CD4 _memory _activated",
                    "CIBERSORT_T _cells _follicular _helper",
                    "CIBERSORT_T _cells _regulatory _(Tregs)",
                    "CIBERSORT_T _cells _gamma _delta",
                    "CIBERSORT_NK _cells _resting",
                    "CIBERSORT_NK _cells _activated",
                    "CIBERSORT_Monocytes",
                    "CIBERSORT_Macrophages _M0",
                    "CIBERSORT_Macrophages _M1",
                    "CIBERSORT_Macrophages _M2",
                    "CIBERSORT_Dendritic _cells _resting",
                    "CIBERSORT_Dendritic _cells _activated",
                    "CIBERSORT_Mast _cells _resting",
                    "CIBERSORT_Mast _cells _activated",
                    "CIBERSORT_Eosinophils",
                    "CIBERSORT_Neutrophils",
                    "CIBERSORT_Absolute _score",
                    "ESTIMATE_StromalScore",
                    "ESTIMATE_ImmuneScore",
                    "ESTIMATE_ESTIMATEScore",
                    "Stemness_score",
                    "ER_ESR1",
                    "PR_PGR",
                    "Pathway_activity_EGFR",
                    "Pathway_activity_Hypoxia",
                    "Pathway_activity_JAK.STAT",
                    "Pathway_activity_MAPK",
                    "Pathway_activity_NFkB",
                    "Pathway_activity_PI3K",
                    "Pathway_activity_TGFb",
                    "Pathway_activity_TNFa",
                    "Pathway_activity_Trail",
                    "Pathway_activity_VEGF",
                    "Pathway_activity_p53",
                    "TP53_ATM",
                    "TP53_CHEK2",
                    "TP53_MDM4",
                    "TP53_RPS6KA3",
                    "TP53_TP53",
                    "TP53_pathway",
                    "PI3K_AKT1",
                    "PI3K_AKT2",
                    "PI3K_AKT3",
                    "PI3K_DEPDC5",
                    "PI3K_DEPTOR",
                    "PI3K_INPP4B",
                    "PI3K_MAPKAP1",
                    "PI3K_MLST8",
                    "PI3K_MTOR",
                    "PI3K_NPRL2",
                    "PI3K_NPRL3",
                    "PI3K_PDK1",
                    "PI3K_PIK3CA",
                    "PI3K_PIK3CB",
                    "PI3K_PIK3R1",
                    "PI3K_PIK3R2",
                    "PI3K_PPP2R1A",
                    "PI3K_PTEN",
                    "PI3K_RHEB",
                    "PI3K_RICTOR",
                    "PI3K_RPS6",
                    "PI3K_RPS6KB1",
                    "PI3K_RPTOR",
                    "PI3K_STK11",
                    "PI3K_TSC1",
                    "PI3K_TSC2",
                    "PI3K_pathway",
                    "HRD_BRCA1",
                    "HRD_BRCA2",
                    "HRD_BRCA1_or_BRCA2",
                    "CNV_DEL",
                    "CNV_AMP",
                    "CNV_class",
                    "CNV_idx",
                    "CNV_1q_DEL",
                    "CNV_3q_DEL",
                    "CNV_4q_DEL",
                    "CNV_1q_AMP",
                    "CNV_3q_AMP",
                    "CNV_4q_AMP",
                    "Purity_Immune",
                    "Purity_Cancer",
                    "Purity_Stroma",
                    "MSI_status",
                    "POLE_subtype",
                    "JAK1_MS_INDEL",
                    "JAK1_Mutation",
                    "Log2_variant_per_Mbp",
                    "Log2_SNP_per_Mbp",
                    "Log2_INDEL_per_Mbp",
                    "Log2_variant_total",
                    "Log2_SNP_total",
                    "Log2_INDEL_total",
                    "Genomics_subtype",
                    "Mutation_signature_C>A",
                    "Mutation_signature_C>G",
                    "Mutation_signature_C>T",
                    "Mutation_signature_T>C",
                    "Mutation_signature_T>A",
                    "Mutation_signature_T>G",
                    "WXS_normal_sample_type",
                    "WXS_normal_filename",
                    "WXS_normal_UUID",
                    "WXS_tumor_sample_type",
                    "WXS_tumor_filename",
                    "WXS_tumor_UUID",
                    "WGS_normal_sample_type",
                    "WGS_normal_UUID",
                    "WGS_tumor_sample_type",
                    "WGS_tumor_UUID",
                    "RNAseq_R1_sample_type",
                    "RNAseq_R1_filename",
                    "RNAseq_R1_UUID",
                    "RNAseq_R2_sample_type",
                    "RNAseq_R2_filename",
                    "RNAseq_R2_UUID",
                    "miRNAseq_sample_type",
                    "miRNAseq_UUID",
                    "Methylation_available",
                    "Methylation_quality"
                  ]
                }
              ]
            },
            {
              "id": "subtask-2",
              "query": "Load those worksheet(s), clean the data (strip white-space, harmonise column names to a standard format, coerce Age to numeric, set FIGO/FIGO_stage to string, drop rows with missing Age or FIGO).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Rows after cleaning: 0, columns used -> Age: 'Path_Stage_Primary_Tumor-pT', FIGO: 'Histologic_Grade_FIGO'"
            },
            {
              "id": "subtask-3",
              "query": "From the cleaned data, filter all rows with Age > 70.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Rows with Age > 70: 0"
            },
            {
              "id": "subtask-4",
              "query": "Among the filtered rows, compute the frequency of each FIGO stage value (case-insensitive, trimmed). Identify the most common stage (the mode). If there is a tie, return all tied stages.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "FIGO_mode": null
              }
            }
          ],
          "answer": null
        },
        "code": "import json\nimport pandas as pd\nfrom collections import Counter\nfrom pathlib import Path\n\nxlsx_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# ---------- subtask-1 : Identify sheet(s) & columns ----------\nsheets = pd.ExcelFile(xlsx_path).sheet_names\nage_cols_keywords = {'age', 'patient age', 'age_years'}\nfigo_cols_keywords = {'figo', 'figo_stage', 'stage', 'figo stage'}\ncandidate_info = []\n\nfor sheet in sheets:\n    df_head = pd.read_excel(xlsx_path, sheet_name=sheet, nrows=10)  # small sample just for header scan\n    lower_cols = [c.lower().strip() for c in df_head.columns]\n    has_age = any(any(k in col for k in age_cols_keywords) for col in lower_cols)\n    has_figo = any(any(k in col for k in figo_cols_keywords) for col in lower_cols)\n    if has_age and has_figo:\n        candidate_info.append({\n            \"sheet\": sheet,\n            \"columns\": df_head.columns.tolist()\n        })\n\n# choose the first candidate sheet (assuming only one patient-level sheet)\nif not candidate_info:\n    raise RuntimeError(\"Could not locate suitable sheet with Age and FIGO information\")\n\ntarget_sheet = candidate_info[0]['sheet']\nfull_df = pd.read_excel(xlsx_path, sheet_name=target_sheet)\n\n# ---------- subtask-2 : Clean data ----------\n# harmonise column names\nfull_df.columns = [c.strip() for c in full_df.columns]\n\n# locate age & figo columns again with robust search\ndef find_col(df, keywords):\n    for col in df.columns:\n        if any(k in col.lower() for k in keywords):\n            return col\n    return None\n\nage_col = find_col(full_df, age_cols_keywords)\nfigo_col = find_col(full_df, figo_cols_keywords)\n\nif age_col is None or figo_col is None:\n    raise RuntimeError(\"Failed to find essential columns\")\n\n# Coerce Age to numeric\nfull_df[age_col] = pd.to_numeric(full_df[age_col], errors='coerce')\n\n# Standardise FIGO as string, stripped upper\nfull_df[figo_col] = full_df[figo_col].astype(str).str.strip().str.upper()\n\nclean_df = full_df.dropna(subset=[age_col, figo_col])\n\n# ---------- subtask-3 : Filter Age > 70 ----------\nover70_df = clean_df[clean_df[age_col] > 70]\n\n# ---------- subtask-4 : Mode of FIGO ----------\nfigo_counts = Counter(over70_df[figo_col])\nif not figo_counts:\n    most_common_figo = None\nelse:\n    max_count = max(figo_counts.values())\n    most_common_figo = [stage for stage, cnt in figo_counts.items() if cnt == max_count]\n    # if single value, unwrap\n    most_common_figo = most_common_figo[0] if len(most_common_figo) == 1 else most_common_figo\n\n# ---------- print answers ----------\nprint(json.dumps({\n    \"subtask-1\": candidate_info,\n    \"subtask-2\": f\"Rows after cleaning: {len(clean_df)}, columns used -> Age: '{age_col}', FIGO: '{figo_col}'\",\n    \"subtask-3\": f\"Rows with Age > 70: {len(over70_df)}\",\n    \"subtask-4\": {\"FIGO_mode\": most_common_figo},\n    \"main-task\": most_common_figo\n}, indent=4))",
        "runtime": 19.0360369682312
      }
    ],
    "runtime": 20.641398191452026
  },
  {
    "task_id": "biomedical-hard-7",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "How many are the significant genes by acetylproteomics?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc3.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Which sheet(s) in the Excel file contain acetyl-proteomics data?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": [
            "D-SE-acetyl"
          ]
        },
        {
          "id": "subtask-2",
          "query": "Within the acetyl-proteomics sheet(s), what column(s) indicate that a gene/protein is significant and what rule should be used (e.g., Boolean flag, FDR < 0.05)?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": {
            "D-SE-acetyl": {
              "gene_column": null,
              "significance_column": null,
              "rule": "Not found"
            }
          }
        },
        {
          "id": "subtask-3",
          "query": "After applying the significance rule, how many unique genes/proteins are significant in the acetyl-proteomics data?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": 0
        }
      ],
      "answer": 0
    },
    "code": "import json\nimport re\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------------\ndef to_numeric(series):\n    \"\"\"Convert a pandas Series to numeric, coercing errors to NaN.\"\"\"\n    return pd.to_numeric(series, errors='coerce')\n\ndef find_gene_column(df):\n    \"\"\"\n    Try to locate the column that contains gene symbols.\n    We look for typical names such as 'gene', 'symbol', 'gene name', etc.\n    \"\"\"\n    candidates = [c for c in df.columns \n                  if re.search(r'gene|symbol', str(c), flags=re.I)]\n    return candidates[0] if candidates else None\n\ndef detect_significance_column(df):\n    \"\"\"\n    Try to detect a column that explicitly flags significance.\n    Returns (column_name, rule_function) or (None, None) if not found.\n    \"\"\"\n    # 1. Explicit Boolean / yes-no column\n    for col in df.columns:\n        if re.search(r'significant', str(col), flags=re.I):\n            def bool_rule(s):  # noqa: E306 shadowing quick inline\n                return s.astype(str).str.lower().isin(['true', 'yes', '1'])\n            return col, bool_rule\n    \n    # 2. Adjusted P-value, q-value, FDR\n    for p_col in df.columns:\n        if re.search(r'adj|q[-_ ]?value|fdr', str(p_col), flags=re.I):\n            def p_rule(s):  # noqa: E306\n                return to_numeric(s) < 0.05\n            return p_col, p_rule\n    \n    # 3. Raw P-value (fallback)\n    for p_col in df.columns:\n        if re.fullmatch(r'p[-_ ]?value', str(p_col), flags=re.I):\n            def p_rule(s):  # noqa: E306\n                return to_numeric(s) < 0.05\n            return p_col, p_rule\n    \n    return None, None\n\n# ------------------------------------------------------------------\n# Locate acetyl-proteomics sheets\n# ------------------------------------------------------------------\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\nxls = pd.ExcelFile(file_path)\n\nacetyl_sheets = [name for name in xls.sheet_names if re.search(r'acetyl', name, flags=re.I)]\n\n# If none were found, inspect all sheets for the word \"acetyl\" in any cell of the first row\nif not acetyl_sheets:\n    for name in xls.sheet_names:\n        tmp = pd.read_excel(xls, sheet_name=name, nrows=1, header=None)\n        if tmp.astype(str).apply(lambda col: col.str.contains(r'acetyl', flags=re.I)).any().any():\n            acetyl_sheets.append(name)\n\n# ------------------------------------------------------------------\n# Determine the significance rule & count significant genes\n# ------------------------------------------------------------------\nsignificant_genes = set()\nsignificance_columns_chosen = {}\n\nfor sheet in acetyl_sheets:\n    df = pd.read_excel(xls, sheet_name=sheet)\n    gene_col = find_gene_column(df)\n    sig_col, sig_rule = detect_significance_column(df)\n    \n    # Store what we discovered for reporting back (subtask-2)\n    significance_columns_chosen[sheet] = {\n        \"gene_column\": gene_col,\n        \"significance_column\": sig_col,\n        \"rule\": \"Boolean flag\" if sig_col and 'significant' in sig_col.lower()\n                else \"< 0.05 threshold on p/q/FDR\" if sig_col else \"Not found\"\n    }\n    \n    if gene_col is None or sig_col is None:\n        # Skip this sheet if we can't determine what is a gene or significance\n        continue\n    \n    sig_mask = sig_rule(df[sig_col])\n    # Clean gene symbols\n    genes = df.loc[sig_mask, gene_col].dropna().astype(str).str.strip()\n    significant_genes.update(genes)\n\n# ------------------------------------------------------------------\n# Assemble answers\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": acetyl_sheets,\n    \"subtask-2\": significance_columns_chosen,\n    \"subtask-3\": len(significant_genes),\n    \"main-task\": len(significant_genes)\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-7-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: How many are the significant genes by acetylproteomics?, please answer the following question:\n What file contains acetylproteomics data? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which file(s) among the provided data sources mention the term \u201cacetyl\u201d (e.g., 'acetylproteomics') in their sheet names or column headers?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            }
          ],
          "answer": "1-s2.0-S0092867420301070-mmc3.xlsx"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper function to check for the word \u201cacetyl\u201d in sheet names\n# or column headers (case-insensitive, strip surrounding spaces).\n# ------------------------------------------------------------------\ndef file_mentions_acetyl(file_path):\n    file_path = Path(file_path)\n    acetyl_found = False\n    locations = []          # keep track of where we found it\n\n    try:\n        if file_path.suffix.lower() == \".xlsx\":\n            excel_obj = pd.ExcelFile(file_path)\n            # 1) look in sheet names\n            for sheet in excel_obj.sheet_names:\n                if \"acetyl\" in sheet.lower():\n                    acetyl_found = True\n                    locations.append(f\"sheet:{sheet}\")\n            # 2) look in first sheet\u2019s column headers\n            if not acetyl_found:      # only open if we still did not find\n                df_preview = excel_obj.parse(excel_obj.sheet_names[0], nrows=5)\n                for col in df_preview.columns:\n                    if isinstance(col, str) and \"acetyl\" in col.lower():\n                        acetyl_found = True\n                        locations.append(f\"column:{col}\")\n        elif file_path.suffix.lower() == \".csv\":\n            # only read a small sample\n            df_preview = pd.read_csv(file_path, nrows=5)\n            for col in df_preview.columns:\n                if isinstance(col, str) and \"acetyl\" in col.lower():\n                    acetyl_found = True\n                    locations.append(f\"column:{col}\")\n    except Exception as e:\n        # If the file can't be read, just report the error\n        locations.append(f\"error:{e}\")\n\n    return acetyl_found, locations\n\n# ------------------------------------------------------------------\n# Iterate through all provided data sources and test for \u201cacetyl\u201d\n# ------------------------------------------------------------------\ndata_sources_paths = [\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx'\n]\n\nacetyl_files = {}\nfor f in data_sources_paths:\n    has_acetyl, where = file_mentions_acetyl(f)\n    if has_acetyl:\n        acetyl_files[Path(f).name] = where\n\n# The answer for subtask-1 is the list of file(s) that mention \u201cacetyl\u201d\nsubtask1_answer = list(acetyl_files.keys())\n\n# For the main task, if exactly one file stands out, that is the answer.\n# Otherwise, return all candidates so that a human can manually verify.\nmain_task_answer = subtask1_answer[0] if len(subtask1_answer) == 1 else subtask1_answer\n\nprint(json.dumps(\n    {\n        \"subtask-1\": subtask1_answer,\n        \"main-task\": main_task_answer\n    },\n    indent=4\n))",
        "runtime": 46.80203437805176
      },
      {
        "task_id": "biomedical-hard-7-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: How many are the significant genes by acetylproteomics?, please answer the following question:\n What sheet name corresponds to the acetylproteomics data? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "What are all the sheet names in the Excel file 1-s2.0-S0092867420301070-mmc3.xlsx?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "README",
                "A-MSI",
                "B-SE-proteomics",
                "C-SE-phospho",
                "D-SE-acetyl",
                "E-Mutation-adjacent",
                "F-SS-phospho"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which sheet name (from those found in subtask-1) corresponds to the acetylproteomics data?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "D-SE-acetyl"
            }
          ],
          "answer": "D-SE-acetyl"
        },
        "code": "import json\nimport pandas as pd\n\n# ------------------------------------------------------------------\n# subtask-1: list every sheet in the workbook\n# ------------------------------------------------------------------\nexcel_path = \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\nxls = pd.ExcelFile(excel_path)\nsheet_names = xls.sheet_names   # list of all sheet names\n\n# ------------------------------------------------------------------\n# subtask-2: identify the acetylproteomics sheet\n# We\u2019ll look for names containing the substring \u201cacetyl\u201d (case-insensitive)\n# ------------------------------------------------------------------\nacetyl_sheets = [s for s in sheet_names if \"acetyl\" in s.lower()]\n\n# It is possible there are multiple matches; in most supplemental files\n# there is typically a single clearly labelled sheet, so we\u2019ll pick the\n# first match.  If none are found we\u2019ll leave the answer as None.\nacetyl_sheet_name = acetyl_sheets[0] if acetyl_sheets else None\n\n# ------------------------------------------------------------------\n# Print the answers in the required JSON format\n# ------------------------------------------------------------------\nprint(json.dumps({\n    \"subtask-1\": sheet_names,\n    \"subtask-2\": acetyl_sheet_name,\n    \"main-task\": acetyl_sheet_name\n}, indent=4))",
        "runtime": 27.412081480026245
      },
      {
        "task_id": "biomedical-hard-7-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: How many are the significant genes by acetylproteomics?, please answer the following question:\n What is the total number of entries in the acetylproteomics data sheet, accounting for a header row? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheet in the Excel file contains the acetyl-proteomics data?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "D-SE-acetyl"
            },
            {
              "id": "subtask-2",
              "query": "How many data rows does the acetyl-proteomics worksheet have (excluding the header)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 15
            }
          ],
          "answer": 16
        },
        "code": "import json\nimport pandas as pd\n\nfile_path = \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\n\n# ---------- subtask-1: identify the acetyl-proteomics worksheet ----------\n# Read only the sheet names first\nxls = pd.ExcelFile(file_path)\nsheet_names = xls.sheet_names\n\nacetyl_sheet_name = None\nfor name in sheet_names:\n    if \"acetyl\" in name.lower():\n        acetyl_sheet_name = name\n        break\n\n# Fallback in case the sheet is not clearly named (will take the first)\nif acetyl_sheet_name is None and len(sheet_names) > 0:\n    acetyl_sheet_name = sheet_names[0]\n\n# ---------- subtask-2: count the data rows ----------\ndf = pd.read_excel(file_path, sheet_name=acetyl_sheet_name)\n# Pandas automatically treats the first row as the header.  \n# The number of data rows (excluding header) is therefore:\ndata_rows = df.shape[0]\n\n# ---------- main-task: total entries including header ----------\ntotal_entries_including_header = data_rows + 1   # add one for the header row\n\nprint(json.dumps(\n    {\n        \"subtask-1\": acetyl_sheet_name,\n        \"subtask-2\": data_rows,\n        \"main-task\": total_entries_including_header\n    },\n    indent=4\n))",
        "runtime": 14.920298337936401
      }
    ],
    "runtime": 43.711299419403076
  },
  {
    "task_id": "biomedical-hard-8",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc6.xlsx",
        "hyperactivated.csv"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "What are the protein sites that are hyperactivated in CNV-high endometroid samples?",
          "data_sources": [
            "hyperactivated.csv"
          ],
          "answer": [
            "S001",
            "S002",
            "S003",
            "S004",
            "S005",
            "S006",
            "S007",
            "S008",
            "S009",
            "S010",
            "S011",
            "S012",
            "S013",
            "S014",
            "S015",
            "S016",
            "S017",
            "S018",
            "S019",
            "S020",
            "S021",
            "S022",
            "S023",
            "S024",
            "S025",
            "S026",
            "S027",
            "S028",
            "S029",
            "S030",
            "S031",
            "S032",
            "S033",
            "S034",
            "S035",
            "S036",
            "S037",
            "S038",
            "S039",
            "S040",
            "S041",
            "S042",
            "S043",
            "S044",
            "S045",
            "S046",
            "S047",
            "S048",
            "S049",
            "S050",
            "S051",
            "S052",
            "S053",
            "S054",
            "S055",
            "S056",
            "S057",
            "S058",
            "S059",
            "S060",
            "S061",
            "S062",
            "S063",
            "S064",
            "S065",
            "S066",
            "S067",
            "S068",
            "S069",
            "S070",
            "S071",
            "S072",
            "S073",
            "S074",
            "S075",
            "S076",
            "S077",
            "S078",
            "S079",
            "S080",
            "S081",
            "S082",
            "S083",
            "S084",
            "S085",
            "S086",
            "S087",
            "S088",
            "S089",
            "S090",
            "S091",
            "S092",
            "S093",
            "S094",
            "S095",
            "S096",
            "S097",
            "S098",
            "S099",
            "S100",
            "S101",
            "S102",
            "S103",
            "S104",
            "S105",
            "S106",
            "S107",
            "S108",
            "S109",
            "S110",
            "S111",
            "S112",
            "S113",
            "S114",
            "S115",
            "S116",
            "S117",
            "S118",
            "S119",
            "S120",
            "S121",
            "S122",
            "S123",
            "S124",
            "S125",
            "S126",
            "S127",
            "S128",
            "S129",
            "S130",
            "S131",
            "S132",
            "S133",
            "S134",
            "S135",
            "S136",
            "S137",
            "S138",
            "S139",
            "S140",
            "S141",
            "S142",
            "S143",
            "S144",
            "S145",
            "S146",
            "S147",
            "S148",
            "S149",
            "S150",
            "S151",
            "S152",
            "S153"
          ]
        },
        {
          "id": "subtask-2",
          "query": "Which protein sites (or their parent genes) are targeted by FDA-approved drugs?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx"
          ],
          "answer": []
        }
      ],
      "answer": []
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper paths (Kramabench runner mounts data here)\n# ------------------------------------------------------------------\nbase = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input\")\nhyper_file = base / \"hyperactivated.csv\"\nmmc1_file   = base / \"1-s2.0-S0092867420301070-mmc1.xlsx\"\nmmc6_file   = base / \"1-s2.0-S0092867420301070-mmc6.xlsx\"\n\n# ------------------------------------------------------------------\n# subtask-1 \u2013  hyper-activated sites\n# ------------------------------------------------------------------\n# We do not assume a specific separator or column names, so try both\nhyper_df = pd.read_csv(hyper_file)\n# normalise column names\nhyper_df.columns = [c.strip().lower() for c in hyper_df.columns]\n\n# Look for any column that contains the word \"site\"\nsite_cols = [c for c in hyper_df.columns if \"site\" in c]\nif not site_cols:               # fallback to a generic first column\n    site_col = hyper_df.columns[0]\nelse:\n    site_col = site_cols[0]\n\n# Clean site strings: remove leading/trailing blanks and cast to str\nhyper_sites = (\n    hyper_df[site_col]\n    .astype(str)\n    .str.strip()\n    .loc[lambda x: x != \"\"]\n    .unique()\n)\n\n# ------------------------------------------------------------------\n# subtask-2 \u2013  FDA-approved drug targets\n# ------------------------------------------------------------------\n# Table S6 (mmc6) is a list of kinase genes and their drugs\n# Table S1 (mmc1) sometimes holds the longer annotation; use both\n\ndef read_excel_safely(path):\n    \"\"\"Return list of dataframes (one per sheet)\"\"\"\n    try:\n        xl = pd.ExcelFile(path)\n        return [xl.parse(s) for s in xl.sheet_names]\n    except Exception:\n        return []\n\ndfs_mmc6 = read_excel_safely(mmc6_file)\ndfs_mmc1 = read_excel_safely(mmc1_file)\n\ndrug_dfs = dfs_mmc6 + dfs_mmc1\n\nfda_targets = set()\nfor df in drug_dfs:\n    df.columns = [c.strip().lower() for c in df.columns]\n    \n    # Identify likely columns\n    drug_col_candidates = [c for c in df.columns if \"drug\" in c or \"inhibitor\" in c]\n    target_col_candidates = [c for c in df.columns if \"target\" in c or \"gene\" in c]\n    fda_col_candidates = [c for c in df.columns if \"fda\" in c or \"approved\" in c]\n    \n    if not drug_col_candidates or not target_col_candidates or not fda_col_candidates:\n        continue\n    \n    drug_col   = drug_col_candidates[0]\n    target_col = target_col_candidates[0]\n    fda_col    = fda_col_candidates[0]\n    \n    sub = df.loc[df[fda_col].astype(str).str.upper().str.contains(\"FDA\"), [target_col]]\n    \n    # Split multiple gene targets written with delimiters\n    targets = (\n        sub[target_col]\n        .astype(str)\n        .str.replace(\"[;/,]\", \"|\", regex=True)\n        .str.split(\"|\")\n        .explode()\n        .str.strip()\n        .loc[lambda x: x != \"\"]\n        .str.upper()\n        .unique()\n    )\n    fda_targets.update(targets)\n\n# ------------------------------------------------------------------\n# map site -> gene symbol\n# sites often look like  \"AKT1 T308\" or \"AKT1_pT308\"\ndef site_to_gene(site):\n    return site.split()[0].split(\"_\")[0].upper()\n\nhyper_genes = {site_to_gene(s) for s in hyper_sites}\n\n# ------------------------------------------------------------------\n# intersect\nhits_genes  = hyper_genes & fda_targets\n\n# Keep only sites whose gene is in hits_genes\nhits_sites = [s for s in hyper_sites if site_to_gene(s) in hits_genes]\n\n# ------------------------------------------------------------------\n# answers\nanswers = {\n    \"subtask-1\": sorted(hyper_sites.tolist()),\n    \"subtask-2\": sorted(fda_targets),\n    \"main-task\": sorted(hits_sites)\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-8-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n Which files contain patient metadata, hyperactivated phosphoproteomics data, and FDA-approved drug target data? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which file(s) contain patient metadata (i.e. clinical or sample\u2013level information such as patient-ID, age, stage, CNV-class, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which file(s) contain hyperactivated phosphoproteomics data?",
              "data_sources": [
                "hyperactivated.csv",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv"
              ]
            },
            {
              "id": "subtask-3",
              "query": "Which file(s) contain FDA-approved drug target data (i.e. a mapping between drugs and the protein targets they inhibit/activate)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": "Patient metadata: ['/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx']; Hyperactivated phosphoproteomics: ['/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv']; FDA-approved drug targets: []"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nfrom openpyxl.utils.exceptions import InvalidFileException\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef excel_column_keywords(file_path, nrows=3, keywords=None):\n    \"\"\"Return a set of lowercase column names that contain any keyword\"\"\"\n    keywords = keywords or []\n    cols_found = set()\n    try:\n        # many of the MMC files have only one sheet \u2013 grab the first one\n        sheet_name = 0\n        df = pd.read_excel(file_path, sheet_name=sheet_name, nrows=nrows, engine=\"openpyxl\")\n        for c in df.columns:\n            c_low = str(c).lower()\n            if any(kw in c_low for kw in keywords):\n                cols_found.add(c_low)\n    except InvalidFileException:\n        pass   # not an excel file\n    except Exception as e:\n        # File might contain multiple sheets with different structures \u2013 ignore for signature scan\n        pass\n    return cols_found\n\ndef scan_files(file_paths):\n    \"\"\"Return three lists with candidate files for (1) patient meta-data, \n       (2) hyperactivated phosphoproteomics, (3) FDA-approved drug targets.\"\"\"\n    patient_meta_files = []\n    hyperact_files     = []\n    fda_target_files   = []\n    \n    # key words we will search for\n    patient_keys  = [\"patient\", \"sample\", \"age\", \"stage\", \"histology\", \"cnv\"]\n    phospho_keys  = [\"phospho\", \"site\", \"residue\", \"hyper\", \"log2fc\"]\n    drug_keys     = [\"fda\", \"drug\", \"target\", \"compound\", \"approved\", \"uniprot\"]\n    \n    for fp in file_paths:\n        p = Path(fp)\n        f_str = str(p)\n        \n        # ----------------------------------------------------------\n        # 1. quick name\u2013based heuristics\n        # ----------------------------------------------------------\n        if \"hyperactivated\" in f_str.lower():\n            hyperact_files.append(f_str)\n            continue   # no need to scan further\n        \n        # ----------------------------------------------------------\n        # 2. scan column names inside the file\n        # ----------------------------------------------------------\n        cols_patient  = excel_column_keywords(fp, keywords=patient_keys)\n        cols_phospho  = excel_column_keywords(fp, keywords=phospho_keys)\n        cols_drug     = excel_column_keywords(fp, keywords=drug_keys)\n        \n        if cols_patient:\n            patient_meta_files.append(f_str)\n        if cols_phospho and (\"phospho\" in f_str.lower() or \"mmc3\" in f_str.lower()):\n            hyperact_files.append(f_str)\n        if cols_drug:\n            fda_target_files.append(f_str)\n    \n    return patient_meta_files, hyperact_files, fda_target_files\n\n# ------------------------------------------------------------------\n# Locate the correct files\n# ------------------------------------------------------------------\nfile_paths = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\n]\n\npatient_meta_files, hyperact_files, fda_target_files = scan_files(file_paths)\n\n# ------------------------------------------------------------------\n# Assemble answers\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": patient_meta_files,\n    \"subtask-2\": hyperact_files,\n    \"subtask-3\": fda_target_files\n}\n\n# a single sentence for the main task\nanswers[\"main-task\"] = (\n    f\"Patient metadata: {patient_meta_files}; \"\n    f\"Hyperactivated phosphoproteomics: {hyperact_files}; \"\n    f\"FDA-approved drug targets: {fda_target_files}\"\n)\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 46.79285669326782
      },
      {
        "task_id": "biomedical-hard-8-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n What endometrioid samples included in the study are CNV-high? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the sheet(s) in 1-s2.0-S0092867420301070-mmc1.xlsx that contain sample-level clinical / molecular annotations (e.g. histological subtype and CNV group).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the selected sheet(s), identify the column that encodes histology (e.g. Endometrioid vs Serous) and the column that encodes CNV group (e.g. CNV-High, CNV-Low/MSS, POLE-mut). Handle possible variants in column names (Histology, Hist_subtype, HistologicalClassification, CNV_Group, CNV cluster, Genomic subtype, etc.).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Automatically detected relevant columns within these sheets; see code comments."
            },
            {
              "id": "subtask-3",
              "query": "Clean and harmonise those two columns (strip white-spaces, capitalisation, convert to string) and filter rows where Histology == \"Endometrioid\" (or variant) AND CNV group == \"CNV-High\" (or variant, e.g. \"High CNV\", \"Copy-number high\").",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "0 rows matched Histology=='Endometrioid' & CNV group=='CNV-High'."
            },
            {
              "id": "subtask-4",
              "query": "Return the unique sample identifiers (e.g. TCGA barcodes or in-house IDs) for those filtered rows \u2013 i.e., the endometrioid samples that are CNV-high.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport re\nimport pandas as pd\nfrom pathlib import Path\n\n# path to the Excel file\nfile_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx')\n\n# --- Helper functions -------------------------------------------------------#\ndef find_candidate_sheets(xls):\n    \"\"\"\n    Return sheet names that have at least one column mentioning histology and\n    one column mentioning CNV/CNA/copy number.\n    \"\"\"\n    histology_patterns = re.compile(r'hist|subtype|histology', re.IGNORECASE)\n    cnv_patterns = re.compile(r'cnv|cna|copy[- _]?number', re.IGNORECASE)\n    candidate_sheets = []\n\n    for sheet in xls.sheet_names:\n        try:\n            df_head = xls.parse(sheet, nrows=5, dtype=str)  # read a small preview\n        except Exception:\n            continue\n        cols = list(df_head.columns)\n        has_hist = any(bool(histology_patterns.search(str(c))) for c in cols)\n        has_cnv  = any(bool(cnv_patterns.search(str(c)))      for c in cols)\n        if has_hist and has_cnv:\n            candidate_sheets.append(sheet)\n    return candidate_sheets\n\ndef extract_cnv_high_endometrioid(df):\n    \"\"\"\n    Given a dataframe with sample annotations, try to identify the relevant\n    columns and filter for Endometrioid & CNV-high.\n    Returns a set of sample IDs (as strings).\n    \"\"\"\n    # Identify sample id column \u2013 the first column that looks like an ID\n    id_col = None\n    for c in df.columns:\n        if re.search(r'sample|id|tcga|case', str(c), re.IGNORECASE):\n            id_col = c\n            break\n    if id_col is None:\n        id_col = df.columns[0]\n\n    # Identify histology column\n    hist_col = None\n    for c in df.columns:\n        if re.search(r'hist|subtype|histology', str(c), re.IGNORECASE):\n            hist_col = c\n            break\n    # Identify CNV group column\n    cnv_col = None\n    for c in df.columns:\n        if re.search(r'cnv|cna|copy[- _]?number', str(c), re.IGNORECASE):\n            cnv_col = c\n            break\n\n    if hist_col is None or cnv_col is None:\n        return set()   # cannot analyse this sheet\n\n    # normalise\n    df[id_col]  = df[id_col].astype(str).str.strip()\n    df[hist_col]= df[hist_col].astype(str).str.strip().str.lower()\n    df[cnv_col] = df[cnv_col].astype(str).str.strip().str.lower()\n\n    # possible keywords for CNV-high\n    cnv_high_keywords = ['cnv-high', 'high', 'copy-number high', 'high cnv', 'serous-like']\n    pattern_cnv_high = re.compile('|'.join([re.escape(k) for k in cnv_high_keywords]), re.IGNORECASE)\n    # filter\n    mask_hist = df[hist_col].str.contains('endometrioid', case=False, na=False)\n    mask_cnv  = df[cnv_col].str.contains(pattern_cnv_high, na=False)\n\n    return set(df.loc[mask_hist & mask_cnv, id_col])\n\n# ---  main workflow ---------------------------------------------------------#\nxls = pd.ExcelFile(file_path)\ncandidate_sheets = find_candidate_sheets(xls)\n\ncnv_high_samples = set()\nfor sheet in candidate_sheets:\n    df_full = xls.parse(sheet, dtype=str)\n    cnv_high_samples.update(extract_cnv_high_endometrioid(df_full))\n\n# Convert to sorted list for consistency\ncnv_high_samples = sorted(cnv_high_samples)\n\n# -------------------- print answers -----------------------------------------#\nanswers = {\n    \"subtask-1\": candidate_sheets,\n    \"subtask-2\": \"Automatically detected relevant columns within these sheets; see code comments.\",\n    \"subtask-3\": f\"{len(cnv_high_samples)} rows matched Histology=='Endometrioid' & CNV group=='CNV-High'.\",\n    \"subtask-4\": cnv_high_samples,\n    \"main-task\": cnv_high_samples\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 44.9594783782959
      },
      {
        "task_id": "biomedical-hard-8-3",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n What proteins sites are hyperactivated in the endometrioid samples which are CNV-high? \n",
          "data_sources": [
            "hyperactivated.csv"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Load the hyperactivated.csv file and list/inspect all its columns together with their inferred data-types so we know what we are dealing with.",
              "data_sources": [
                "hyperactivated.csv"
              ]
            },
            {
              "id": "subtask-2",
              "query": "From hyperactivated.csv keep only rows that correspond to endometrioid tumours (regardless of lower/upper-case, possible leading/trailing blanks) and that belong to the \u2018CNV-high\u2019 group (again, be tolerant to case and surrounding blanks).",
              "data_sources": [
                "hyperactivated.csv"
              ]
            },
            {
              "id": "subtask-3",
              "query": "Within this filtered subset keep only the protein sites that are flagged as \u201chyper-activated\u201d (could be a boolean column, a string such as \u2018True\u2019, or a numeric 1; take care of possible NA\u2019s and type cast). Return the de-duplicated list of protein sites.",
              "data_sources": [
                "hyperactivated.csv"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-hard-8-3/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper -----------------------------------------------------------\ndef normalise_str(x):\n    \"\"\"Lower-case, strip leading/trailing blanks; leave NaN unchanged.\"\"\"\n    if isinstance(x, str):\n        return x.strip().lower()\n    return x\n\n# ------------------------------------------------------------------\n# subtask-1: inspect columns & dtypes ------------------------------\nfile_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv')\ndf = pd.read_csv(file_path, dtype=str)        # read everything as string first\ncolumn_info = {c: str(df[c].dtype) for c in df.columns}\n\n# ------------------------------------------------------------------\n# subtask-2: filter to endometrioid + CNV-high ---------------------\n# create normalised helper columns\ndf['_histotype_norm'] = df['Histotype'].apply(normalise_str) if 'Histotype' in df.columns else pd.NA\ndf['_cnv_group_norm'] = df['CNV_group'].apply(normalise_str) if 'CNV_group' in df.columns else pd.NA\n\nmask_endomet = df['_histotype_norm'] == 'endometrioid'\nmask_cnvhigh = df['_cnv_group_norm'] == 'cnv-high'\nfiltered = df[mask_endomet & mask_cnvhigh].copy()\n\n# ------------------------------------------------------------------\n# subtask-3: keep hyper-activated protein sites --------------------\n# normalise hyperactivation indicator \u2013 the column name might vary\n# guess likely columns that contain that information\ncandidate_cols = [c for c in df.columns if 'hyper' in c.lower() and ('activate' in c.lower() or 'status' in c.lower())]\nif candidate_cols:\n    hyper_col = candidate_cols[0]\nelse:   # fall back to assuming every row is already hyperactivated\n    hyper_col = None\n\nif hyper_col:\n    # treat 'True', 'TRUE', '1', 1 as positive\n    filtered['_hyper_bool'] = filtered[hyper_col].apply(lambda x: str(x).strip().lower() if pd.notna(x) else '')\n    hyper_mask = filtered['_hyper_bool'].isin({'true', '1', 'yes', 'y'})\n    filtered = filtered[hyper_mask]\n\n# now gather protein site column\nprot_col_candidates = [c for c in df.columns if 'protein' in c.lower() and 'site' in c.lower()]\nprot_col = prot_col_candidates[0] if prot_col_candidates else 'Protein_site'\n\nprotein_sites = sorted(filtered[prot_col].dropna().unique())\n\n# ------------------------------------------------------------------\n# answers ----------------------------------------------------------\nanswers = {\n    \"subtask-1\": column_info,\n    \"subtask-2\": f\"{filtered.shape[0]} rows after filtering to endometrioid & CNV-high.\",\n    \"subtask-3\": protein_sites,\n    \"main-task\": protein_sites\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 19.503623485565186
      },
      {
        "task_id": "biomedical-hard-8-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n What protein sites are targeted by FDA-approved drugs? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc6.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheet(s) inside 1-s2.0-S0092867420301070-mmc6.xlsx contain information on protein phosphorylation sites and druggability?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": [
                "README",
                "A-SE protein between Serous and",
                "B-SE phospho site between Serou",
                "C-SE scetyl site between Serous",
                "D-SE protein between MSI-H and ",
                "E-SE phospho site between MSI-H",
                "F-SE scetyl site between MSI-H ",
                "G-FDA approved drugs"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Load each relevant worksheet, standardise the column names (strip spaces, lower-case, replace special characters), and identify the column(s) that specify whether a site is targeted by an FDA-approved drug (e.g. a Boolean column, a list of drugs, or a flag such as \u2018FDA_approved\u2019).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": [
                "G-FDA approved drugs"
              ]
            },
            {
              "id": "subtask-3",
              "query": "From those worksheet(s), extract the protein identifier (e.g. Gene, Protein, Uniprot), the modified residue (site) and keep only the rows where the site is targeted by at least one FDA-approved drug (after handling missing values and different encodings such as \u2018Yes\u2019, \u2018Y\u2019, \u2018TRUE\u2019, \u2018True\u2019, 1, etc.). Cast all indicator values to Boolean first.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-4",
              "query": "Deduplicate the list (same protein-site might appear several times) and output the unique set of protein sites targeted by FDA-approved drugs.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": "Warning: No answer found in the Python pipeline."
            }
          ],
          "answer": []
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper ------------------------------------------------------------\n# ------------------------------------------------------------------\nxlsx_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\")\n\ndef load_all_sheets(excel_file):\n    \"\"\"Return a dict of sheet_name -> dataframe (trying both engine options).\"\"\"\n    try:\n        return pd.read_excel(excel_file, sheet_name=None, engine=\"openpyxl\")\n    except Exception:\n        # fall back on xlrd if necessary\n        return pd.read_excel(excel_file, sheet_name=None)\n\ndef normalise_columns(df):\n    new_cols = (\n        df.columns.str.strip()                # remove leading/trailing spaces\n                 .str.lower()                 # lower-case\n                 .str.replace('[^0-9a-z_]+', '_', regex=True)  # non-alnum to _\n                 .str.replace('__+', '_', regex=True)          # collapse __\n                 .str.strip('_')              # remove leading/ending _\n    )\n    df.columns = new_cols\n    return df\n\ndef to_bool(val):\n    if pd.isna(val):\n        return False\n    if isinstance(val, (int, float)):\n        return bool(val)\n    v = str(val).strip().lower()\n    return v in {\"yes\", \"y\", \"true\", \"t\", \"1\"}\n# ------------------------------------------------------------------\n\n# ================ subtask-1: list sheets ===========================\nall_sheets = load_all_sheets(xlsx_path)\nsheet_names = list(all_sheets.keys())\n\n# heuristic: keep sheets that mention 'drug' (case insensitive) OR 'FDA'\ncandidate_sheet_names = [sn for sn in sheet_names if (\"drug\" in sn.lower() or \"fda\" in sn.lower())]\n\n# if heuristic fails, we also inspect all sheets for a column containing 'fda'\nfor sn, df in all_sheets.items():\n    df = normalise_columns(df.copy())\n    if any('fda' in col for col in df.columns):\n        if sn not in candidate_sheet_names:\n            candidate_sheet_names.append(sn)\n\n# ================ subtask-2 & 3: load and filter ==================\nprotein_site_cols = []\nfda_sites = []\n\nfor sn in candidate_sheet_names:\n    df = normalise_columns(all_sheets[sn].copy())\n    \n    # identify possible indicator columns\n    indicator_cols = [c for c in df.columns if ('fda' in c and 'approved' in c) or ('fda_approved' in c)]\n    if len(indicator_cols) == 0:\n        # maybe there's a 'drugs' column listing names of drugs, filter those rows\n        indicator_cols = [c for c in df.columns if 'drug' in c]  # broad\n    if len(indicator_cols) == 0:\n        continue\n    \n    # choose first indicator for boolean, others we can check if needed\n    ind_col = indicator_cols[0]\n    \n    # Standardise boolean\n    df['fda_flag'] = df[ind_col].apply(to_bool)\n    \n    # Determine protein + site columns (heuristics)\n    prot_cols = [c for c in df.columns if c in {'protein', 'gene', 'gene_symbol'}]\n    site_cols = [c for c in df.columns if 'site' in c or 'position' in c or 'residue' in c]\n    \n    if not prot_cols or not site_cols:\n        # skip sheet if cannot identify\n        continue\n    \n    prot_col = prot_cols[0]\n    site_col = site_cols[0]\n    \n    # Cast to str, guard against missing\n    df[prot_col] = df[prot_col].astype(str).str.strip()\n    df[site_col] = df[site_col].astype(str).str.strip()\n    \n    subset = df.loc[df['fda_flag'], [prot_col, site_col]]\n    \n    # Clean site strings (e.g., \"S345\", \" Y15 \")\n    subset[site_col] = subset[site_col].str.replace(r'\\s+', '', regex=True)\n    \n    fda_sites.extend(list(subset[prot_col] + \"_\" + subset[site_col]))\n\n# ================ subtask-4: deduplicate ===========================\nunique_sites = sorted(set(fda_sites))\n\n# Organise answers --------------------------------------------------\nanswers = {\n    \"subtask-1\": sheet_names,\n    \"subtask-2\": candidate_sheet_names,\n    \"subtask-3\": unique_sites,\n    \"main-task\": unique_sites\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 48.90491986274719
      }
    ],
    "runtime": 55.522817850112915
  },
  {
    "task_id": "biomedical-easy-9",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc3.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Locate the sheet(s) and the exact column names that contain gene identifiers and their corresponding FDR (false-discovery-rate) values.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": {
            "sheet": "F-SS-phospho",
            "gene_column": "Gene",
            "fdr_column": "FDR.phos"
          }
        },
        {
          "id": "subtask-2",
          "query": "Calculate the average FDR for rows whose gene name is CBX3 (case-insensitive, allowing for possible extra spaces or mixed-case spelling).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": 0.016840668972299758
        },
        {
          "id": "subtask-3",
          "query": "Calculate the average FDR for all rows whose gene name is NOT CBX3 (i.e., the rest of the genes).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": 0.024844644346166102
        },
        {
          "id": "subtask-4",
          "query": "Compute the difference between the average FDR for CBX3 genes (result of subtask-2) and the average FDR for all other genes (result of subtask-3).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": -0.008003975373866344
        }
      ],
      "answer": -0.008003975373866344
    },
    "code": "import json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------------\ndef find_gene_fdr_columns(df):\n    \"\"\"\n    Return (gene_col, fdr_col) if both can be guessed, else (None, None).\n    Preference order:\n        1. Column that contains 'gene' in name (for gene_col)\n        2. Column that exactly matches 'FDR' (case-insensitive), else\n           contains 'FDR' or 'padj' or 'adj.p' etc. (for fdr_col)\n    \"\"\"\n    gene_col = None\n    fdr_col = None\n    for col in df.columns:\n        if gene_col is None and 'gene' in str(col).lower():\n            gene_col = col\n        if fdr_col is None:\n            col_low = str(col).lower()\n            if col_low in ['fdr', 'adj_p', 'adj.p']:\n                fdr_col = col\n            elif 'fdr' in col_low or 'padj' in col_low or 'adj.p' in col_low:\n                fdr_col = col\n    return gene_col, fdr_col\n\ndef safe_to_numeric(series):\n    return pd.to_numeric(series, errors='coerce')\n\n# ------------------------------------------------------------------\n# STEP 1 \u2013 look through every sheet to locate gene + FDR columns\n# ------------------------------------------------------------------\nexcel_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\nxls = pd.ExcelFile(excel_path)\nlocation_info = []  # store tuples (sheet_name, gene_col, fdr_col)\n\nfor sheet in xls.sheet_names:\n    try:\n        # read only first 500 rows to speed things up\n        df_sample = xls.parse(sheet_name=sheet, nrows=500, dtype=str)\n    except Exception:\n        # sheet cannot be parsed, skip\n        continue\n    gene_col, fdr_col = find_gene_fdr_columns(df_sample)\n    if gene_col and fdr_col:\n        location_info.append((sheet, gene_col, fdr_col))\n\n# choose the first suitable sheet (if multiple exist, we assume same structure)\nif not location_info:\n    raise ValueError(\"Could not find sheet with gene and FDR columns.\")\n\ntarget_sheet, gene_col, fdr_col = location_info[0]\n\n# ------------------------------------------------------------------\n# STEP 2 \u2013 load that entire sheet robustly  (convert everything to str first)\n# ------------------------------------------------------------------\ndf = xls.parse(sheet_name=target_sheet, dtype=str)\n# guard against duplicate column names after reading\ndf = df.loc[:, ~df.columns.duplicated(keep='first')]\n\n# ------------------------------------------------------------------\n# STEP 3 \u2013 clean & convert columns\n# ------------------------------------------------------------------\n# Strip whitespace in gene names, make upper-case to compare\ndf[gene_col] = df[gene_col].astype(str).str.strip()\n\n# convert FDR column to numeric, coercing errors to NaN\ndf[fdr_col] = safe_to_numeric(df[fdr_col])\n\n# Drop rows without numeric FDR\ndf = df.dropna(subset=[fdr_col])\n\n# ------------------------------------------------------------------\n# STEP 4 \u2013 compute averages\n# ------------------------------------------------------------------\nis_cbx3 = df[gene_col].str.upper() == \"CBX3\"\n\ncbx3_mean = df.loc[is_cbx3, fdr_col].mean() if any(is_cbx3) else np.nan\nrest_mean = df.loc[~is_cbx3, fdr_col].mean()\n\ndifference = cbx3_mean - rest_mean if not np.isnan(cbx3_mean) else np.nan\n\n# ------------------------------------------------------------------\n# Prepare answers\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": {\n        \"sheet\": target_sheet,\n        \"gene_column\": gene_col,\n        \"fdr_column\": fdr_col\n    },\n    \"subtask-2\": cbx3_mean,\n    \"subtask-3\": rest_mean,\n    \"subtask-4\": difference,\n    \"main-task\": difference\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-easy-9-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What file contains false discovery rate (FDR) values? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Does 1-s2.0-S0092867420301070-mmc1.xlsx contain a column with FDR or False Discovery Rate information (in any sheet)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "file": "1-s2.0-S0092867420301070-mmc7.xlsx",
                "contains_FDR": false,
                "matching_columns": []
              }
            },
            {
              "id": "subtask-2",
              "query": "Does 1-s2.0-S0092867420301070-mmc2.xlsx contain a column with FDR or False Discovery Rate information (in any sheet)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "file": "hyperactivated.csv",
                "contains_FDR": false,
                "matching_columns": []
              }
            },
            {
              "id": "subtask-3",
              "query": "Does 1-s2.0-S0092867420301070-mmc3.xlsx contain a column with FDR or False Discovery Rate information (in any sheet)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": {
                "file": "1-s2.0-S0092867420301070-mmc2.xlsx",
                "contains_FDR": false,
                "matching_columns": []
              }
            },
            {
              "id": "subtask-4",
              "query": "Does 1-s2.0-S0092867420301070-mmc4.xlsx contain a column with FDR or False Discovery Rate information (in any sheet)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc4.xlsx"
              ],
              "answer": {
                "file": "1-s2.0-S0092867420301070-mmc4.xlsx",
                "contains_FDR": false,
                "matching_columns": []
              }
            },
            {
              "id": "subtask-5",
              "query": "Does 1-s2.0-S0092867420301070-mmc5.xlsx contain a column with FDR or False Discovery Rate information (in any sheet)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc5.xlsx"
              ],
              "answer": {
                "file": "1-s2.0-S0092867420301070-mmc5.xlsx",
                "contains_FDR": false,
                "matching_columns": []
              }
            },
            {
              "id": "subtask-6",
              "query": "Does 1-s2.0-S0092867420301070-mmc6.xlsx contain a column with FDR or False Discovery Rate information (in any sheet)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": {
                "file": "1-s2.0-S0092867420301070-mmc1.xlsx",
                "contains_FDR": false,
                "matching_columns": []
              }
            },
            {
              "id": "subtask-7",
              "query": "Does 1-s2.0-S0092867420301070-mmc7.xlsx contain a column with FDR or False Discovery Rate information (in any sheet)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": {
                "file": "1-s2.0-S0092867420301070-mmc6.xlsx",
                "contains_FDR": true,
                "matching_columns": [
                  "A-SE protein between Serous and:FDR.protein",
                  "A-SE protein between Serous and:FDR.rna",
                  "B-SE phospho site between Serou:FDR.phos",
                  "B-SE phospho site between Serou:FDR.rna",
                  "C-SE scetyl site between Serous:FDR.acetyl",
                  "C-SE scetyl site between Serous:FDR.rna",
                  "D-SE protein between MSI-H and :FDR.protein",
                  "D-SE protein between MSI-H and :FDR.rna",
                  "E-SE phospho site between MSI-H:FDR.phos",
                  "E-SE phospho site between MSI-H:FDR.rna",
                  "E-SE phospho site between MSI-H:logfdr.phos",
                  "F-SE scetyl site between MSI-H :FDR.acetyl",
                  "F-SE scetyl site between MSI-H :FDR.rna"
                ]
              }
            },
            {
              "id": "subtask-8",
              "query": "Does hyperactivated.csv contain a column with FDR or False Discovery Rate information?",
              "data_sources": [
                "hyperactivated.csv"
              ],
              "answer": {
                "file": "1-s2.0-S0092867420301070-mmc3.xlsx",
                "contains_FDR": true,
                "matching_columns": [
                  "F-SS-phospho:FDR.phos",
                  "F-SS-phospho:FDR.prot",
                  "F-SS-phospho:logfdr.phos",
                  "F-SS-phospho:logfdr.prot",
                  "F-SS-phospho:signedfdr.phos",
                  "F-SS-phospho:signedfdr.prot"
                ]
              }
            }
          ],
          "answer": [
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ]
        },
        "code": "import json\nimport os\nimport pandas as pd\n\n# ------------------------------------------------------------------\n# Helper: read each data source safely and search for 'FDR' columns\n# ------------------------------------------------------------------\ndef find_fdr_columns(file_path):\n    fdr_columns_found = set()\n    try:\n        _, ext = os.path.splitext(file_path)\n        if ext.lower() in [\".csv\", \".tsv\", \".txt\"]:\n            # Try comma first, then tab\n            for sep in [\",\", \"\\t\", \";\"]:\n                try:\n                    df = pd.read_csv(file_path, sep=sep, nrows=5)   # Just peek at a few rows\n                    break\n                except Exception:\n                    continue\n            else:\n                return fdr_columns_found\n            for col in df.columns:\n                if isinstance(col, str) and 'fdr' in col.lower():\n                    fdr_columns_found.add(col)\n        elif ext.lower() in [\".xlsx\", \".xls\"]:\n            sheets = pd.ExcelFile(file_path).sheet_names\n            for sh in sheets:\n                try:\n                    df = pd.read_excel(file_path, sheet_name=sh, nrows=5)\n                except Exception:\n                    continue\n                for col in df.columns:\n                    if isinstance(col, str) and 'fdr' in col.lower():\n                        fdr_columns_found.add(f\"{sh}:{col}\")\n    except Exception as e:\n        # If anything goes wrong reading the file, we just report no columns\n        pass\n    return fdr_columns_found\n\ndata_dir = \"/home/gridsan/elai/Kramabench/data/biomedical/input\"\nfiles = [\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"hyperactivated.csv\",\n    \"1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc3.xlsx\"\n]\n\nanswers = {}\nfor i, file in enumerate(files, start=1):\n    file_path = os.path.join(data_dir, file)\n    fdr_cols = find_fdr_columns(file_path)\n    answers[f\"subtask-{i}\"] = {\n        \"file\": file,\n        \"contains_FDR\": len(fdr_cols) > 0,\n        \"matching_columns\": sorted(list(fdr_cols))\n    }\n\n# Determine which file(s) contain the FDR values\nfiles_with_fdr = [v[\"file\"] for v in answers.values() if v[\"contains_FDR\"]]\nanswers[\"main-task\"] = files_with_fdr\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 41.97495079040527
      },
      {
        "task_id": "biomedical-easy-9-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n Which columns are necessary from the input data file? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Inspect the Excel workbook and list the sheet names and their respective column headers so we know what information is available.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": {
                "README": [
                  "Sheet",
                  "Description"
                ],
                "A-MSI": [
                  "idx",
                  "MSMuTect_classification",
                  "MSIsensor_classification",
                  "MLH1_methylation_classification",
                  "Mutation_burden_classification",
                  "MLH1_mutation",
                  "MLH3_mutation",
                  "MSH2_mutation",
                  "MSH3_mutation",
                  "MSH6_mutation"
                ],
                "B-SE-proteomics": [
                  "WDR36"
                ],
                "C-SE-phospho": [
                  "AHCTF1"
                ],
                "D-SE-acetyl": [
                  "BRD8"
                ],
                "E-Mutation-adjacent": [
                  "Sample",
                  "CHROM",
                  "POS",
                  "ID",
                  "REF",
                  "ALT",
                  "QUAL",
                  "FILTER",
                  "INFO",
                  "FORMAT",
                  "NORMAL",
                  "TUMOR"
                ],
                "F-SS-phospho": [
                  "Gene",
                  "Feature",
                  "FoldChange.phos",
                  "pval.phos",
                  "logp.phos",
                  "FDR.phos",
                  "logfdr.phos",
                  "signedlogp.phos",
                  "signedfdr.phos",
                  "FoldChange.prot",
                  "pval.prot",
                  "logp.prot",
                  "FDR.prot",
                  "logfdr.prot",
                  "signedlogp.prot",
                  "signedfdr.prot",
                  "Comparison"
                ]
              }
            },
            {
              "id": "subtask-2",
              "query": "From the discovered column headers, which one contains the gene symbol / gene name?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "Gene"
            },
            {
              "id": "subtask-3",
              "query": "From the discovered column headers, which one contains the FDR (False-Discovery-Rate / adjusted-p-value)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "FDR.phos"
            }
          ],
          "answer": {
            "necessary_columns": {
              "gene_column": "Gene",
              "fdr_column": "FDR.phos"
            }
          }
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper-function to get headers from every sheet\ndef get_sheet_headers(excel_path):\n    xls = pd.ExcelFile(excel_path)\n    headers_per_sheet = {}\n    for sheet in xls.sheet_names:\n        try:\n            df_head = pd.read_excel(excel_path, sheet_name=sheet, nrows=0)\n            headers_per_sheet[sheet] = list(df_head.columns)\n        except Exception as e:\n            headers_per_sheet[sheet] = f\"Could not read sheet: {e}\"\n    return headers_per_sheet\n\n# ------------------------------------------------------------------\n# path to the provided data file\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\n\n# ---------------- answer for subtask-1 -----------------------------\nheaders_info = get_sheet_headers(file_path)\n\n# ---------------- answer for subtask-2 and subtask-3 ---------------\n# na\u00efvely detect gene & FDR columns by keyword search\ngene_keywords = {\"gene\", \"symbol\", \"genename\", \"gene_name\"}\nfdr_keywords  = {\"fdr\", \"adj\", \"padj\", \"qvalue\", \"q_value\", \"adj_p\", \"adjusted\", \"false\"}\n\ngene_cols = set()\nfdr_cols  = set()\n\nfor sheet, cols in headers_info.items():\n    if isinstance(cols, list):\n        for c in cols:\n            c_lower = str(c).lower()\n            if any(k in c_lower for k in gene_keywords):\n                gene_cols.add(c)\n            if any(k in c_lower for k in fdr_keywords):\n                fdr_cols.add(c)\n\n# take the first plausible hit if many\ngene_col = sorted(gene_cols)[0] if gene_cols else None\nfdr_col  = sorted(fdr_cols)[0]  if fdr_cols  else None\n\n# ---------------- collect answers ---------------------------------\nanswers = {\n    \"subtask-1\": headers_info,\n    \"subtask-2\": gene_col,\n    \"subtask-3\": fdr_col,\n    \"main-task\": {\n        \"necessary_columns\": {\n            \"gene_column\": gene_col,\n            \"fdr_column\": fdr_col\n        }\n    }\n}\n\nprint(json.dumps(answers, indent=4, default=str))",
        "runtime": 14.84910774230957
      },
      {
        "task_id": "biomedical-easy-9-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What are the average FDR values for each gene? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in 1-s2.0-S0092867420301070-mmc3.xlsx contain both a gene identifier column and an FDR (false discovery rate) column?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "F-SS-phospho"
              ]
            },
            {
              "id": "subtask-2",
              "query": "What are the exact column names for gene identifiers and FDR values in the identified sheet(s)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": {
                "F-SS-phospho": [
                  "Gene",
                  "FDR.phos"
                ]
              }
            },
            {
              "id": "subtask-3",
              "query": "After cleaning the data (e.g., removing missing/invalid entries and casting FDR to numeric), what is the average FDR value for each gene?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": {
                "AAK1": 0.043956340583415396,
                "ABCF1": 0.0078521613399921,
                "ABLIM1": 0.0496760101814453,
                "ACTL6A": 0.0205708302930587,
                "ADD2": 0.0267724421983287,
                "ADNP": 0.00511262716054596,
                "AFF4": 0.0394362313390499,
                "AGAP2": 0.030202402696061,
                "AGRN": 0.0396716365884564,
                "AHNAK": 0.0300909141488162,
                "AIF1L": 0.0141086530576075,
                "AKAP11": 0.0365433285394853,
                "AKAP9": 0.0247082834366274,
                "AKIRIN2": 0.00575581465720716,
                "AKNA": 0.0180754648214029,
                "AMOT": 0.0429541861987379,
                "ANKLE2": 0.03641930848065587,
                "ANKRD17": 0.01882743304837297,
                "AP4B1": 0.015723209794619,
                "APC": 0.04673785183598855,
                "API5": 0.013905958534478,
                "APOBR": 0.0457711894718315,
                "APTX": 0.0377307000368468,
                "ARFGAP1": 0.0189119245800733,
                "ARHGAP21": 0.0365433285394853,
                "ARHGAP23": 0.0429123695917402,
                "ARHGAP29": 0.03742468300232555,
                "ARHGEF12": 0.024818221188327463,
                "ARHGEF18": 0.0116912247226169,
                "ARID2": 0.0464203504903622,
                "ARID5B": 0.0225928753273997,
                "ARPC5L": 0.011298571216312,
                "ASPSCR1": 0.0468420084479082,
                "ATN1": 0.0297994065230863,
                "ATR": 0.03465631347636565,
                "ATXN2L": 0.02374687882857916,
                "AUP1": 0.0338437319620087,
                "BABAM1": 0.022915051818072064,
                "BAZ1B": 0.00264170698308115,
                "BCL11A": 0.030525886289553023,
                "BICDL2": 0.0433973813975053,
                "BIN2": 0.0121729427813012,
                "BLM": 0.0330686437156121,
                "BOD1L1": 0.01411303457891054,
                "BPTF": 0.0244596186580932,
                "BUB3": 0.0457101882637775,
                "BUD13": 0.00703530023458142,
                "C17orf49": 0.000980881159299924,
                "C18orf25": 0.040056765216411,
                "C2CD5": 0.0102162361818213,
                "C2orf49": 0.000358566570028842,
                "C2orf88": 0.0348398874245796,
                "C5orf30": 3.53760522249561e-05,
                "C6orf106": 0.009443200898048985,
                "C8orf33": 0.0375334621383179,
                "CAP1": 0.0109260628771659,
                "CBLB": 0.0486162909120264,
                "CBX3": 0.016840668972299792,
                "CBX4": 0.0189119245800733,
                "CCDC137": 0.0439488652893517,
                "CCDC78": 0.0307072245607024,
                "CCDC82": 0.0263860865210061,
                "CCDC86": 0.044663410889452845,
                "CCDC92": 0.0263551921581343,
                "CCNL1": 0.024556301830255006,
                "CCNYL1": 0.0257083221557895,
                "CCSAP": 0.0281129797331958,
                "CD2AP": 0.015777524985309314,
                "CD34": 0.0365433285394853,
                "CDAN1": 0.00209151767284925,
                "CDC14A": 0.0335462926741171,
                "CDC42EP4": 0.0258799666476875,
                "CDCA2": 0.0445660045541175,
                "CDCA5": 0.013376677521683325,
                "CENPC": 0.01572743529945242,
                "CEP131": 0.0321323153270959,
                "CEP95": 0.00903663259943753,
                "CGNL1": 0.0352564339325247,
                "CHAMP1": 0.0322946216561759,
                "CHD3": 0.04333206028716445,
                "CHD6": 0.0373202471752503,
                "CHD7": 0.0401441524276685,
                "CHEK2": 0.00881894886244621,
                "CIC": 0.0168133556489441,
                "CIZ1": 0.02093247757915614,
                "CLCC1": 0.027842528865652,
                "CLIP1": 0.0476862206203252,
                "CLK3": 0.0334892594110799,
                "CNOT3": 0.00432965572242424,
                "CRYBG1": 0.0354891195256936,
                "CTNND1": 0.014247697199297258,
                "CTTN": 0.047774896206704,
                "CUL4B": 0.023219697293174066,
                "DACT3": 0.0362867053879639,
                "DBR1": 0.0037774448807903,
                "DCP2": 0.00120989257999747,
                "DDB1": 0.0363634383914052,
                "DDB2": 0.0470860227567865,
                "DDHD1": 0.0164523294921849,
                "DDX20": 0.0211175754635949,
                "DDX50": 0.00696837494906315,
                "DENND4C": 0.0392812998337344,
                "DGCR8": 0.0440804124283028,
                "DLGAP4": 0.0480237851027848,
                "DMXL2": 0.0298664551937986,
                "DNAJA1": 0.0445505646862037,
                "DOCK11": 0.0477951114660338,
                "DOT1L": 0.0436974479354995,
                "DSN1": 0.012139852775975067,
                "DYNC1LI1": 0.0145893733025529,
                "EAF1": 0.0168697641557901,
                "EEF2": 0.00829125996436007,
                "EIF2S2": 0.028334264747518746,
                "EIF3H": 0.00756429895549288,
                "EIF4B": 0.0473670166191686,
                "EIF4G3": 0.0460613348652966,
                "EML4": 0.0345718255898893,
                "EMSY": 0.0381411566387611,
                "ENAH": 0.00819256283991357,
                "ENTPD4": 0.0305027122396485,
                "EPRS": 0.0106618952416762,
                "EXOC7": 0.0333909430904532,
                "EXOSC2": 0.0443090256138802,
                "FAM129A": 0.0281129797331958,
                "FAM193A": 0.0197997969511195,
                "FAM208B": 0.0235703093112661,
                "FAM219A": 0.0304365074448916,
                "FAM83D": 0.000879784873047865,
                "FAM83G": 0.0494723914078752,
                "FAM84B": 0.0304219855097464,
                "FARP2": 0.03115419502385014,
                "FBXO28": 0.000164513068071164,
                "FIP1L1": 0.00870437374226926,
                "FLII": 0.00542256800118294,
                "FLNB": 0.02179416147232348,
                "FLOT1": 0.00935388605974624,
                "FNBP4": 0.00518328925662211,
                "FOXA2": 0.009849521679926395,
                "FOXK1": 0.0003134655198048007,
                "FOXK2": 0.0176422435789253,
                "FOXO3": 0.025577594478051524,
                "FUBP1": 0.0204898876900786,
                "FXR1": 0.0461412011156122,
                "GABARAPL2": 0.0346388028086022,
                "GARNL3": 0.0408201218683491,
                "GART": 0.0360336537657683,
                "GAS2L2": 0.025161375628582273,
                "GCFC2": 0.0290578234220004,
                "GNL3L": 0.0187971767630604,
                "GPALPP1": 0.0382360557777518,
                "GPATCH4": 0.0445646481793812,
                "GPBP1L1": 0.0189981163392183,
                "GPI": 0.0147769221258296,
                "H2AFX": 0.0050204946798786745,
                "H2AFY": 0.0165403467376094,
                "HDGF": 0.02547520559747745,
                "HDLBP": 0.033185880218213,
                "HIP1": 0.0165096012577211,
                "HIST1H1E": 0.0168329439119639,
                "HMBOX1": 0.00961012850632467,
                "HMGA1": 0.018806420933700357,
                "HMGXB4": 0.00996002755398111,
                "HNRNPAB": 0.011827107497728374,
                "HNRNPM": 0.0354122347844906,
                "HP1BP3": 0.0399768129248203,
                "HSF1": 0.0053191829352203,
                "HSP90B1": 0.019531869075253866,
                "HSPH1": 0.00317067607895234,
                "HTATSF1": 0.02671835137353153,
                "HUWE1": 0.0411406623713756,
                "ICA1L": 0.0386466483903568,
                "IGF2BP1": 0.029324466627102,
                "ILF3": 0.00375363666519567,
                "INA": 0.0389763270603149,
                "INCENP": 0.0364253658298323,
                "ING5": 0.0227513131502596,
                "INTS3": 0.0404939378115201,
                "IPCEF1": 0.0478035141775167,
                "IPO5": 0.0365433285394853,
                "IRF2BP1": 0.0284355707777988,
                "IWS1": 0.01837915535348926,
                "JPT1": 0.0353571291519793,
                "KANSL3": 0.0096327620783505,
                "KAT14": 0.0375334621383179,
                "KAT6B": 0.0365484291209722,
                "KCTD3": 0.0496567595583864,
                "KDM1A": 0.0213763830894492,
                "KHSRP": 0.0460934138591996,
                "KIAA0232": 0.022504514565002073,
                "KIAA0556": 0.03502566337261288,
                "KIAA0753": 0.0461422370512207,
                "KIAA1191": 0.0360336537657683,
                "KIDINS220": 0.0427962896047914,
                "KIF13A": 0.0411293078541561,
                "KMT2A": 0.0279546183659566,
                "KMT2B": 0.0147163919366181,
                "KRT16": 0.0385203422302532,
                "KRT17": 0.0327843618335718,
                "LARP1B": 0.027271199482557698,
                "LAS1L": 0.018760216755420642,
                "LASP1": 0.0272872531074229,
                "LDB1": 0.0304219855097464,
                "LIG3": 0.0200499197741112,
                "LIMCH1": 0.0443092834734158,
                "LIN37": 0.0130337399053229,
                "LIN54": 0.0209904919636789,
                "LLGL2": 0.0442151786820634,
                "LMNA": 0.0437639532625381,
                "LMNB1": 0.0322830937335546,
                "LRCH1": 0.0479082393688815,
                "LRRFIP1": 0.00696837494906315,
                "LRWD1": 0.00716930926044789,
                "MAP1S": 0.0463009055794489,
                "MAP4": 0.000490284131252745,
                "MAP7": 0.0214500497215805,
                "MAP7D1": 0.0285307502682166,
                "MAPK8IP3": 0.0291040759630139,
                "MARCKS": 0.030202402696061,
                "MARK1": 0.016782176925974,
                "MARS": 0.0404570693732631,
                "MCC": 0.03266018569266625,
                "MCM3": 0.00646221665138953,
                "MED1": 0.021481848893426733,
                "MEF2C": 0.0277338213611755,
                "MEPCE": 0.0162587503419938,
                "MFHAS1": 0.00956282909468471,
                "MGA": 0.029893238309011153,
                "MIA3": 0.00579881961113552,
                "MICALL1": 0.0274324904452464,
                "MISP3": 0.0382559216084444,
                "MKI67": 0.0289385957956281,
                "MRTFA": 0.0411293078541561,
                "MSL1": 0.00901481031238281,
                "MTA1": 0.0297939975628482,
                "MTCL1": 0.0276381632261513,
                "MYO9A": 0.0399897321430616,
                "NAA10": 0.018634628703567403,
                "NACA": 0.0277338213611755,
                "NASP": 0.0496417849998354,
                "NAV1": 0.00449873675088239,
                "NBEAL2": 0.0324706026414712,
                "NBN": 0.021061738038853594,
                "NCOA5": 0.0397406307173996,
                "NCOR1": 0.0218704519227371,
                "NCOR2": 0.028641975489636852,
                "NEDD1": 0.0245566675369464,
                "NFATC2IP": 0.0160715198530403,
                "NFATC4": 0.03035523705042805,
                "NICN1": 0.016423176158984,
                "NIPBL": 0.0031617303720742037,
                "NKAP": 0.0368116113424702,
                "NKTR": 0.0441420547926059,
                "NME9": 0.0298858210285503,
                "NMT2": 0.01944485426083505,
                "NOC2L": 0.008253048728965404,
                "NOL12": 0.0355555088154915,
                "NOL8": 0.00615640064312127,
                "NOLC1": 0.024612885819526437,
                "NOP58": 0.0182584698459063,
                "NPM1": 0.02259129099789245,
                "NT5C": 0.00192535497831041,
                "NUCKS1": 0.021928099342605575,
                "NUDC": 0.00820832149950257,
                "NUP214": 0.019765663265695167,
                "NUP88": 0.00104413235463156,
                "NUP98": 0.0432475697805802,
                "OCIAD1": 0.0134943067389894,
                "ORC2": 0.00828642708073982,
                "PACS2": 0.0219290747075447,
                "PAK3": 0.0246007874898777,
                "PALLD": 0.0288254566458157,
                "PALM": 0.020542664435670634,
                "PANK4": 0.00881894886244621,
                "PARD3": 0.0460062658000308,
                "PATJ": 0.0164677420111589,
                "PBRM1": 0.044317821258459,
                "PBX2": 0.00320829748025083,
                "PCDH19": 0.0272872531074229,
                "PCF11": 0.0291705443705582,
                "PDS5B": 0.021698767270994696,
                "PELP1": 0.0373202471752503,
                "PEX19": 0.0132248961123523,
                "PFKP": 0.00034823519263003,
                "PHF14": 0.0297580241863299,
                "PHF20": 0.010155514207283065,
                "PHF3": 0.00124168361335943,
                "PI4KB": 0.0375025771584155,
                "PIK3AP1": 0.0252481993101516,
                "PITPNM3": 0.0168133556489441,
                "PKN1": 0.0425037272974999,
                "PLCH1": 0.0290345545706679,
                "PLEKHG2": 0.0303160095576093,
                "PLEKHM1": 0.027169999767462,
                "POP1": 0.0318857971755643,
                "PPIL6": 0.0465605467419352,
                "PPIP5K1": 0.0263869627455006,
                "PPP1R12A": 0.0347477199760802,
                "PPP1R14A": 0.0374144887933437,
                "PPP1R9B": 0.00815232888990476,
                "PPP4R2": 0.00641088493834257,
                "PPP6R2": 0.0332082349479447,
                "PRCC": 0.0110460079305209,
                "PRKCZ": 0.0443854065571069,
                "PRPF3": 0.0167825809464426,
                "PRPF38B": 0.02667985483378142,
                "PRRC2A": 0.0162882856389512,
                "PRRC2B": 0.021999780852726105,
                "PRRC2C": 0.00530469798848326,
                "PSMA2": 0.0101658193319756,
                "PTPN21": 0.03239689119022557,
                "PWWP2A": 0.0432594729442924,
                "RAB13": 0.0494174705629229,
                "RAB3IP": 0.0197997969511195,
                "RAB8A": 0.0186492495913322,
                "RAD50": 0.00626865096280216,
                "RAD9A": 0.0261263934393165,
                "RALGPS2": 0.0353669229204015,
                "RANBP2": 0.01840232892259712,
                "RB1": 0.019894896884840807,
                "RBBP5": 0.000545955530476966,
                "RBM12": 0.0102034242673521,
                "RBM15": 0.00426965673197485,
                "RBM15B": 0.0499983073946662,
                "RICTOR": 0.0272872531074229,
                "RLIM": 0.0302252052808086,
                "RNF168": 0.00682139309494012,
                "RNF4": 0.028399545589546948,
                "RPA1": 0.02427581947495829,
                "RPL30": 0.0355065359477124,
                "RPLP2": 0.00526230309191826,
                "RPS2": 0.0192834209107639,
                "RPS27": 0.00747588714540171,
                "RRBP1": 0.0365844158219992,
                "RREB1": 0.0249362393233076,
                "RSF1": 0.024039027030464438,
                "RSPH3": 0.0151020920805762,
                "RSPH4A": 0.013848228193634555,
                "RUBCN": 0.0365484291209722,
                "S100PBP": 0.03308185336504,
                "SACS": 0.0261263934393165,
                "SASH3": 0.0463806341791065,
                "SCRIB": 0.0374144887933437,
                "SEC16A": 0.0152443679189897,
                "SEL1L": 0.0476862206203252,
                "SET": 0.00340924548957681,
                "SETD1B": 0.0349645382832812,
                "SETD2": 0.02520368878493828,
                "SF3B1": 0.00425583102022784,
                "SF3B2": 0.0257041415329438,
                "SF3B3": 0.0150696287432672,
                "SIPA1L3": 0.0180754648214029,
                "SKA3": 0.00228078600498271,
                "SLBP": 0.0371408674066081,
                "SLC8A1": 0.0232741834932739,
                "SLK": 0.01928110738494836,
                "SMARCA1": 0.0293532485733266,
                "SMARCA4": 0.03214970436771115,
                "SMARCC1": 0.017453824072883,
                "SNX24": 0.034327414579176,
                "SOGA1": 0.0454691231013339,
                "SP140": 0.0234811824231471,
                "SPAG9": 0.020878258460720434,
                "SPATS2L": 0.0374144887933437,
                "SPEG": 0.0399751718574391,
                "SPEN": 0.02010414682979206,
                "SPP1": 0.0370751465629521,
                "SRPK2": 0.0106445729035592,
                "SRRM2": 0.03071484591912444,
                "SRSF11": 0.0492913248497778,
                "SRSF12": 0.029411523087563347,
                "SRSF2": 0.030790189398542653,
                "ST13": 0.0382559216084444,
                "STAU1": 0.0217280527645178,
                "STMN1": 0.00419495709841418,
                "STX17": 0.00148589263873434,
                "SYNGR2": 0.0144359469762761,
                "SYNPO": 0.020369419494287,
                "SYNRG": 0.018077287090468797,
                "SZRD1": 0.0360336537657683,
                "TAF15": 0.0327034020860868,
                "TANGO6": 0.0155898300627306,
                "TBX2": 0.03638005478163332,
                "TCF12": 0.0497992644587145,
                "TCF20": 0.0315773649909543,
                "TCOF1": 0.009315999464395012,
                "TERF2IP": 0.01999275807124975,
                "TFAM": 0.0341716449098999,
                "TGM2": 0.0275304040264136,
                "TLE2": 0.0156871837667427,
                "TMPO": 0.02590155835447288,
                "TNIP1": 0.0163057182734311,
                "TNRC6A": 0.0455837739848171,
                "TNS1": 0.0317740787112154,
                "TNS3": 0.0218235881321484,
                "TOMM70": 0.030900171772955152,
                "TP53BP1": 0.015914743806578344,
                "TRAPPC8": 0.0341716449098999,
                "TRIM28": 0.03068943571831135,
                "TRIT1": 0.0390932758187843,
                "TSC2": 0.026996410406962615,
                "TSHZ3": 0.0336356206979959,
                "TTF2": 0.0486250708765263,
                "TUBGCP3": 0.0274324904452464,
                "TXLNA": 0.0350854516754513,
                "UBA1": 0.014833098717392055,
                "UBAP2L": 0.0466049147436694,
                "UBE2O": 0.0314950361168889,
                "UCKL1": 0.00207247098461645,
                "UFD1": 0.023825581665335202,
                "UGDH": 0.00518328925662211,
                "UNC45A": 0.0496567595583864,
                "USF2": 0.0491924982363911,
                "USP10": 0.0037774448807903,
                "USP32": 0.0395243155467003,
                "USP47": 0.0138565710361594,
                "USP8": 0.0492742541491687,
                "USP9X": 0.0230727213712009,
                "UTP14A": 0.004274746861318775,
                "VIM": 0.0324706026414712,
                "WIPF2": 0.0156657923191347,
                "WNK1": 0.0219399301026229,
                "XRCC1": 0.00253764928159042,
                "XRCC6": 0.0060231290822709,
                "YEATS2": 0.03394187398361015,
                "ZBTB10": 0.0281384896852693,
                "ZBTB21": 0.00701550687582381,
                "ZBTB9": 0.0123832951383463,
                "ZC3H13": 0.027406113169242498,
                "ZC3H8": 0.0399768129248203,
                "ZC3HC1": 0.0004264386291949855,
                "ZFAND2B": 0.0375334621383179,
                "ZFAND3": 0.00115885398086681,
                "ZFAND5": 0.0198740515186142,
                "ZFC3H1": 0.0115115007567177,
                "ZFP91": 0.0218235881321484,
                "ZMYND11": 0.0270620914209947,
                "ZMYND8": 0.0168133556489441,
                "ZNF318": 0.026252497345200288,
                "ZNF326": 0.0173870040853599,
                "ZNF451": 0.01540701734134332,
                "ZNF592": 0.0202793914528402,
                "ZNF638": 0.0120809958371911,
                "ZNF687": 0.0415452010828718
              }
            }
          ],
          "answer": {
            "AAK1": 0.043956340583415396,
            "ABCF1": 0.0078521613399921,
            "ABLIM1": 0.0496760101814453,
            "ACTL6A": 0.0205708302930587,
            "ADD2": 0.0267724421983287,
            "ADNP": 0.00511262716054596,
            "AFF4": 0.0394362313390499,
            "AGAP2": 0.030202402696061,
            "AGRN": 0.0396716365884564,
            "AHNAK": 0.0300909141488162,
            "AIF1L": 0.0141086530576075,
            "AKAP11": 0.0365433285394853,
            "AKAP9": 0.0247082834366274,
            "AKIRIN2": 0.00575581465720716,
            "AKNA": 0.0180754648214029,
            "AMOT": 0.0429541861987379,
            "ANKLE2": 0.03641930848065587,
            "ANKRD17": 0.01882743304837297,
            "AP4B1": 0.015723209794619,
            "APC": 0.04673785183598855,
            "API5": 0.013905958534478,
            "APOBR": 0.0457711894718315,
            "APTX": 0.0377307000368468,
            "ARFGAP1": 0.0189119245800733,
            "ARHGAP21": 0.0365433285394853,
            "ARHGAP23": 0.0429123695917402,
            "ARHGAP29": 0.03742468300232555,
            "ARHGEF12": 0.024818221188327463,
            "ARHGEF18": 0.0116912247226169,
            "ARID2": 0.0464203504903622,
            "ARID5B": 0.0225928753273997,
            "ARPC5L": 0.011298571216312,
            "ASPSCR1": 0.0468420084479082,
            "ATN1": 0.0297994065230863,
            "ATR": 0.03465631347636565,
            "ATXN2L": 0.02374687882857916,
            "AUP1": 0.0338437319620087,
            "BABAM1": 0.022915051818072064,
            "BAZ1B": 0.00264170698308115,
            "BCL11A": 0.030525886289553023,
            "BICDL2": 0.0433973813975053,
            "BIN2": 0.0121729427813012,
            "BLM": 0.0330686437156121,
            "BOD1L1": 0.01411303457891054,
            "BPTF": 0.0244596186580932,
            "BUB3": 0.0457101882637775,
            "BUD13": 0.00703530023458142,
            "C17orf49": 0.000980881159299924,
            "C18orf25": 0.040056765216411,
            "C2CD5": 0.0102162361818213,
            "C2orf49": 0.000358566570028842,
            "C2orf88": 0.0348398874245796,
            "C5orf30": 3.53760522249561e-05,
            "C6orf106": 0.009443200898048985,
            "C8orf33": 0.0375334621383179,
            "CAP1": 0.0109260628771659,
            "CBLB": 0.0486162909120264,
            "CBX3": 0.016840668972299792,
            "CBX4": 0.0189119245800733,
            "CCDC137": 0.0439488652893517,
            "CCDC78": 0.0307072245607024,
            "CCDC82": 0.0263860865210061,
            "CCDC86": 0.044663410889452845,
            "CCDC92": 0.0263551921581343,
            "CCNL1": 0.024556301830255006,
            "CCNYL1": 0.0257083221557895,
            "CCSAP": 0.0281129797331958,
            "CD2AP": 0.015777524985309314,
            "CD34": 0.0365433285394853,
            "CDAN1": 0.00209151767284925,
            "CDC14A": 0.0335462926741171,
            "CDC42EP4": 0.0258799666476875,
            "CDCA2": 0.0445660045541175,
            "CDCA5": 0.013376677521683325,
            "CENPC": 0.01572743529945242,
            "CEP131": 0.0321323153270959,
            "CEP95": 0.00903663259943753,
            "CGNL1": 0.0352564339325247,
            "CHAMP1": 0.0322946216561759,
            "CHD3": 0.04333206028716445,
            "CHD6": 0.0373202471752503,
            "CHD7": 0.0401441524276685,
            "CHEK2": 0.00881894886244621,
            "CIC": 0.0168133556489441,
            "CIZ1": 0.02093247757915614,
            "CLCC1": 0.027842528865652,
            "CLIP1": 0.0476862206203252,
            "CLK3": 0.0334892594110799,
            "CNOT3": 0.00432965572242424,
            "CRYBG1": 0.0354891195256936,
            "CTNND1": 0.014247697199297258,
            "CTTN": 0.047774896206704,
            "CUL4B": 0.023219697293174066,
            "DACT3": 0.0362867053879639,
            "DBR1": 0.0037774448807903,
            "DCP2": 0.00120989257999747,
            "DDB1": 0.0363634383914052,
            "DDB2": 0.0470860227567865,
            "DDHD1": 0.0164523294921849,
            "DDX20": 0.0211175754635949,
            "DDX50": 0.00696837494906315,
            "DENND4C": 0.0392812998337344,
            "DGCR8": 0.0440804124283028,
            "DLGAP4": 0.0480237851027848,
            "DMXL2": 0.0298664551937986,
            "DNAJA1": 0.0445505646862037,
            "DOCK11": 0.0477951114660338,
            "DOT1L": 0.0436974479354995,
            "DSN1": 0.012139852775975067,
            "DYNC1LI1": 0.0145893733025529,
            "EAF1": 0.0168697641557901,
            "EEF2": 0.00829125996436007,
            "EIF2S2": 0.028334264747518746,
            "EIF3H": 0.00756429895549288,
            "EIF4B": 0.0473670166191686,
            "EIF4G3": 0.0460613348652966,
            "EML4": 0.0345718255898893,
            "EMSY": 0.0381411566387611,
            "ENAH": 0.00819256283991357,
            "ENTPD4": 0.0305027122396485,
            "EPRS": 0.0106618952416762,
            "EXOC7": 0.0333909430904532,
            "EXOSC2": 0.0443090256138802,
            "FAM129A": 0.0281129797331958,
            "FAM193A": 0.0197997969511195,
            "FAM208B": 0.0235703093112661,
            "FAM219A": 0.0304365074448916,
            "FAM83D": 0.000879784873047865,
            "FAM83G": 0.0494723914078752,
            "FAM84B": 0.0304219855097464,
            "FARP2": 0.03115419502385014,
            "FBXO28": 0.000164513068071164,
            "FIP1L1": 0.00870437374226926,
            "FLII": 0.00542256800118294,
            "FLNB": 0.02179416147232348,
            "FLOT1": 0.00935388605974624,
            "FNBP4": 0.00518328925662211,
            "FOXA2": 0.009849521679926395,
            "FOXK1": 0.0003134655198048007,
            "FOXK2": 0.0176422435789253,
            "FOXO3": 0.025577594478051524,
            "FUBP1": 0.0204898876900786,
            "FXR1": 0.0461412011156122,
            "GABARAPL2": 0.0346388028086022,
            "GARNL3": 0.0408201218683491,
            "GART": 0.0360336537657683,
            "GAS2L2": 0.025161375628582273,
            "GCFC2": 0.0290578234220004,
            "GNL3L": 0.0187971767630604,
            "GPALPP1": 0.0382360557777518,
            "GPATCH4": 0.0445646481793812,
            "GPBP1L1": 0.0189981163392183,
            "GPI": 0.0147769221258296,
            "H2AFX": 0.0050204946798786745,
            "H2AFY": 0.0165403467376094,
            "HDGF": 0.02547520559747745,
            "HDLBP": 0.033185880218213,
            "HIP1": 0.0165096012577211,
            "HIST1H1E": 0.0168329439119639,
            "HMBOX1": 0.00961012850632467,
            "HMGA1": 0.018806420933700357,
            "HMGXB4": 0.00996002755398111,
            "HNRNPAB": 0.011827107497728374,
            "HNRNPM": 0.0354122347844906,
            "HP1BP3": 0.0399768129248203,
            "HSF1": 0.0053191829352203,
            "HSP90B1": 0.019531869075253866,
            "HSPH1": 0.00317067607895234,
            "HTATSF1": 0.02671835137353153,
            "HUWE1": 0.0411406623713756,
            "ICA1L": 0.0386466483903568,
            "IGF2BP1": 0.029324466627102,
            "ILF3": 0.00375363666519567,
            "INA": 0.0389763270603149,
            "INCENP": 0.0364253658298323,
            "ING5": 0.0227513131502596,
            "INTS3": 0.0404939378115201,
            "IPCEF1": 0.0478035141775167,
            "IPO5": 0.0365433285394853,
            "IRF2BP1": 0.0284355707777988,
            "IWS1": 0.01837915535348926,
            "JPT1": 0.0353571291519793,
            "KANSL3": 0.0096327620783505,
            "KAT14": 0.0375334621383179,
            "KAT6B": 0.0365484291209722,
            "KCTD3": 0.0496567595583864,
            "KDM1A": 0.0213763830894492,
            "KHSRP": 0.0460934138591996,
            "KIAA0232": 0.022504514565002073,
            "KIAA0556": 0.03502566337261288,
            "KIAA0753": 0.0461422370512207,
            "KIAA1191": 0.0360336537657683,
            "KIDINS220": 0.0427962896047914,
            "KIF13A": 0.0411293078541561,
            "KMT2A": 0.0279546183659566,
            "KMT2B": 0.0147163919366181,
            "KRT16": 0.0385203422302532,
            "KRT17": 0.0327843618335718,
            "LARP1B": 0.027271199482557698,
            "LAS1L": 0.018760216755420642,
            "LASP1": 0.0272872531074229,
            "LDB1": 0.0304219855097464,
            "LIG3": 0.0200499197741112,
            "LIMCH1": 0.0443092834734158,
            "LIN37": 0.0130337399053229,
            "LIN54": 0.0209904919636789,
            "LLGL2": 0.0442151786820634,
            "LMNA": 0.0437639532625381,
            "LMNB1": 0.0322830937335546,
            "LRCH1": 0.0479082393688815,
            "LRRFIP1": 0.00696837494906315,
            "LRWD1": 0.00716930926044789,
            "MAP1S": 0.0463009055794489,
            "MAP4": 0.000490284131252745,
            "MAP7": 0.0214500497215805,
            "MAP7D1": 0.0285307502682166,
            "MAPK8IP3": 0.0291040759630139,
            "MARCKS": 0.030202402696061,
            "MARK1": 0.016782176925974,
            "MARS": 0.0404570693732631,
            "MCC": 0.03266018569266625,
            "MCM3": 0.00646221665138953,
            "MED1": 0.021481848893426733,
            "MEF2C": 0.0277338213611755,
            "MEPCE": 0.0162587503419938,
            "MFHAS1": 0.00956282909468471,
            "MGA": 0.029893238309011153,
            "MIA3": 0.00579881961113552,
            "MICALL1": 0.0274324904452464,
            "MISP3": 0.0382559216084444,
            "MKI67": 0.0289385957956281,
            "MRTFA": 0.0411293078541561,
            "MSL1": 0.00901481031238281,
            "MTA1": 0.0297939975628482,
            "MTCL1": 0.0276381632261513,
            "MYO9A": 0.0399897321430616,
            "NAA10": 0.018634628703567403,
            "NACA": 0.0277338213611755,
            "NASP": 0.0496417849998354,
            "NAV1": 0.00449873675088239,
            "NBEAL2": 0.0324706026414712,
            "NBN": 0.021061738038853594,
            "NCOA5": 0.0397406307173996,
            "NCOR1": 0.0218704519227371,
            "NCOR2": 0.028641975489636852,
            "NEDD1": 0.0245566675369464,
            "NFATC2IP": 0.0160715198530403,
            "NFATC4": 0.03035523705042805,
            "NICN1": 0.016423176158984,
            "NIPBL": 0.0031617303720742037,
            "NKAP": 0.0368116113424702,
            "NKTR": 0.0441420547926059,
            "NME9": 0.0298858210285503,
            "NMT2": 0.01944485426083505,
            "NOC2L": 0.008253048728965404,
            "NOL12": 0.0355555088154915,
            "NOL8": 0.00615640064312127,
            "NOLC1": 0.024612885819526437,
            "NOP58": 0.0182584698459063,
            "NPM1": 0.02259129099789245,
            "NT5C": 0.00192535497831041,
            "NUCKS1": 0.021928099342605575,
            "NUDC": 0.00820832149950257,
            "NUP214": 0.019765663265695167,
            "NUP88": 0.00104413235463156,
            "NUP98": 0.0432475697805802,
            "OCIAD1": 0.0134943067389894,
            "ORC2": 0.00828642708073982,
            "PACS2": 0.0219290747075447,
            "PAK3": 0.0246007874898777,
            "PALLD": 0.0288254566458157,
            "PALM": 0.020542664435670634,
            "PANK4": 0.00881894886244621,
            "PARD3": 0.0460062658000308,
            "PATJ": 0.0164677420111589,
            "PBRM1": 0.044317821258459,
            "PBX2": 0.00320829748025083,
            "PCDH19": 0.0272872531074229,
            "PCF11": 0.0291705443705582,
            "PDS5B": 0.021698767270994696,
            "PELP1": 0.0373202471752503,
            "PEX19": 0.0132248961123523,
            "PFKP": 0.00034823519263003,
            "PHF14": 0.0297580241863299,
            "PHF20": 0.010155514207283065,
            "PHF3": 0.00124168361335943,
            "PI4KB": 0.0375025771584155,
            "PIK3AP1": 0.0252481993101516,
            "PITPNM3": 0.0168133556489441,
            "PKN1": 0.0425037272974999,
            "PLCH1": 0.0290345545706679,
            "PLEKHG2": 0.0303160095576093,
            "PLEKHM1": 0.027169999767462,
            "POP1": 0.0318857971755643,
            "PPIL6": 0.0465605467419352,
            "PPIP5K1": 0.0263869627455006,
            "PPP1R12A": 0.0347477199760802,
            "PPP1R14A": 0.0374144887933437,
            "PPP1R9B": 0.00815232888990476,
            "PPP4R2": 0.00641088493834257,
            "PPP6R2": 0.0332082349479447,
            "PRCC": 0.0110460079305209,
            "PRKCZ": 0.0443854065571069,
            "PRPF3": 0.0167825809464426,
            "PRPF38B": 0.02667985483378142,
            "PRRC2A": 0.0162882856389512,
            "PRRC2B": 0.021999780852726105,
            "PRRC2C": 0.00530469798848326,
            "PSMA2": 0.0101658193319756,
            "PTPN21": 0.03239689119022557,
            "PWWP2A": 0.0432594729442924,
            "RAB13": 0.0494174705629229,
            "RAB3IP": 0.0197997969511195,
            "RAB8A": 0.0186492495913322,
            "RAD50": 0.00626865096280216,
            "RAD9A": 0.0261263934393165,
            "RALGPS2": 0.0353669229204015,
            "RANBP2": 0.01840232892259712,
            "RB1": 0.019894896884840807,
            "RBBP5": 0.000545955530476966,
            "RBM12": 0.0102034242673521,
            "RBM15": 0.00426965673197485,
            "RBM15B": 0.0499983073946662,
            "RICTOR": 0.0272872531074229,
            "RLIM": 0.0302252052808086,
            "RNF168": 0.00682139309494012,
            "RNF4": 0.028399545589546948,
            "RPA1": 0.02427581947495829,
            "RPL30": 0.0355065359477124,
            "RPLP2": 0.00526230309191826,
            "RPS2": 0.0192834209107639,
            "RPS27": 0.00747588714540171,
            "RRBP1": 0.0365844158219992,
            "RREB1": 0.0249362393233076,
            "RSF1": 0.024039027030464438,
            "RSPH3": 0.0151020920805762,
            "RSPH4A": 0.013848228193634555,
            "RUBCN": 0.0365484291209722,
            "S100PBP": 0.03308185336504,
            "SACS": 0.0261263934393165,
            "SASH3": 0.0463806341791065,
            "SCRIB": 0.0374144887933437,
            "SEC16A": 0.0152443679189897,
            "SEL1L": 0.0476862206203252,
            "SET": 0.00340924548957681,
            "SETD1B": 0.0349645382832812,
            "SETD2": 0.02520368878493828,
            "SF3B1": 0.00425583102022784,
            "SF3B2": 0.0257041415329438,
            "SF3B3": 0.0150696287432672,
            "SIPA1L3": 0.0180754648214029,
            "SKA3": 0.00228078600498271,
            "SLBP": 0.0371408674066081,
            "SLC8A1": 0.0232741834932739,
            "SLK": 0.01928110738494836,
            "SMARCA1": 0.0293532485733266,
            "SMARCA4": 0.03214970436771115,
            "SMARCC1": 0.017453824072883,
            "SNX24": 0.034327414579176,
            "SOGA1": 0.0454691231013339,
            "SP140": 0.0234811824231471,
            "SPAG9": 0.020878258460720434,
            "SPATS2L": 0.0374144887933437,
            "SPEG": 0.0399751718574391,
            "SPEN": 0.02010414682979206,
            "SPP1": 0.0370751465629521,
            "SRPK2": 0.0106445729035592,
            "SRRM2": 0.03071484591912444,
            "SRSF11": 0.0492913248497778,
            "SRSF12": 0.029411523087563347,
            "SRSF2": 0.030790189398542653,
            "ST13": 0.0382559216084444,
            "STAU1": 0.0217280527645178,
            "STMN1": 0.00419495709841418,
            "STX17": 0.00148589263873434,
            "SYNGR2": 0.0144359469762761,
            "SYNPO": 0.020369419494287,
            "SYNRG": 0.018077287090468797,
            "SZRD1": 0.0360336537657683,
            "TAF15": 0.0327034020860868,
            "TANGO6": 0.0155898300627306,
            "TBX2": 0.03638005478163332,
            "TCF12": 0.0497992644587145,
            "TCF20": 0.0315773649909543,
            "TCOF1": 0.009315999464395012,
            "TERF2IP": 0.01999275807124975,
            "TFAM": 0.0341716449098999,
            "TGM2": 0.0275304040264136,
            "TLE2": 0.0156871837667427,
            "TMPO": 0.02590155835447288,
            "TNIP1": 0.0163057182734311,
            "TNRC6A": 0.0455837739848171,
            "TNS1": 0.0317740787112154,
            "TNS3": 0.0218235881321484,
            "TOMM70": 0.030900171772955152,
            "TP53BP1": 0.015914743806578344,
            "TRAPPC8": 0.0341716449098999,
            "TRIM28": 0.03068943571831135,
            "TRIT1": 0.0390932758187843,
            "TSC2": 0.026996410406962615,
            "TSHZ3": 0.0336356206979959,
            "TTF2": 0.0486250708765263,
            "TUBGCP3": 0.0274324904452464,
            "TXLNA": 0.0350854516754513,
            "UBA1": 0.014833098717392055,
            "UBAP2L": 0.0466049147436694,
            "UBE2O": 0.0314950361168889,
            "UCKL1": 0.00207247098461645,
            "UFD1": 0.023825581665335202,
            "UGDH": 0.00518328925662211,
            "UNC45A": 0.0496567595583864,
            "USF2": 0.0491924982363911,
            "USP10": 0.0037774448807903,
            "USP32": 0.0395243155467003,
            "USP47": 0.0138565710361594,
            "USP8": 0.0492742541491687,
            "USP9X": 0.0230727213712009,
            "UTP14A": 0.004274746861318775,
            "VIM": 0.0324706026414712,
            "WIPF2": 0.0156657923191347,
            "WNK1": 0.0219399301026229,
            "XRCC1": 0.00253764928159042,
            "XRCC6": 0.0060231290822709,
            "YEATS2": 0.03394187398361015,
            "ZBTB10": 0.0281384896852693,
            "ZBTB21": 0.00701550687582381,
            "ZBTB9": 0.0123832951383463,
            "ZC3H13": 0.027406113169242498,
            "ZC3H8": 0.0399768129248203,
            "ZC3HC1": 0.0004264386291949855,
            "ZFAND2B": 0.0375334621383179,
            "ZFAND3": 0.00115885398086681,
            "ZFAND5": 0.0198740515186142,
            "ZFC3H1": 0.0115115007567177,
            "ZFP91": 0.0218235881321484,
            "ZMYND11": 0.0270620914209947,
            "ZMYND8": 0.0168133556489441,
            "ZNF318": 0.026252497345200288,
            "ZNF326": 0.0173870040853599,
            "ZNF451": 0.01540701734134332,
            "ZNF592": 0.0202793914528402,
            "ZNF638": 0.0120809958371911,
            "ZNF687": 0.0415452010828718
          }
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# Helper functions\n# ------------------------------------------------------------------\ndef find_sheets_with_gene_fdr(excel_path):\n    \"\"\"Return sheets that contain both a gene column and an FDR column.\"\"\"\n    xl = pd.ExcelFile(excel_path)\n    sheets_with_info = []\n    for sheet in xl.sheet_names:\n        df = xl.parse(sheet, nrows=5)  # read only first rows to inspect headers\n        lower_cols = [c.lower() for c in df.columns]\n        has_gene = any(\"gene\" in c or \"symbol\" in c or \"name\" in c for c in lower_cols)\n        has_fdr  = any(\"fdr\" in c or \"qval\" in c or \"q-value\" in c or \"q value\" in c for c in lower_cols)\n        if has_gene and has_fdr:\n            sheets_with_info.append(sheet)\n    return sheets_with_info\n\ndef get_gene_and_fdr_columns(df):\n    \"\"\"Return best-guess column names for gene and FDR in a DataFrame.\"\"\"\n    lower_map = {c.lower(): c for c in df.columns}\n    # gene column\n    gene_col = None\n    for key in [\"gene\", \"gene_symbol\", \"symbol\", \"gene name\", \"hg symbol\", \"name\"]:\n        if key in lower_map:\n            gene_col = lower_map[key]\n            break\n    if gene_col is None:\n        # fallback: first column containing 'gene'\n        for lc, orig in lower_map.items():\n            if \"gene\" in lc or \"symbol\" in lc:\n                gene_col = orig\n                break\n    # FDR column\n    fdr_col = None\n    for key in [\"fdr\", \"adj p\", \"adjp\", \"q-value\", \"q value\", \"qval\", \"q_value\"]:\n        if key in lower_map:\n            fdr_col = lower_map[key]\n            break\n    if fdr_col is None:\n        for lc, orig in lower_map.items():\n            if \"fdr\" in lc or \"q\" in lc and \"val\" in lc:\n                fdr_col = orig\n                break\n    return gene_col, fdr_col\n\n# ------------------------------------------------------------------\n# Paths\n# ------------------------------------------------------------------\nexcel_file = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\n\n# ------------------------------------------------------------------\n# subtask-1: locate sheet(s) with gene & FDR columns\n# ------------------------------------------------------------------\nsheets_with_info = find_sheets_with_gene_fdr(excel_file)\n\n# ------------------------------------------------------------------\n# subtask-2: identify column names\n# ------------------------------------------------------------------\nsheet_cols = {}\nfor sheet in sheets_with_info:\n    df_preview = pd.read_excel(excel_file, sheet_name=sheet, nrows=200)\n    gene_col, fdr_col = get_gene_and_fdr_columns(df_preview)\n    if gene_col and fdr_col:\n        sheet_cols[sheet] = (gene_col, fdr_col)\n\n# ------------------------------------------------------------------\n# subtask-3: compute average FDR per gene\n# ------------------------------------------------------------------\navg_fdr_per_gene = {}\nfor sheet, (gene_col, fdr_col) in sheet_cols.items():\n    df = pd.read_excel(excel_file, sheet_name=sheet)\n    # Clean columns\n    df = df[[gene_col, fdr_col]].copy()\n    df[gene_col] = df[gene_col].astype(str).str.strip()\n    # convert FDR to numeric, coercing errors to NaN\n    df[fdr_col] = pd.to_numeric(df[fdr_col], errors='coerce')\n    # drop rows where either is NaN / empty gene\n    df = df.dropna(subset=[gene_col, fdr_col])\n    df = df[df[gene_col] != \"\"]\n    # group by gene, compute mean\n    avg_df = df.groupby(gene_col, as_index=False)[fdr_col].mean()\n    # merge into final dict (if multiple sheets, later overwrites earlier duplicates)\n    avg_fdr_per_gene.update(dict(zip(avg_df[gene_col], avg_df[fdr_col])))\n\n# ------------------------------------------------------------------\n# Construct answers for printing\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": sheets_with_info,\n    \"subtask-2\": sheet_cols,\n    \"subtask-3\": avg_fdr_per_gene,\n    \"main-task\": avg_fdr_per_gene   # main task asks for same in this step\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 24.164515256881714
      },
      {
        "task_id": "biomedical-easy-9-4",
        "model_output": {
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What is the mean FDR value for CBX3? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet in the Excel file contains a column with an FDR-like measure (e.g., \u201cFDR\u201d, \u201cadj.P.Val\u201d, \u201cq-value\u201d)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "What is the exact column name that stores the gene symbol (e.g., \u201cGene\u201d, \u201cgene_name\u201d, \u201csymbol\u201d)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            },
            {
              "id": "subtask-3",
              "query": "What is the exact column name that stores the FDR values?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            },
            {
              "id": "subtask-4",
              "query": "Compute the mean FDR for rows whose gene symbol equals \u201cCBX3\u201d (case-insensitive, ignoring leading/trailing spaces).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            }
          ],
          "answer": "** ERRORS ** found during execution in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3Naive/biomedical-easy-9-4/_intermediate/errors-0.txt"
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -----------------------------------------------------------\n# Helper functions\n# -----------------------------------------------------------\ndef find_sheet_with_fdr(xl):\n    \"\"\"\n    Return the first sheet that contains a column likely to be FDR.\n    \"\"\"\n    fdr_keywords = {\"fdr\", \"adj.p.val\", \"adj_p_val\", \"q-value\", \"padj\", \"p_adj\", \"fdr_bh\"}\n    for sheet in xl.sheet_names:\n        try:\n            df_head = xl.parse(sheet, nrows=5)   # quick peek\n        except Exception:\n            continue\n        normalized_cols = {c.strip().lower() for c in df_head.columns if isinstance(c, str)}\n        if normalized_cols & fdr_keywords:\n            return sheet\n    return None\n\n\ndef find_column(possible_names, columns):\n    \"\"\"\n    Look for the first column whose lower-case stripped name is in possible_names.\n    \"\"\"\n    normalized_names = {col.lower().strip(): col for col in columns}\n    for key in possible_names:\n        if key in normalized_names:\n            return normalized_names[key]\n    return None\n\n\n# -----------------------------------------------------------\n# Load the Excel file\n# -----------------------------------------------------------\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\nxl = pd.ExcelFile(file_path)\n\n# -----------------------------------------------------------\n# subtask-1 \u2013 locate the sheet that contains an FDR column\n# -----------------------------------------------------------\nsheet_with_fdr = find_sheet_with_fdr(xl)\n\n# -----------------------------------------------------------\n# Read that sheet fully (if found); otherwise default to the first sheet\n# -----------------------------------------------------------\ndf = xl.parse(sheet_with_fdr) if sheet_with_fdr else xl.parse(xl.sheet_names[0])\n\n# Standardise column names for easier search\ndf.columns = [c if not isinstance(c, str) else c.strip() for c in df.columns]\n\n# -----------------------------------------------------------\n# subtask-2 \u2013 find the gene symbol column\n# -----------------------------------------------------------\ngene_col_candidates = {\n    \"gene\", \"gene_name\", \"gene symbol\", \"symbol\", \"gene.symbol\",\n    \"genesymbol\", \"hgnc_symbol\", \"geneid\", \"gene id\"\n}\ngene_col = find_column(gene_col_candidates, df.columns)\n\n# -----------------------------------------------------------\n# subtask-3 \u2013 find the FDR column\n# -----------------------------------------------------------\nfdr_col_candidates = {\n    \"fdr\", \"adj.p.val\", \"adj_p_val\", \"q-value\", \"padj\", \"p_adj\", \"fdr_bh\"\n}\nfdr_col = find_column(fdr_col_candidates, df.columns)\n\n# -----------------------------------------------------------\n# Ensure we found the required columns; if not, raise informative error\n# -----------------------------------------------------------\nif gene_col is None:\n    raise ValueError(\"Could not locate the gene symbol column.\")\nif fdr_col is None:\n    raise ValueError(\"Could not locate the FDR column.\")\n\n# -----------------------------------------------------------\n# Clean data types\n# -----------------------------------------------------------\ndf[gene_col] = df[gene_col].astype(str).str.strip()\n# Coerce FDR column to numeric, forcing errors to NaN so they can be ignored\ndf[fdr_col] = pd.to_numeric(df[fdr_col], errors='coerce')\n\n# -----------------------------------------------------------\n# subtask-4 \u2013 compute mean FDR for CBX3\n# -----------------------------------------------------------\nmask_cbx3 = df[gene_col].str.upper() == \"CBX3\"\nmean_fdr_cbx3 = df.loc[mask_cbx3, fdr_col].mean()\n\n# -----------------------------------------------------------\n# Collect answers for printing\n# -----------------------------------------------------------\nanswers = {\n    \"subtask-1\": sheet_with_fdr,\n    \"subtask-2\": gene_col,\n    \"subtask-3\": fdr_col,\n    \"subtask-4\": mean_fdr_cbx3,\n    \"main-task\": mean_fdr_cbx3            # The mean FDR for CBX3 is the requested answer\n}\n\nprint(json.dumps(answers, indent=4, default=str))",
        "runtime": 23.846893548965454
      },
      {
        "task_id": "biomedical-easy-9-5",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What is the average FDR value for genes other than CBX3? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in the Excel file contain gene-level statistical results (i.e. columns with gene identifiers and an FDR/q-value/p-adj value)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "F-SS-phospho"
              ]
            },
            {
              "id": "subtask-2",
              "query": "What is/are the exact column name(s) that hold the gene identifiers (e.g. \u2018Gene\u2019, \u2018GeneSymbol\u2019, \u2018gene\u2019 \u2026) in the relevant sheet(s)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "gene"
            },
            {
              "id": "subtask-3",
              "query": "What is/are the exact column name(s) that hold the FDR (or q-value / adjusted p-value) in the relevant sheet(s)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "fdr"
            },
            {
              "id": "subtask-4",
              "query": "After loading the data, convert the FDR column to numeric, coercing errors to NaN, and drop rows with missing FDR values.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "630 rows retained after cleaning"
            },
            {
              "id": "subtask-5",
              "query": "Filter out the rows where the gene identifier equals \u2018CBX3\u2019 (case-insensitive) and calculate the average FDR of the remaining rows.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 0.02484464434616611
            }
          ],
          "answer": 0.02484464434616611
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\n\n# ----------------------------\n# Path to the source workbook\n# ----------------------------\nfile_path = Path(\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\n)\n\n# -------------------------------------------------\n# Helper: pick the first matching column if several\n# -------------------------------------------------\ndef pick_first(cols, keywords):\n    \"\"\"\n    Return the first column whose name contains *all*\n    substrings listed in `keywords` (case-insensitive).\n    \"\"\"\n    for c in cols:\n        lc = c.lower()\n        if all(k in lc for k in keywords):\n            return c\n    return None\n\n\n# --------------------------------------------------------------------\n# subtask-1, 2, 3 \u2013 inspect every sheet and keep the gene/FDR columns\n# --------------------------------------------------------------------\ndfs = []                              # store useful (gene, FDR) data frames\nxls = pd.ExcelFile(file_path)         # will read sheet names only once\n\nsheets_with_results = []              # for reporting (subtask-1)\n\nfor sheet in xls.sheet_names:\n    df = xls.parse(sheet)\n    # tidy column names (strip white-space but keep original case for later use)\n    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n\n    gene_col = pick_first(df.columns, [\"gene\"])        # e.g. \"Gene\", \"GeneSymbol\"\n    fdr_col  = (\n        pick_first(df.columns, [\"fdr\"])                # e.g. \"FDR\"\n        or pick_first(df.columns, [\"q\", \"value\"])      # e.g. \"q_value\"\n        or pick_first(df.columns, [\"padj\"])            # e.g. \"padj\"\n    )\n\n    if gene_col and fdr_col:\n        sheets_with_results.append(sheet)\n        sub = df[[gene_col, fdr_col]].copy()\n        sub.columns = [\"gene\", \"fdr\"]                  # standardise names\n        dfs.append(sub)\n\n# --------------------------------------------------------------------\n# Concatenate every (gene, FDR) pair we found (if any)\n# --------------------------------------------------------------------\nif not dfs:\n    raise ValueError(\"No (gene, FDR) columns found in the workbook.\")\n\nall_data = pd.concat(dfs, ignore_index=True)\n\n# -----------------------------------------\n# subtask-4 \u2013 clean/convert types & NaNs\n# -----------------------------------------\nall_data[\"gene\"] = (\n    all_data[\"gene\"]\n    .astype(str)\n    .str.strip()\n)  # keep as string for comparison\nall_data[\"fdr\"] = pd.to_numeric(all_data[\"fdr\"], errors=\"coerce\")\n\nclean = all_data.dropna(subset=[\"fdr\"])          # drop rows with missing FDR\n\n# ---------------------------------------------------------\n# subtask-5 \u2013 remove CBX3 rows and compute the mean FDR\n# ---------------------------------------------------------\nnon_cbx3 = clean[clean[\"gene\"].str.upper() != \"CBX3\"]\n\nif non_cbx3.empty:\n    avg_fdr_non_cbx3 = np.nan\nelse:\n    avg_fdr_non_cbx3 = non_cbx3[\"fdr\"].mean()\n\n# ----------------------------\n# Prepare answers to print out\n# ----------------------------\nanswers = {\n    \"subtask-1\": sheets_with_results,\n    \"subtask-2\": \"gene\",\n    \"subtask-3\": \"fdr\",\n    \"subtask-4\": f\"{len(clean)} rows retained after cleaning\",\n    \"subtask-5\": avg_fdr_non_cbx3,\n    \"main-task\": avg_fdr_non_cbx3,        # the requested value\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 31.798696041107178
      },
      {
        "task_id": "biomedical-easy-9-6",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What is the difference between the average FDR of CBX3 and the average FDR of the other genes? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Identify which sheet(s) in 1-s2.0-S0092867420301070-mmc3.xlsx contain both a gene-name column and an FDR (false discovery rate) column, and capture their column names.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                [
                  "F-SS-phospho",
                  "Gene",
                  "FDR.phos"
                ]
              ]
            },
            {
              "id": "subtask-2",
              "query": "From the sheet(s) identified in subtask-1, extract the rows whose gene symbol is EXACTLY \u201cCBX3\u201d, clean/convert the FDR column to numeric, drop missing or non-numeric values, and compute the average FDR for CBX3.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 0.016840668972299758
            },
            {
              "id": "subtask-3",
              "query": "From the same sheet(s), extract all rows whose gene symbol is NOT \u201cCBX3\u201d, clean/convert the FDR column to numeric, drop missing or non-numeric values, and compute the average FDR for these \u2018other\u2019 genes.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 0.024844644346166102
            },
            {
              "id": "subtask-4",
              "query": "Compute the numerical difference:  (average FDR of CBX3) \u2013 (average FDR of other genes).  Return the value.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": -0.008003975373866344
            }
          ],
          "answer": -0.008003975373866344
        },
        "code": "import json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# -----------------------------------------------------------\n# helper functions\n# -----------------------------------------------------------\ndef find_gene_fdr_columns(xl_file):\n    \"\"\"\n    Scan every sheet, look for a pair of columns that *appear* to be\n    gene names and FDR/q-values.  Return a list with tuples:\n    (sheet_name, gene_col_name, fdr_col_name)\n    \"\"\"\n    xl = pd.ExcelFile(xl_file)\n    candidates = []\n    for sh in xl.sheet_names:\n        df = xl.parse(sh, nrows=5)   # peek at a few rows\n        df_cols = [c.strip() if isinstance(c, str) else c for c in df.columns]\n\n        # possible gene columns\n        gene_cols = [c for c in df_cols if isinstance(c, str) and c.upper() in\n                     {\"GENE\", \"GENE_SYMBOL\", \"GENE SYMBOL\", \"GENE NAME\", \"SYMBOL\"}]\n\n        # possible FDR columns\n        fdr_cols = [c for c in df_cols if isinstance(c, str) and\n                    any(key in c.upper() for key in [\"FDR\", \"QVALUE\", \"Q-VALUE\", \"Q_VALUE\"])]\n\n        if gene_cols and fdr_cols:\n            candidates.append((sh, gene_cols[0], fdr_cols[0]))\n\n    # If nothing matched strict rules, fall back to *any* text + any fdr-like column\n    if not candidates:\n        xl = pd.ExcelFile(xl_file)\n        for sh in xl.sheet_names:\n            df = xl.parse(sh, nrows=5)\n            for g in df.columns:\n                for f in df.columns:\n                    if g != f and \"FDR\" in str(f).upper():\n                        candidates.append((sh, g, f))\n                        break\n    return candidates\n\n\ndef safe_to_numeric(series):\n    \"\"\"Convert a Series to numeric, coercing errors to NaN and dropping them\"\"\"\n    return pd.to_numeric(series, errors=\"coerce\")\n\n# -----------------------------------------------------------\n# File location\n# -----------------------------------------------------------\nxlsx_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\nassert xlsx_path.exists(), f\"File not found: {xlsx_path}\"\n\n# -----------------------------------------------------------\n# subtask-1 : locate sheets / columns\n# -----------------------------------------------------------\ngene_fdr_locations = find_gene_fdr_columns(xlsx_path)\n# For this benchmark file there is usually ONE relevant sheet,\n# but we keep the logic generic.\nsubtask_1_answer = gene_fdr_locations           # list of (sheet, gene_col, fdr_col)\n\n# -----------------------------------------------------------\n# Read data and concatenate rows from all identified sheets\n# -----------------------------------------------------------\nframes = []\nfor sheet, gene_col, fdr_col in gene_fdr_locations:\n    df = pd.read_excel(xlsx_path, sheet_name=sheet, dtype=str)  # read as text first\n    # standardise column names (strip spaces etc.)\n    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]\n    # rename chosen columns to unified names for easy concatenation\n    df = df.rename(columns={gene_col: \"Gene\", fdr_col: \"FDR\"})\n    frames.append(df[[\"Gene\", \"FDR\"]])\n\nif not frames:\n    raise ValueError(\"No sheet contained both gene and FDR columns.\")\n\nfull_df = pd.concat(frames, ignore_index=True)\n\n# Clean FDR column\nfull_df[\"FDR\"] = safe_to_numeric(full_df[\"FDR\"])\n\n# strip/uppercase gene names so \u201ccbx3\u201d, \u201cCBX3 \u201d, etc. all match\nfull_df[\"Gene\"] = full_df[\"Gene\"].astype(str).str.strip()\n\n# -----------------------------------------------------------\n# subtask-2 : average FDR for CBX3\n# -----------------------------------------------------------\ncbx3_avg = full_df.loc[full_df[\"Gene\"].str.upper() == \"CBX3\", \"FDR\"].dropna().mean()\nsubtask_2_answer = None if pd.isna(cbx3_avg) else float(cbx3_avg)\n\n# -----------------------------------------------------------\n# subtask-3 : average FDR for OTHER genes\n# -----------------------------------------------------------\nother_avg = full_df.loc[full_df[\"Gene\"].str.upper() != \"CBX3\", \"FDR\"].dropna().mean()\nsubtask_3_answer = None if pd.isna(other_avg) else float(other_avg)\n\n# -----------------------------------------------------------\n# subtask-4 / main-task : difference\n# -----------------------------------------------------------\nif pd.isna(cbx3_avg) or pd.isna(other_avg):\n    diff = None\nelse:\n    diff = float(cbx3_avg - other_avg)\n\nsubtask_4_answer = diff\nmain_task_answer = diff   # identical for this workflow\n\n# -----------------------------------------------------------\n# print answers\n# -----------------------------------------------------------\nprint(json.dumps({\n    \"subtask-1\": subtask_1_answer,\n    \"subtask-2\": subtask_2_answer,\n    \"subtask-3\": subtask_3_answer,\n    \"subtask-4\": subtask_4_answer,\n    \"main-task\": main_task_answer\n}, indent=4))",
        "runtime": 45.94605350494385
      }
    ],
    "runtime": 47.40934085845947
  }
]