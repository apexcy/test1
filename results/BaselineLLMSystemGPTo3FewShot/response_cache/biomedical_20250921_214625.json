[
  {
    "task_id": "biomedical-hard-1",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places.",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc2.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Find all sample IDs that correspond to tumour samples (and are flagged as \u2018in-study\u2019) in the metadata / annotation file(s).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "answer": []
        },
        {
          "id": "subtask-2",
          "query": "Extract the quantitative-proteomics abundance vector for the gene symbol PLK1 in those tumour samples.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": "0 abundances extracted"
        },
        {
          "id": "subtask-3",
          "query": "Extract the quantitative-proteomics abundance vector for the phosphosite CHEK2-S163 (CHEK2-pS163) in those same tumour samples.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": "0 abundances extracted"
        },
        {
          "id": "subtask-4",
          "query": "Using the two abundance vectors (PLK1 and CHEK2-S163) from identical tumour samples, drop any sample with a missing value in either vector and calculate the Pearson correlation coefficient.  Round the result to 4 decimal places.",
          "data_sources": [],
          "answer": null
        }
      ],
      "answer": null
    },
    "code": "import json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef read_all_sheets(path: Path) -> dict[str, pd.DataFrame]:\n    \"\"\"\n    Read every sheet of an Excel workbook into a dict of data-frames.\n    Everything is read as *string* first \u2013 we cast to numeric later.\n    \"\"\"\n    dfs: dict[str, pd.DataFrame] = {}\n    try:\n        xl = pd.ExcelFile(path, engine=\"openpyxl\")   # openpyxl is robust for xlsx\n        for sheet in xl.sheet_names:\n            try:\n                dfs[f\"{path.name}::{sheet}\"] = xl.parse(sheet, dtype=str)\n            except Exception:\n                # silently skip sheets that cannot be parsed by pandas\n                continue\n    except Exception as exc:\n        print(f\"Could not open \u201c{path}\u201d: {exc}\")\n    return dfs\n\n\ndef find_tumour_samples(meta_dfs: dict[str, pd.DataFrame]) -> list[str]:\n    \"\"\"\n    Return sample IDs that are (1) tumour and (2) \u201cin-study\u201d (if such a flag exists).\n    \"\"\"\n    tumour_ids: set[str] = set()\n\n    for df in meta_dfs.values():\n        if df.empty:\n            continue\n\n        df = df.copy()\n        df.columns = df.columns.str.strip().str.lower()\n\n        ann_cols      = [c for c in df.columns if any(k in c for k in\n                             [\"tumor\", \"tumour\", \"sample_type\", \"type\"])]\n        id_cols       = [c for c in df.columns if any(k in c for k in\n                             [\"sample\", \"id\"])]\n        in_study_cols = [c for c in df.columns if any(k in c for k in\n                             [\"in_study\", \"include\", \"use\", \"keep\"])]\n\n        if not ann_cols or not id_cols:\n            continue  # cannot decide for this sheet\n\n        ann_col = ann_cols[0]\n        id_col  = id_cols[0]\n\n        tmp = df[[ann_col, id_col] + in_study_cols].copy()\n\n        # keep \u201cin-study\u201d rows, if that information exists\n        if in_study_cols:\n            flag_col = in_study_cols[0]\n            tmp = tmp[tmp[flag_col].astype(str).str.lower().isin([\"1\", \"true\", \"yes\"])]\n\n        # keep tumour rows\n        tum_mask = tmp[ann_col].astype(str).str.lower().isin(\n            [\"tumor\", \"tumour\", \"primary\", \"cancer\"]\n        )\n        tumour_ids.update(tmp.loc[tum_mask, id_col].astype(str).str.strip())\n\n    return sorted(tumour_ids)\n\n\ndef orient_as_gene_rows(df: pd.DataFrame) -> pd.DataFrame | None:\n    \"\"\"\n    Return a data-frame where rows are gene / phosphosite identifiers\n    and columns are sample IDs.  Several heuristics are tried because\n    supplementary Excel formats differ.\n    \"\"\"\n    if df.empty:\n        return None\n\n    # Helper: does a Series contain *any* value with \u201cPLK1\u201d substring?\n    def series_has_plk1(s: pd.Series) -> bool:\n        return s.astype(str).str.upper().str.contains(\"PLK1\").any()\n\n    # 1) gene symbols already in the index?\n    if series_has_plk1(df.index.to_series()):\n        return df\n\n    # 2) gene symbols in a *column*  \u2192 use that column as index\n    for col in df.columns:\n        if series_has_plk1(df[col]):\n            return df.set_index(col)\n\n    # 3) gene symbols as *column headers*  \u2192 transpose\n    if any(\"PLK1\" in c.upper() for c in df.columns.astype(str)):\n        sample_col = df.columns[0]  # assume first column holds sample IDs\n        df2 = df.set_index(sample_col, drop=True)\n        return df2.T\n\n    # give up\n    return None\n\n\ndef extract_vector(\n    oriented_dfs: list[pd.DataFrame], identifier: str, sample_ids: list[str]\n) -> pd.Series:\n    \"\"\"\n    Extract the abundance vector for `identifier` restricted to `sample_ids`.\n    \"\"\"\n    identifier_clean = identifier.upper().replace(\"-\", \"\").replace(\"_\", \"\")\n\n    for df in oriented_dfs:\n        if df is None or df.empty:\n            continue\n\n        idx_clean = (\n            df.index.astype(str)\n            .str.upper()\n            .str.replace(\"-\", \"\")\n            .str.replace(\"_\", \"\")\n            .str.strip()\n        )\n\n        if identifier == \"PLK1\":\n            match = idx_clean.str.contains(\"PLK1\")\n        else:  # CHEK2-S163\n            match = idx_clean.str.contains(\"CHEK2\") & idx_clean.str.contains(\"S163\")\n\n        if match.any():\n            row = df.loc[match].iloc[0].copy()\n\n            # harmonise column names ~ sample IDs\n            row.index = row.index.astype(str).str.strip()\n\n            # keep tumour samples only, preserve their order\n            series = (\n                row.reindex(sample_ids)\n                   .astype(str)\n                   .replace({\"\": np.nan, \"NA\": np.nan, \"nan\": np.nan})\n            )\n            return pd.to_numeric(series, errors=\"coerce\")\n\n    raise ValueError(f\"Identifier \u201c{identifier}\u201d not found in any sheet.\")\n\n\n# ------------------------------------------------------------------\n# paths\n# ------------------------------------------------------------------\nbase = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input\")\nfile_meta      = base / \"1-s2.0-S0092867420301070-mmc2.xlsx\"  # sample / clinical info\nfile_prot_1    = base / \"1-s2.0-S0092867420301070-mmc1.xlsx\"  # one of the data tables\nfile_prot_2    = base / \"1-s2.0-S0092867420301070-mmc2.xlsx\"  # might also hold proteomics\n\n# ------------------------------------------------------------------\n# 1) tumour samples\n# ------------------------------------------------------------------\nmeta_sheets = read_all_sheets(file_meta)\ntumour_samples = find_tumour_samples(meta_sheets)\n\n# ------------------------------------------------------------------\n# 2) read *all* candidate sheets that might hold quantitative data\n# ------------------------------------------------------------------\nraw_sheets: dict[str, pd.DataFrame] = {}\nfor p in [file_prot_1, file_prot_2]:\n    raw_sheets.update(read_all_sheets(p))\n\n# orient those sheets that actually contain PLK1 (proxy for proteomics tables)\noriented_sheets = [orient_as_gene_rows(df) for df in raw_sheets.values()]\noriented_sheets = [df for df in oriented_sheets if df is not None]\n\nif not oriented_sheets:\n    raise RuntimeError(\"Could not orient any sheet into a gene-by-sample matrix.\")\n\n# ------------------------------------------------------------------\n# 3) extract abundance vectors\n# ------------------------------------------------------------------\nplk1_vec  = extract_vector(oriented_sheets, \"PLK1\",      tumour_samples)\nchek2_vec = extract_vector(oriented_sheets, \"CHEK2-S163\", tumour_samples)\n\n# ------------------------------------------------------------------\n# 4) correlation\n# ------------------------------------------------------------------\ncombined = (\n    pd.concat([plk1_vec, chek2_vec], axis=1, keys=[\"PLK1\", \"CHEK2-S163\"])\n      .dropna()\n)\n\ncorr_val = combined[\"PLK1\"].corr(combined[\"CHEK2-S163\"])\ncorr_rounded = round(corr_val, 4) if not np.isnan(corr_val) else None\n\n# ------------------------------------------------------------------\n# output\n# ------------------------------------------------------------------\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": tumour_samples,\n            \"subtask-2\": f\"{plk1_vec.notna().sum()} abundances extracted\",\n            \"subtask-3\": f\"{chek2_vec.notna().sum()} abundances extracted\",\n            \"subtask-4\": corr_rounded,\n            \"main-task\": corr_rounded,\n        },\n        indent=4,\n    )\n)",
    "subresponses": [
      {
        "task_id": "biomedical-hard-1-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n Which file(s) contain clinical information and proteomics data? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which file(s) contain clinical information?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which file(s) contain proteomics data (i.e., global protein abundance matrix)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": {
            "clinical_files": [
              "1-s2.0-S0092867420301070-mmc1.xlsx"
            ],
            "proteomics_files": []
          }
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------------------------\n# helper that tries to read an Excel file and returns its sheet names\ndef describe_excel(path, n_rows=3):\n    try:\n        xl = pd.ExcelFile(path)\n        sheets = xl.sheet_names\n        preview = {}\n        for s in sheets[:2]:                 # only check first two sheets to save time\n            try:\n                df = xl.parse(sheet_name=s, nrows=n_rows)\n                preview[s] = list(df.columns)\n            except Exception:\n                preview[s] = \"could not parse\"\n        return {\"sheets\": sheets, \"preview\": preview}\n    except Exception as e:\n        return {\"error\": str(e)}\n\nbase = Path('/home/gridsan/elai/Kramabench/data/biomedical/input')\nfiles = [f for f in base.glob('1-s2.0-S0092867420301070-mmc*.xlsx')]\n\nsummary = {}\nfor f in files:\n    summary[f.name] = describe_excel(f)\n\n# quick heuristic classification ------------------------------------------------\nclinical_files = []\nproteomics_files = []\n\nfor fname, info in summary.items():\n    if \"error\" in info:\n        continue\n    # look for columns that suggest sample meta data\n    flat_cols = set()\n    for cols in info[\"preview\"].values():\n        if isinstance(cols, list):\n            flat_cols.update([str(c).lower() for c in cols])\n    # simple rules\n    if any(key in flat_cols for key in [\"age\", \"sex\", \"tumor_stage\", \"sample id\", \"sample_id\"]):\n        clinical_files.append(fname)\n    if any(key.startswith(\"plk1\") or key.startswith(\"chek2\") for key in flat_cols):\n        proteomics_files.append(fname)\n    # also, big matrices often have dozens/hundreds of sheet names \u2013 check that\n    if len(info[\"sheets\"]) > 30:\n        proteomics_files.append(fname)\n\n# deduplicate\nclinical_files = sorted(set(clinical_files))\nproteomics_files = sorted(set(proteomics_files))\n\nanswers = {\n    \"subtask-1\": clinical_files,\n    \"subtask-2\": proteomics_files,\n    \"main-task\": {\n        \"clinical_files\": clinical_files,\n        \"proteomics_files\": proteomics_files\n    }\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 23.93791103363037
      },
      {
        "task_id": "biomedical-hard-1-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n Which cases id correspond to tumor samples in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheet(s) in 1-s2.0-S0092867420301070-mmc1.xlsx contain sample-level meta-data (case id / sample id and tissue type such as Tumor vs Normal)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "UCEC_CPTAC3_meta_table_V2.1"
            },
            {
              "id": "subtask-2",
              "query": "In the identified worksheet, what is the exact column that designates whether a sample is Tumor (vs Normal/adjacent)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Proteomics_Tumor_Normal"
            },
            {
              "id": "subtask-3",
              "query": "Filter the meta-data to rows where the tissue-type column equals \"Tumor\" (case-insensitive) and return the list of unique case ids corresponding to tumor samples in the study.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "No",
                "Yes"
              ]
            }
          ],
          "answer": [
            "No",
            "Yes"
          ]
        },
        "code": "import json\nimport pandas as pd\nimport re\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper -----------------------------------------------------------\ndef find_metadata_sheet(xls):\n    \"\"\"\n    Heuristic:  look for a sheet where at least one column contains\n    values such as 'Tumor' or 'Normal'. Return the sheet name.\n    \"\"\"\n    tumor_keywords = {\"tumor\", \"normal\", \"adjacent\", \"tissue\", \"solid\"}\n    for sheet in xls.sheet_names:\n        try:\n            df_sample = xls.parse(sheet, nrows=100)  # small peek\n        except Exception:\n            continue\n        for col in df_sample.columns:\n            # skip non-string columns quickly\n            try:\n                uniques = df_sample[col].dropna().astype(str).str.lower().unique()\n            except Exception:\n                continue\n            if any(any(k in str(u) for k in tumor_keywords) for u in uniques):\n                return sheet\n    return None\n\ndef find_tissue_col(df):\n    \"\"\"\n    Among dataframe columns, return column that contains 'tumor' or 'normal'\n    in its unique values.  If multiple, pick the first.\n    \"\"\"\n    for col in df.columns:\n        uniques = df[col].dropna().astype(str).str.lower().unique()\n        if any(u in {\"tumor\", \"normal\", \"normaladjacent\", \"normal_adjacent\"} for u in uniques):\n            return col\n    return None\n\ndef find_id_col(df):\n    \"\"\"\n    guess id column; prioritise those containing 'case', 'patient', 'sample'\n    \"\"\"\n    for pattern in ['case', 'patient', 'sample', 'id']:\n        candidates = [c for c in df.columns if re.search(pattern, str(c), re.I)]\n        if candidates:\n            return candidates[0]\n    # fallback to first column\n    return df.columns[0]\n\n# ------------------------------------------------------------------\nfile_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx')\nxls = pd.ExcelFile(file_path)\n\n# ---- subtask-1: locate meta sheet --------------------------------\nmeta_sheet = find_metadata_sheet(xls)\n\n# ---- subtask-2: locate tissue-type column ------------------------\nif meta_sheet:\n    meta_df_full = xls.parse(meta_sheet, dtype=str)  # read entire sheet as str to avoid dtype issues\n    tissue_col = find_tissue_col(meta_df_full)\nelse:\n    meta_df_full = pd.DataFrame()\n    tissue_col = None\n\n# ---- subtask-3: extract tumor case ids ---------------------------\nif tissue_col:\n    id_col = find_id_col(meta_df_full)\n    tumor_cases = (\n        meta_df_full[meta_df_full[tissue_col].str.lower().str.strip() == 'tumor'][id_col]\n        .dropna()\n        .unique()\n        .tolist()\n    )\nelse:\n    tumor_cases = []\n\n# Sort for reproducibility\ntumor_cases = sorted(tumor_cases)\n\n# ------------------------------------------------------------------\n# Print answers ----------------------------------------------------\nprint(json.dumps(\n    {\n        \"subtask-1\": meta_sheet,\n        \"subtask-2\": tissue_col,\n        \"subtask-3\": tumor_cases,\n        \"main-task\": tumor_cases  # same answer for this particular prompt\n    }, indent=4\n))",
        "runtime": 22.807645320892334
      },
      {
        "task_id": "biomedical-hard-1-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n What are the levels of the protein PLK1 for the cases in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in 1-s2.0-S0092867420301070-mmc2.xlsx contain protein (including phosphoprotein) abundance data for tumor samples?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "sheet_names": [
                  "README",
                  "A-global-proteomics",
                  "B-phospho-proteomics"
                ],
                "protein_sheets_detected": [
                  "README",
                  "A-global-proteomics",
                  "B-phospho-proteomics"
                ],
                "sheet_selected": "A-global-proteomics"
              }
            },
            {
              "id": "subtask-2",
              "query": "What is the exact column name (or index) that stores PLK1 protein abundance in the identified sheet?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "orientation": "rows",
                "plk1_column_name": "PLK1 located in a row"
              }
            },
            {
              "id": "subtask-3",
              "query": "After filtering to tumor samples that belong to the study and dropping rows with missing PLK1 values, what are the PLK1 abundance levels for each case (sample)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "S001": 0.295,
                "S002": 1.01,
                "S003": 0.301,
                "S004": 1.19,
                "S005": -0.00057,
                "S006": 2.46,
                "S007": -0.671,
                "S008": 0.232,
                "S009": 1.6,
                "S010": -1.2,
                "S011": -0.241,
                "S012": 0.109,
                "S013": 2.17,
                "S014": 0.722,
                "S015": 0.976,
                "S016": 0.786,
                "S017": 0.263,
                "S018": 0.0957,
                "S019": 0.163,
                "S020": -0.432,
                "S021": -0.595,
                "S022": 0.224,
                "S023": 0.881,
                "S024": 0.173,
                "S025": -0.00489,
                "S026": -0.461,
                "S027": 0.887,
                "S028": -0.377,
                "S029": 1.02,
                "S030": 0.767,
                "S031": 0.663,
                "S032": 0.197,
                "S033": -0.889,
                "S034": 1.54,
                "S035": -0.718,
                "S036": -0.0289,
                "S037": 0.9,
                "S038": 0.0136,
                "S039": -0.199,
                "S040": -0.565,
                "S041": 0.687,
                "S042": 1.07,
                "S043": 0.0421,
                "S044": -0.332,
                "S045": -0.529,
                "S046": -0.252,
                "S047": 0.0968,
                "S048": -0.0267,
                "S049": -0.0119,
                "S050": 0.316,
                "S051": 0.213,
                "S052": -0.442,
                "S053": -0.633,
                "S054": 0.328,
                "S055": -0.104,
                "S056": -0.715,
                "S057": 1.17,
                "S058": 2.14,
                "S059": 0.837,
                "S060": -0.583,
                "S061": 1.15,
                "S062": -0.247,
                "S063": -0.32,
                "S064": 0.404,
                "S065": 0.233,
                "S066": 0.65,
                "S067": -0.179,
                "S068": 0.0603,
                "S069": 0.467,
                "S070": -0.352,
                "S071": 1.13,
                "S072": 0.662,
                "S073": 1.07,
                "S074": 0.147,
                "S075": -0.0204,
                "S076": -0.625,
                "S077": 0.904,
                "S078": 0.132,
                "S079": 0.866,
                "S080": 0.0119,
                "S081": 1.02,
                "S082": 0.896,
                "S083": 0.557,
                "S084": -0.296,
                "S085": 0.554,
                "S086": 0.12,
                "S087": -0.0296,
                "S088": -0.0064,
                "S089": 1.54,
                "S090": -1.05,
                "S091": 1.1,
                "S092": 0.498,
                "S093": 0.4,
                "S094": 0.237,
                "S095": -0.687,
                "S096": 1.3,
                "S097": 1.36,
                "S098": 0.571,
                "S099": 0.229,
                "S100": 0.243,
                "S101": 0.953,
                "S102": 1.04,
                "S103": 0.55,
                "S104": -0.323,
                "S105": -0.433,
                "S106": -0.408,
                "S107": -0.0607,
                "S108": -0.365,
                "S109": -0.535,
                "S110": -0.409,
                "S111": -0.274,
                "S112": -0.747,
                "S113": -0.994,
                "S114": -1.03,
                "S115": -0.799,
                "S116": -0.749,
                "S117": -0.113,
                "S118": -1.01,
                "S119": -0.59,
                "S120": -0.549,
                "S121": -0.61,
                "S122": -0.0276,
                "S123": 0.138,
                "S124": -0.338,
                "S125": -0.027,
                "S126": -0.0747,
                "S127": -0.864,
                "S128": -0.915,
                "S129": -0.37,
                "S130": -0.525,
                "S131": -0.431,
                "S132": -1.68,
                "S133": -0.503,
                "S134": -0.549,
                "S135": -0.759,
                "S136": -0.226,
                "S137": -0.48,
                "S138": -0.697,
                "S139": -0.766,
                "S140": -1.08,
                "S141": -0.863,
                "S142": -1.11,
                "S143": -0.347,
                "S144": -1.21,
                "S145": -0.736,
                "S146": 0.284,
                "S147": 0.323,
                "S148": 0.68,
                "S149": -1.08,
                "S150": 0.752,
                "S151": -0.661,
                "S152": -0.683,
                "S153": 0.585
              }
            }
          ],
          "answer": {
            "S001": 0.295,
            "S002": 1.01,
            "S003": 0.301,
            "S004": 1.19,
            "S005": -0.00057,
            "S006": 2.46,
            "S007": -0.671,
            "S008": 0.232,
            "S009": 1.6,
            "S010": -1.2,
            "S011": -0.241,
            "S012": 0.109,
            "S013": 2.17,
            "S014": 0.722,
            "S015": 0.976,
            "S016": 0.786,
            "S017": 0.263,
            "S018": 0.0957,
            "S019": 0.163,
            "S020": -0.432,
            "S021": -0.595,
            "S022": 0.224,
            "S023": 0.881,
            "S024": 0.173,
            "S025": -0.00489,
            "S026": -0.461,
            "S027": 0.887,
            "S028": -0.377,
            "S029": 1.02,
            "S030": 0.767,
            "S031": 0.663,
            "S032": 0.197,
            "S033": -0.889,
            "S034": 1.54,
            "S035": -0.718,
            "S036": -0.0289,
            "S037": 0.9,
            "S038": 0.0136,
            "S039": -0.199,
            "S040": -0.565,
            "S041": 0.687,
            "S042": 1.07,
            "S043": 0.0421,
            "S044": -0.332,
            "S045": -0.529,
            "S046": -0.252,
            "S047": 0.0968,
            "S048": -0.0267,
            "S049": -0.0119,
            "S050": 0.316,
            "S051": 0.213,
            "S052": -0.442,
            "S053": -0.633,
            "S054": 0.328,
            "S055": -0.104,
            "S056": -0.715,
            "S057": 1.17,
            "S058": 2.14,
            "S059": 0.837,
            "S060": -0.583,
            "S061": 1.15,
            "S062": -0.247,
            "S063": -0.32,
            "S064": 0.404,
            "S065": 0.233,
            "S066": 0.65,
            "S067": -0.179,
            "S068": 0.0603,
            "S069": 0.467,
            "S070": -0.352,
            "S071": 1.13,
            "S072": 0.662,
            "S073": 1.07,
            "S074": 0.147,
            "S075": -0.0204,
            "S076": -0.625,
            "S077": 0.904,
            "S078": 0.132,
            "S079": 0.866,
            "S080": 0.0119,
            "S081": 1.02,
            "S082": 0.896,
            "S083": 0.557,
            "S084": -0.296,
            "S085": 0.554,
            "S086": 0.12,
            "S087": -0.0296,
            "S088": -0.0064,
            "S089": 1.54,
            "S090": -1.05,
            "S091": 1.1,
            "S092": 0.498,
            "S093": 0.4,
            "S094": 0.237,
            "S095": -0.687,
            "S096": 1.3,
            "S097": 1.36,
            "S098": 0.571,
            "S099": 0.229,
            "S100": 0.243,
            "S101": 0.953,
            "S102": 1.04,
            "S103": 0.55,
            "S104": -0.323,
            "S105": -0.433,
            "S106": -0.408,
            "S107": -0.0607,
            "S108": -0.365,
            "S109": -0.535,
            "S110": -0.409,
            "S111": -0.274,
            "S112": -0.747,
            "S113": -0.994,
            "S114": -1.03,
            "S115": -0.799,
            "S116": -0.749,
            "S117": -0.113,
            "S118": -1.01,
            "S119": -0.59,
            "S120": -0.549,
            "S121": -0.61,
            "S122": -0.0276,
            "S123": 0.138,
            "S124": -0.338,
            "S125": -0.027,
            "S126": -0.0747,
            "S127": -0.864,
            "S128": -0.915,
            "S129": -0.37,
            "S130": -0.525,
            "S131": -0.431,
            "S132": -1.68,
            "S133": -0.503,
            "S134": -0.549,
            "S135": -0.759,
            "S136": -0.226,
            "S137": -0.48,
            "S138": -0.697,
            "S139": -0.766,
            "S140": -1.08,
            "S141": -0.863,
            "S142": -1.11,
            "S143": -0.347,
            "S144": -1.21,
            "S145": -0.736,
            "S146": 0.284,
            "S147": 0.323,
            "S148": 0.68,
            "S149": -1.08,
            "S150": 0.752,
            "S151": -0.661,
            "S152": -0.683,
            "S153": 0.585
          }
        },
        "code": "import json\nimport pandas as pd\nimport numpy as np\n\nfile_path = (\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/\"\n    \"1-s2.0-S0092867420301070-mmc2.xlsx\"\n)\n\n# ----------------------------------------------------------------------\n# Helper functions\n# ----------------------------------------------------------------------\ndef column_orientation(sheet_name: str):\n    \"\"\"\n    Check if PLK1 appears in (part of) a column header for a given sheet.\n    Returns the column name if found, otherwise None.\n    \"\"\"\n    header_only = pd.read_excel(file_path, sheet_name=sheet_name, nrows=0)\n    for col in header_only.columns:\n        if isinstance(col, str) and \"plk1\" in col.lower():\n            return col\n    return None\n\n\ndef extract_from_column(sheet_name: str, plk1_col: str):\n    \"\"\"\n    Sheet layout: proteins are columns, samples are rows.\n    Build {sample_id: PLK1_abundance}.\n    \"\"\"\n    df = pd.read_excel(file_path, sheet_name=sheet_name)\n\n    # Heuristic: sample ID column = first non-numeric column\n    possible_id_cols = [c for c in df.columns if df[c].dtype == object]\n    sample_col = possible_id_cols[0] if possible_id_cols else df.columns[0]\n\n    df_subset = df[[sample_col, plk1_col]].copy()\n    df_subset.columns = [\"Sample\", \"PLK1\"]\n    df_subset[\"PLK1\"] = pd.to_numeric(df_subset[\"PLK1\"], errors=\"coerce\")\n    df_subset = df_subset.dropna(subset=[\"PLK1\"])\n    return df_subset.set_index(\"Sample\")[\"PLK1\"].to_dict()\n\n\ndef extract_from_row(sheet_name: str):\n    \"\"\"\n    Sheet layout: proteins are rows, samples are columns.\n    Locate the row corresponding to PLK1 and return {sample_id: abundance}.\n    \"\"\"\n    df = pd.read_excel(file_path, sheet_name=sheet_name)\n\n    # Build a boolean mask: rows where ANY cell equals 'PLK1' (case-insensitive).\n    mask = df.apply(\n        lambda row: row.astype(str).str.strip().str.upper().eq(\"PLK1\").any(), axis=1\n    )\n\n    if not mask.any():\n        return None  # PLK1 not found in this sheet\n\n    plk1_row = df.loc[mask].iloc[0]\n\n    # Keep only numeric values (sample abundance values)\n    plk1_row_numeric = pd.to_numeric(plk1_row, errors=\"coerce\").dropna()\n    return plk1_row_numeric.to_dict()\n\n\n# ----------------------------------------------------------------------\n# Main execution\n# ----------------------------------------------------------------------\nxls = pd.ExcelFile(file_path)\nsheet_names = xls.sheet_names\nprotein_sheets = [s for s in sheet_names if \"protein\" in s.lower()] or sheet_names\n\nplk1_levels = None\nsheet_used = None\nplk1_in_cols = None  # Stores column name if orientation == column\n\nfor sh in protein_sheets:\n    # 1) Try \"proteins as columns\" orientation  ------------------------\n    col_name = column_orientation(sh)\n    if col_name is not None:\n        try:\n            levels = extract_from_column(sh, col_name)\n            if levels:  # Found at least one numeric entry\n                plk1_levels = levels\n                sheet_used = sh\n                plk1_in_cols = col_name\n                break\n        except Exception:\n            # If something goes wrong, move on to the next sheet\n            pass\n\n    # 2) Try \"proteins as rows\" orientation  ---------------------------\n    levels = extract_from_row(sh)\n    if levels:\n        plk1_levels = levels\n        sheet_used = sh\n        plk1_in_cols = None\n        break\n\nif plk1_levels is None:\n    raise ValueError(\"PLK1 could not be located in any sheet of the workbook.\")\n\n# ----------------------------------------------------------------------\n# Build output for each subtask\n# ----------------------------------------------------------------------\noutput = {\n    \"subtask-1\": {\n        \"sheet_names\": sheet_names,\n        \"protein_sheets_detected\": protein_sheets,\n        \"sheet_selected\": sheet_used,\n    },\n    \"subtask-2\": {\n        \"orientation\": \"columns\" if plk1_in_cols else \"rows\",\n        \"plk1_column_name\": plk1_in_cols if plk1_in_cols else \"PLK1 located in a row\",\n    },\n    \"subtask-3\": plk1_levels,\n    \"main-task\": plk1_levels,\n}\n\nprint(json.dumps(output, indent=4, default=str))",
        "runtime": 156.78409385681152
      },
      {
        "task_id": "biomedical-hard-1-5",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n What are the levels of the protein CHEK2-S163 for the cases in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "What are the levels of the protein CHEK2-S163 for the cases in the study?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "S001": 0.462,
                "S004": 1.53,
                "S006": 3.07,
                "S008": -0.951,
                "S009": 2.28,
                "S010": 0.453,
                "S012": -0.925,
                "S013": 3.27,
                "S014": -0.443,
                "S015": 1.31,
                "S018": 0.0,
                "S019": -0.875,
                "S020": -1.07,
                "S021": 0.308,
                "S022": 0.475,
                "S023": -0.273,
                "S026": 0.0461,
                "S028": 0.356,
                "S029": 0.483,
                "S032": -0.0546,
                "S033": -1.37,
                "S034": -0.394,
                "S035": -0.47,
                "S037": -0.319,
                "S040": -0.366,
                "S041": 1.77,
                "S042": 1.7,
                "S049": -0.177,
                "S051": -0.8,
                "S052": -1.16,
                "S053": -0.727,
                "S054": 0.594,
                "S056": -0.917,
                "S058": 1.89,
                "S062": -1.02,
                "S063": -0.0239,
                "S064": -0.272,
                "S065": -0.249,
                "S067": -0.945,
                "S068": 0.0091,
                "S069": 0.682,
                "S070": -1.2,
                "S071": 1.4,
                "S073": -0.861,
                "S077": 0.201,
                "S079": -0.768,
                "S082": 1.05,
                "S084": -0.817,
                "S087": -0.0382,
                "S092": -0.752,
                "S093": -0.0416,
                "S096": 1.91,
                "S097": 2.04,
                "S098": 0.554,
                "S100": 0.134,
                "S101": -1.45,
                "S105": 0.496,
                "S107": 0.45,
                "S108": 0.0679,
                "S109": 0.103,
                "S110": 0.341,
                "S111": -0.325,
                "S113": 0.0629,
                "S114": 0.334,
                "S118": 0.0362,
                "S119": 0.782,
                "S120": 0.641,
                "S121": 0.903,
                "S124": 0.435,
                "S126": 0.491,
                "S128": -0.522,
                "S129": -0.0547,
                "S130": 0.798,
                "S133": 0.98,
                "S136": -1.05,
                "S137": -1.1,
                "S138": -0.0799,
                "S139": -0.0465,
                "S140": -2.17,
                "S141": -2.58,
                "S142": -1.64
              }
            },
            {
              "id": "subtask-2",
              "query": "What are the levels of the protein PLK1 for the cases in the study?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": "Warning: No answer found in the Python pipeline."
            },
            {
              "id": "subtask-3",
              "query": "Which samples are tumour samples that belong to the study (i.e. exclude samples flagged as not-in-study or with missing identifiers)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": "Warning: No answer found in the Python pipeline."
            },
            {
              "id": "subtask-4",
              "query": "Using the tumour samples identified in subtask-3, compute the Pearson correlation coefficient between PLK1 abundance (from subtask-2) and CHEK2-S163 abundance (from subtask-1), after dropping samples with missing values in either protein, and round the result to 4 decimal places.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": "Warning: No answer found in the Python pipeline."
            }
          ],
          "answer": "Warning: No answer found in the Python pipeline."
        },
        "code": "import json\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\")\n\n# ---------------------------\n# Helper: locate the sheet that contains the phosphoproteome data\n# (rows = proteins | phosphosites, columns = samples)\n# ---------------------------\ndef find_row_in_workbook(row_name, xls):\n    \"\"\"\n    Returns a Series whose index are sample IDs and whose values are abundances\n    for the requested row_name (e.g. 'CHEK2-S163').\n    The search is case-insensitive and ignores leading/trailing spaces.\n    \"\"\"\n    target = row_name.strip().lower()\n    for sheet in xls.sheet_names:\n        try:\n            # read with first column as index, let pandas try to guess dtypes\n            df = pd.read_excel(xls, sheet_name=sheet, header=0, index_col=0)\n        except Exception:\n            # sheet might be huge or badly formatted; skip if unreadable\n            continue\n        \n        # normalise the index for comparison\n        norm_index = df.index.astype(str).str.strip().str.lower()\n        if target in norm_index.values:\n            # fetch the exact label (in original capitalisation)\n            true_label = df.index[norm_index == target][0]\n            row_series = df.loc[true_label]\n            \n            # Retain only numeric columns (samples).  Some sheets have extra\n            # annotation columns (e.g. gene name, chromosome).  We coerce to\n            # float, turning non-numeric into NaN, then drop all-NaN columns.\n            row_series = pd.to_numeric(row_series, errors=\"coerce\")\n            row_series = row_series.dropna(how=\"all\")\n            return row_series, sheet, true_label\n    raise ValueError(f\"Row '{row_name}' was not found in any sheet.\")\n\n# ---------------------------\n# 1.  Extract CHEK2-S163 levels\n# ---------------------------\nxls = pd.ExcelFile(file_path)\nchek2_series, chek2_sheet, chek2_label = find_row_in_workbook(\"CHEK2-S163\", xls)\n\n# ---------------------------\n# (For completeness of the plan: we would repeat the above for PLK1 and then\n# identify tumour samples, but only subtask-1 is required to be answered here.)\n# ---------------------------\n\n# Prepare the answer for subtask-1 as a dict {sample_id: abundance}\nchek2_levels = chek2_series.dropna().to_dict()\n\n# ---------------------------\n# PRINT answers\n# ---------------------------\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": chek2_levels\n        },\n        indent=4,\n        default=lambda x: None   # ensure NaNs are serialised as null\n    )\n)",
        "runtime": 195.8987956047058
      },
      {
        "task_id": "biomedical-hard-1-6",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the correlation between the abundance of the protein PLK1 and the abundance of CHEK2-S163 in tumor samples? Exclude samples not in the study and with missing values. Round the result to 4 decimal places., please answer the following question:\n What is the correlation between the levels of the protein PLK1 and CHEK2? Round the result to 4 decimal places. \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc2.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the worksheet that contains quantitative protein-level data for both PLK1 and CHEK2 in the Excel file.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "sheet_name": "A-global-proteomics",
                "orientation": "row"
              }
            },
            {
              "id": "subtask-2",
              "query": "After locating the correct worksheet, extract the abundance vectors for PLK1 and CHEK2, restrict to samples that have non-missing measurements for both proteins (i.e. drop rows/columns with NA or non-numeric values), and calculate the Pearson correlation coefficient between the two vectors.  Round the result to 4 decimal places.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx"
              ],
              "answer": {
                "num_samples_used": 153,
                "correlation": 0.591
              }
            }
          ],
          "answer": 0.591
        },
        "code": "import pandas as pd\nimport numpy as np\nimport json\nimport re\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef _contains_gene(target: str, gene: str) -> bool:\n    \"\"\"\n    Return True if `gene` (case\u2013insensitive) is found inside `target`.\n    The function trims whitespaces and ignores everything after a '|'\n    (e.g.  'PLK1|9606-XYZ').\n    \"\"\"\n    if target is None:\n        return False\n    target_clean = str(target).split(\"|\")[0].strip().upper()\n    return gene.upper() in target_clean\n\n\ndef locate_sheet_with_genes(xl_file, genes):\n    \"\"\"\n    Iterate through every sheet in an Excel file and try to find a sheet\n    where *all* `genes` are present **either**\n        \u2022 in the columns   (protein rows = samples)\n        \u2022 in the 1st column (protein rows = genes)\n    Returns\n    -------\n    sheet_name : str   \u2013 first sheet that satisfies the condition\n    orientation : str  \u2013 'row'  \u2192 genes are in the rows  (1st column)\n                         'col'  \u2192 genes are in the columns\n    \"\"\"\n    xl = pd.ExcelFile(xl_file)\n\n    for sheet in xl.sheet_names:\n        # ------------  check if genes are in the COLUMNS ------------\n        try:\n            # header only \u2192 very cheap\n            df_head = pd.read_excel(xl, sheet_name=sheet, nrows=0)\n        except Exception:   # corrupted sheet, chart sheet, etc.\n            continue\n\n        cols_upper = [str(c).upper() for c in df_head.columns]\n\n        if all(any(_contains_gene(col, g) for col in cols_upper) for g in genes):\n            return sheet, \"col\"\n\n        # ------------  check if genes are in the ROWS ---------------\n        try:\n            # read ONLY the 1st column (no need to load whole sheet)\n            df_first_col = pd.read_excel(\n                xl, sheet_name=sheet, usecols=[0], dtype=str, header=None\n            )\n        except Exception:\n            continue\n\n        first_col_upper = df_first_col.iloc[:, 0].str.upper()\n\n        if all(any(_contains_gene(entry, g) for entry in first_col_upper.dropna()) for g in genes):\n            return sheet, \"row\"\n\n    # nothing found\n    return None, None\n\n\ndef extract_gene_series(df, gene_name, orientation):\n    \"\"\"\n    Return a pandas Series with the abundance values of `gene_name`.\n    The index of the Series will be the sample IDs.\n    \"\"\"\n    gene = gene_name.upper()\n\n    if orientation == \"col\":\n        # locate the column that contains the gene name\n        match_cols = [c for c in df.columns if _contains_gene(c, gene)]\n        if not match_cols:\n            raise KeyError(f\"Gene '{gene_name}' not found in the columns.\")\n        # take the first match\n        col = match_cols[0]\n        return pd.to_numeric(df[col], errors=\"coerce\")\n\n    elif orientation == \"row\":\n        # set gene names as index\n        df_indexed = df.set_index(df.columns[0])\n        df_indexed.index = df_indexed.index.astype(str).str.upper()\n\n        # locate rows that contain the gene\n        match_rows = df_indexed.index[df_indexed.index.to_series().apply(_contains_gene, args=(gene,))]\n        if match_rows.empty:\n            raise KeyError(f\"Gene '{gene_name}' not found in the first column.\")\n        row = df_indexed.loc[match_rows[0]]\n\n        return pd.to_numeric(row, errors=\"coerce\")\n\n    else:\n        raise ValueError(\"Orientation must be 'row' or 'col'.\")\n\n\n# ------------------------------------------------------------------\n# main script starts here\n# ------------------------------------------------------------------\nFILE  = \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\"\nGENES = [\"PLK1\", \"CHEK2\"]\n\n# -------------------  subtask-1  ----------------------------------\nsheet_name, orientation = locate_sheet_with_genes(FILE, GENES)\nanswer_subtask_1 = {\n    \"sheet_name\": sheet_name,\n    \"orientation\": orientation\n}\n\n# -------------------  subtask-2  ----------------------------------\nif sheet_name is None:\n    raise RuntimeError(\"Could not find a sheet that contains both PLK1 and CHEK2.\")\n\n# read the full sheet (may be large but unavoidable now)\ndf_full = pd.read_excel(FILE, sheet_name=sheet_name)\n\n# extract abundance vectors\nplk1_series  = extract_gene_series(df_full, \"PLK1\",  orientation)\nchek2_series = extract_gene_series(df_full, \"CHEK2\", orientation)\n\n# align indices (sample IDs) and drop missing values\ngene_df = pd.concat([plk1_series, chek2_series], axis=1, keys=[\"PLK1\", \"CHEK2\"])\ngene_df_clean = gene_df.dropna(how=\"any\")\n\n# calculate Pearson correlation\ncorr_value   = gene_df_clean[\"PLK1\"].corr(gene_df_clean[\"CHEK2\"])\ncorr_rounded = round(float(corr_value), 4)\n\nanswer_subtask_2 = {\n    \"num_samples_used\": int(gene_df_clean.shape[0]),\n    \"correlation\": corr_rounded\n}\n\n# -------------------  main-task  ----------------------------------\nanswer_main_task = corr_rounded\n\n# ------------------------------------------------------------------\n# print all answers\n# ------------------------------------------------------------------\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": answer_subtask_1,\n            \"subtask-2\": answer_subtask_2,\n            \"main-task\": answer_main_task\n        },\n        indent=4\n    )\n)",
        "runtime": 116.99720525741577
      }
    ],
    "runtime": 1289.9901297092438
  },
  {
    "task_id": "biomedical-easy-2",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the average age of patients with serous tumor samples analyzed in the study?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Which worksheet(s) in 1-s2.0-S0092867420301070-mmc1.xlsx contain patient-level metadata (age, histology / tumor type)?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": "UCEC_CPTAC3_meta_table_V2.1"
        },
        {
          "id": "subtask-2",
          "query": "What are the exact column names that store (a) patient age and (b) tumor histology / tumor type in the identified worksheet?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": {
            "age_column": "Path_Stage_Primary_Tumor-pT",
            "histology_column": "Proteomics_Tumor_Normal"
          }
        },
        {
          "id": "subtask-3",
          "query": "After coercing the age column to numeric and removing missing / non-numeric values, what are the ages corresponding to rows where the histology / tumor type column contains the word \u201cSerous\u201d (case-insensitive)?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": []
        },
        {
          "id": "subtask-4",
          "query": "What is the mean (average) of the ages obtained in subtask-3?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": null
        }
      ],
      "answer": null
    },
    "code": "import json\nimport pandas as pd\n\n# -------------------------------------------------\n# helper: load all sheets to locate the metadata\n# -------------------------------------------------\nxlsx_path = '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx'\nxls = pd.ExcelFile(xlsx_path)\nsheet_names = xls.sheet_names\n\n# --- subtask-1 -------------------------------------------------------------\nmeta_sheets = []\nfor s in sheet_names:\n    df_head = xls.parse(s, nrows=5)  # peek first few rows\n    cols_lower = [c.lower() for c in df_head.columns.astype(str)]\n    if any('age' in c for c in cols_lower) and (\n        any('histology' in c for c in cols_lower) or any('tumor' in c for c in cols_lower)\n    ):\n        meta_sheets.append(s)\n\n# assume first match is our metadata worksheet\nmeta_sheet = meta_sheets[0] if meta_sheets else None\n\n# --- subtask-2 -------------------------------------------------------------\ndf_meta = xls.parse(meta_sheet)\ncols_lower = df_meta.columns.str.lower()\n\n# try to locate columns\nage_col_candidates = [c for c in df_meta.columns if 'age' in c.lower()]\nhist_col_candidates = [\n    c for c in df_meta.columns if ('histology' in c.lower()) or ('tumor' in c.lower())\n]\n\nage_col = age_col_candidates[0]\nhist_col = hist_col_candidates[0]\n\n# --- subtask-3 -------------------------------------------------------------\n# clean age column\nages_raw = pd.to_numeric(df_meta[age_col], errors='coerce')\nhist_vals = df_meta[hist_col].astype(str)\n\nmask_serous = hist_vals.str.contains('serous', case=False, na=False)\nages_serous = ages_raw[mask_serous].dropna().astype(float).tolist()\n\n# --- subtask-4 -------------------------------------------------------------\navg_age = sum(ages_serous) / len(ages_serous) if ages_serous else None\n\n# -------------------------------------------------\n# print answers for each step\n# -------------------------------------------------\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": meta_sheet,\n            \"subtask-2\": {\"age_column\": age_col, \"histology_column\": hist_col},\n            \"subtask-3\": ages_serous,\n            \"subtask-4\": avg_age,\n            \"main-task\": avg_age,\n        },\n        indent=4,\n    )\n)",
    "subresponses": [
      {
        "task_id": "biomedical-easy-2-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n Which file(s) contains clinical information? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "For every Excel file, list all sheet names so that we know what tables are present.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": {
                "1-s2.0-S0092867420301070-mmc1.xlsx": [
                  "UCEC_CPTAC3_meta_table_V2.1"
                ],
                "1-s2.0-S0092867420301070-mmc2.xlsx": [
                  "README",
                  "A-global-proteomics",
                  "B-phospho-proteomics"
                ],
                "1-s2.0-S0092867420301070-mmc3.xlsx": [
                  "README",
                  "A-MSI",
                  "B-SE-proteomics",
                  "C-SE-phospho",
                  "D-SE-acetyl",
                  "E-Mutation-adjacent",
                  "F-SS-phospho"
                ],
                "1-s2.0-S0092867420301070-mmc4.xlsx": [
                  "README",
                  "A-Variants",
                  "B-Novel Splice Junctions",
                  "C-Alternate Splice Junctions"
                ],
                "1-s2.0-S0092867420301070-mmc5.xlsx": [
                  "README",
                  "A-SE in SCNA hotspots",
                  "B-circRNA and RBP correlation"
                ],
                "1-s2.0-S0092867420301070-mmc6.xlsx": [
                  "README",
                  "A-SE protein between Serous and",
                  "B-SE phospho site between Serou",
                  "C-SE scetyl site between Serous",
                  "D-SE protein between MSI-H and ",
                  "E-SE phospho site between MSI-H",
                  "F-SE scetyl site between MSI-H ",
                  "G-FDA approved drugs"
                ],
                "1-s2.0-S0092867420301070-mmc7.xlsx": [
                  "README",
                  "A-predicted neoantigen",
                  "B-APM subtypes"
                ]
              }
            },
            {
              "id": "subtask-2",
              "query": "Inspect the header row in every sheet and decide whether the sheet holds clinical/phenotypic metadata (e.g., columns such as age, sex, grade, stage, histology, overall survival). Return a list of files that do contain such information.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            }
          ],
          "answer": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ]
        },
        "code": "import pandas as pd\nimport json\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper that tries to read only header row of every sheet\n# ------------------------------------------------------------------\ndef get_sheet_headers(xlsx_path):\n    headers_per_sheet = {}\n    try:\n        # gather sheet names first\n        with pd.ExcelFile(xlsx_path) as xls:\n            for sheet in xls.sheet_names:\n                try:\n                    # read ONLY the first row (header row)\n                    df_head = pd.read_excel(xls, sheet_name=sheet, nrows=1, header=0, dtype=str)\n                    headers_per_sheet[sheet] = [c.strip().lower() for c in df_head.columns.astype(str)]\n                except Exception:\n                    # fallback \u2013 sometimes header may not exist or file encrypted\n                    headers_per_sheet[sheet] = []\n    except Exception:\n        # could not open the file\n        pass\n    return headers_per_sheet\n\n# ------------------------------------------------------------------\n# decide if any sheet in a file looks like clinical metadata\n# ------------------------------------------------------------------\nCLINICAL_KEYWORDS = {\n    'age', 'sex', 'gender', 'stage', 'grade', 'histology', 'histotype',\n    'survival', 'vital_status', 'days_to', 'os_time', 'follow_up',\n    'overall survival', 'tumor site', 'sample_id', 'patient', 'platinum',\n}\n\nxlsx_files = [\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx'\n]\n\nfile_contains_clinical = []\n\n# ------------------------------------------------------------------\n# SUBTASK-1: gather sheet names\n# ------------------------------------------------------------------\nsheet_names_by_file = {}\nfor f in xlsx_files:\n    try:\n        with pd.ExcelFile(f) as xls:\n            sheet_names_by_file[Path(f).name] = xls.sheet_names\n    except Exception:\n        sheet_names_by_file[Path(f).name] = []\n\n# ------------------------------------------------------------------\n# SUBTASK-2: inspect headers\n# ------------------------------------------------------------------\nfor f in xlsx_files:\n    headers = get_sheet_headers(f)\n    for sheet, cols in headers.items():\n        # check if any clinical keyword appears in the headers\n        if any(any(kw in c for kw in CLINICAL_KEYWORDS) for c in cols):\n            file_contains_clinical.append(Path(f).name)\n            break   # no need to look at other sheets in this file\n\n# deduplicate & sort\nfile_contains_clinical = sorted(set(file_contains_clinical))\n\n# ------------------------------------------------------------------\n# answers to print\n# ------------------------------------------------------------------\nanswers = {\n    \"subtask-1\": sheet_names_by_file,   # sheet names for transparency\n    \"subtask-2\": file_contains_clinical,\n    \"main-task\": file_contains_clinical\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 37.09187412261963
      },
      {
        "task_id": "biomedical-easy-2-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n Which cases id correspond to serous tumour samples included in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Load the Excel workbook and list all the sheet names so we know which sheet(s) may hold sample-level metadata.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "For every sheet, detect whether it contains a column that looks like a sample or patient identifier (e.g. Case ID, Sample ID, Patient, ID) AND a column describing tumour histology / subtype (e.g. Histology, Histotype, Diagnosis, Subtype).  Return the sheet names that pass this test so we know where to look for the serous samples.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-3",
              "query": "Within the sheet(s) identified, standardise column names (strip spaces, lower-case, remove punctuation) so that Case IDs and Histology fields can be accessed reliably, even if the file is messy.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Normalised column names and cleaned 0 candidate sheet(s)."
            },
            {
              "id": "subtask-4",
              "query": "Filter all rows whose histology / subtype contains the term \u201cserous\u201d (case-insensitive, also allow variants such as \u201chigh-grade serous\u201d, \u201cHGSC\u201d, etc.).  Extract the corresponding case IDs, drop duplicates, and sort them so we can report them cleanly.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport re\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# Helper functions\n# -------------------------------------------------\ndef normalise(col_name: str) -> str:\n    \"\"\"\n    Strip, lower-case and remove non-alphanumeric characters\n    so that 'Case ID', 'case_id', 'Case-Id' all map to 'caseid'.\n    \"\"\"\n    return re.sub(r'[^a-z0-9]', '', col_name.strip().lower())\n\n\ndef sheet_has_id_and_histology(df: pd.DataFrame) -> bool:\n    \"\"\"\n    Heuristic to decide if a sheet contains both an ID column and a histology column.\n    \"\"\"\n    norm_cols = [normalise(c) for c in df.columns]\n    has_id = any(re.search(r'\\b(id|case|sample|patient)\\b', c) for c in norm_cols)\n    has_hist = any(re.search(r'(hist|tumour|tumor|subtype|diagnosis)', c) for c in norm_cols)\n    return has_id and has_hist\n\n\ndef find_id_and_histology_columns(df: pd.DataFrame):\n    \"\"\"\n    Return the (id_col, hist_col) chosen heuristically.\n    \"\"\"\n    norm_map = {normalise(c): c for c in df.columns}\n    id_candidates = [c for n, c in norm_map.items() if re.search(r'\\b(id|case|sample|patient)\\b', n)]\n    hist_candidates = [c for n, c in norm_map.items() if re.search(r'(hist|tumour|tumor|subtype|diagnosis)', n)]\n    id_col = id_candidates[0] if id_candidates else None\n    hist_col = hist_candidates[0] if hist_candidates else None\n    return id_col, hist_col\n\n\nEXCEL_PATH = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\nanswers = {}\n# -------------------------------------------------\n# subtask-1: list sheet names\n# -------------------------------------------------\nexcel_file = pd.ExcelFile(EXCEL_PATH)\nanswers[\"subtask-1\"] = excel_file.sheet_names\n\n# -------------------------------------------------\n# subtask-2: detect candidate sheets\n# -------------------------------------------------\ncandidate_sheets = []\nfor sh in excel_file.sheet_names:\n    try:\n        df_head = pd.read_excel(EXCEL_PATH, sheet_name=sh, nrows=20)  # read a small chunk\n        if sheet_has_id_and_histology(df_head):\n            candidate_sheets.append(sh)\n    except Exception:\n        # Some sheets may be non-tabular (e.g. images) \u2011 skip\n        pass\n\nanswers[\"subtask-2\"] = candidate_sheets\n\n# -------------------------------------------------\n# subtask-3 & 4: extract serous cases\n# -------------------------------------------------\nserous_case_ids = set()\nfor sh in candidate_sheets:\n    df = pd.read_excel(EXCEL_PATH, sheet_name=sh)\n    id_col, hist_col = find_id_and_histology_columns(df)\n    if not id_col or not hist_col:\n        continue\n\n    # Standardise: coerce to str, drop NaNs\n    df[id_col] = df[id_col].astype(str).str.strip()\n    df[hist_col] = df[hist_col].astype(str).str.strip().str.lower()\n\n    # Filter histology containing 'serous' or common abbreviations\n    mask_serous = (\n        df[hist_col].str.contains(r'\\\\bserous\\\\b', case=False, na=False) |\n        df[hist_col].str.contains(r'\\\\bhgsc\\\\b', case=False, na=False)  # high-grade serous carcinoma\n    )\n    serous_case_ids.update(df.loc[mask_serous, id_col].dropna().unique().tolist())\n\nanswers[\"subtask-3\"] = f\"Normalised column names and cleaned {len(candidate_sheets)} candidate sheet(s).\"\nanswers[\"subtask-4\"] = sorted(serous_case_ids)\n\n# -------------------------------------------------\n# main task\n# -------------------------------------------------\nanswers[\"main-task\"] = sorted(serous_case_ids)\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 23.15912914276123
      },
      {
        "task_id": "biomedical-easy-2-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n Which is the age of patients with serous tumour samples in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the sheet(s) in the Excel file that contain clinical / sample meta-data (e.g. patient age, histology or tumor type).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the selected sheet(s), identify the column(s) that specify tumor histology or tumor type and the column that records patient age (e.g. \u201cHistology\u201d, \u201cTumor type\u201d, \u201cAge\u201d, \u201cAge (years)\u201d, etc.).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "UCEC_CPTAC3_meta_table_V2.1": [
                  "Proteomics_Tumor_Normal",
                  "Path_Stage_Primary_Tumor-pT"
                ]
              }
            },
            {
              "id": "subtask-3",
              "query": "Filter the rows so that only the samples with the word \u201cserous\u201d (case-insensitive) in the histology / tumor-type column are retained.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Number of serous-tumour rows extracted: 0"
            },
            {
              "id": "subtask-4",
              "query": "Extract the age values for those serous-tumor samples, coerce them to numeric, drop missing / non-numeric entries and list the unique ages observed.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-5",
              "query": "Compute the average (mean) age of the patients with serous tumor samples.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "average_age": null
              }
            }
          ],
          "answer": {
            "serous_tumor_patient_ages": [],
            "average_age": null
          }
        },
        "code": "import json\nimport re\nfrom pathlib import Path\n\nimport pandas as pd\n\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef is_histology_column(col_name: str) -> bool:\n    \"\"\"\n    Heuristic: does the column name look like it stores tumour\u2013type / histology\n    information?\n    \"\"\"\n    if not isinstance(col_name, str):\n        return False\n    lc = col_name.lower()\n    keys = [\"histology\", \"subtype\", \"tumor\", \"tumour\", \"pathology\",\n            \"morphology\", \"type\"]\n    return any(k in lc for k in keys)\n\n\ndef is_age_column(col_name: str) -> bool:\n    \"\"\"\n    Heuristic: does the column name look like it stores an age?\n    (simplest rule \u2013 it simply contains the word \u201cage\u201d)\n    \"\"\"\n    if not isinstance(col_name, str):\n        return False\n    return \"age\" in col_name.lower()\n\n\ndef extract_numeric(series: pd.Series) -> pd.Series:\n    \"\"\"\n    Pull the first numeric token (int/float) out of each cell in a pandas\n    Series.  Non-convertible entries become NaN.\n    \"\"\"\n    tmp = (\n        series.astype(str)\n        .str.extract(r\"(\\d+\\.?\\d*)\")[0]       # first capture group\n        .astype(float, errors=\"ignore\")       # still str if NA / non-numeric\n    )\n    return pd.to_numeric(tmp, errors=\"coerce\")    # final coercion\n\n\n# ------------------------------------------------------------------\n# main logic\n# ------------------------------------------------------------------\nxlsx_path = Path(\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/\"\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\"\n)\nassert xlsx_path.exists(), f\"File not found: {xlsx_path}\"\n\nxls = pd.ExcelFile(xlsx_path)\n\n# We will collect diagnostic details per sub-task:\ncandidate_sheets = []          # sub-task-1\ncolumn_map = {}                # sub-task-2   {sheet: (histology_col, age_col)}\nage_values = []                # sub-task-3 & 4\n\n# -- sub-task-1 & 2 -------------------------------------------------\nfor sheet in xls.sheet_names:\n    # Read a tiny preview; treat *everything* as str to avoid dtype surprises\n    try:\n        df_head = xls.parse(sheet, nrows=5, dtype=str)\n    except Exception as exc:\n        # Skip unreadable sheets\n        continue\n\n    cols = list(df_head.columns)\n    histology_in_sheet = [c for c in cols if is_histology_column(c)]\n    age_in_sheet = [c for c in cols if is_age_column(c)]\n\n    if histology_in_sheet:\n        candidate_sheets.append(sheet)\n\n    # record only the *first* hit per class (arbitrary but consistent)\n    if histology_in_sheet and age_in_sheet:\n        column_map[sheet] = (histology_in_sheet[0], age_in_sheet[0])\n\n# -- sub-task-3 & 4 -------------------------------------------------\nfor sheet, (hist_col, age_col) in column_map.items():\n    df = xls.parse(sheet, dtype=str)\n\n    if hist_col not in df.columns or age_col not in df.columns:\n        continue  # defensive, but shouldn\u2019t happen\n\n    mask = (\n        df[hist_col]\n        .astype(str)\n        .str.contains(r\"serous\", flags=re.IGNORECASE, na=False)\n    )\n    serous_df = df.loc[mask, [hist_col, age_col]].copy()\n    if serous_df.empty:\n        continue\n\n    serous_df[age_col] = extract_numeric(serous_df[age_col])\n    age_values.extend(serous_df[age_col].dropna().tolist())\n\n# -- sub-task-5 -----------------------------------------------------\nif age_values:\n    avg_age = sum(age_values) / len(age_values)\n    unique_ages_sorted = sorted(set(age_values))\nelse:\n    avg_age = None\n    unique_ages_sorted = []\n\n# ------------------------------------------------------------------\n# print answers in the requested JSON structure\n# ------------------------------------------------------------------\nanswer_dict = {\n    \"subtask-1\": candidate_sheets,\n    \"subtask-2\": column_map,\n    \"subtask-3\": f\"Number of serous-tumour rows extracted: {len(age_values)}\",\n    \"subtask-4\": unique_ages_sorted,\n    \"subtask-5\": {\"average_age\": avg_age},\n    \"main-task\": {\n        \"serous_tumor_patient_ages\": unique_ages_sorted,\n        \"average_age\": avg_age,\n    },\n}\n\nprint(json.dumps(answer_dict, indent=4))",
        "runtime": 65.09927368164062
      },
      {
        "task_id": "biomedical-easy-2-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the average age of patients with serous tumor samples analyzed in the study?, please answer the following question:\n What is the average the age of patients with serous tumour samples in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the sheet(s) in the Excel file that contain patient/sample-level metadata (columns such as Age, Histology/Tumor type).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the identified sheet(s) load the data, standardise column names, and isolate the two columns that hold patient age and tumour histology/type.  Convert the Age column to numeric, coercing non-numeric or missing values to NaN.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "153 total rows, columns=['age', 'histology']"
            },
            {
              "id": "subtask-3",
              "query": "Filter the metadata for rows where the tumour histology/type indicates a serous tumour (case-insensitive match for the word \"serous\").  Remove rows with missing age values.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "0 serous rows used for averaging"
            },
            {
              "id": "subtask-4",
              "query": "Calculate the average (mean) age of the remaining rows (patients with serous tumour samples).  Return the mean as a float rounded to one decimal place.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": null
            }
          ],
          "answer": null
        },
        "code": "import json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------------\ndef find_metadata_sheets(excel_path):\n    \"\"\"Return sheet names that look like they hold sample metadata.\"\"\"\n    xls = pd.ExcelFile(excel_path)\n    candidate = []\n    for sh in xls.sheet_names:\n        try:\n            # read only a few rows to inspect the headers\n            df_head = xls.parse(sh, nrows=5, header=0)\n        except Exception:\n            # sheet contains something that cannot be parsed as a table\n            continue\n        # normalise column names\n        cols = [str(c).lower() for c in df_head.columns]\n        has_age = any(\"age\" in c for c in cols)\n        has_hist = any((\"histology\" in c) or (\"tumor\" in c) or (\"histotype\" in c) for c in cols)\n        if has_age and has_hist:\n            candidate.append(sh)\n    return candidate\n\ndef load_and_prepare_metadata(excel_path, sheet_name):\n    \"\"\"Return a cleaned DataFrame with normalised column names.\"\"\"\n    df = pd.read_excel(excel_path, sheet_name=sheet_name, header=0)\n    # strip and lower column names to make them easy to work with\n    df.columns = [str(c).strip().lower() for c in df.columns]\n    # locate age column\n    age_col = [c for c in df.columns if \"age\" in c][0]  # first match\n    hist_col_candidates = [c for c in df.columns if (\"histology\" in c) or (\"histotype\" in c) or (\"tumor\" in c)]\n    hist_col = hist_col_candidates[0]\n    # keep only what we need\n    meta = df[[age_col, hist_col]].copy()\n    meta.rename(columns={age_col: \"age\", hist_col: \"histology\"}, inplace=True)\n    # clean age\n    meta[\"age\"] = pd.to_numeric(meta[\"age\"], errors=\"coerce\")\n    # clean histology text\n    meta[\"histology\"] = meta[\"histology\"].astype(str).str.lower()\n    return meta\n\n# ------------------------------------------------------------------\nexcel_file = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# ---------------- subtask-1 ---------------------------------------\nmeta_sheets = find_metadata_sheets(excel_file)\n\n# ---------------- subtask-2 & 3 -----------------------------------\nall_meta = []\nfor sh in meta_sheets:\n    try:\n        m = load_and_prepare_metadata(excel_file, sh)\n        all_meta.append(m)\n    except Exception:\n        # If something goes wrong with a sheet, ignore it (robustness)\n        continue\nif not all_meta:\n    raise RuntimeError(\"No metadata sheets with age & histology columns were found.\")\n\nmetadata = pd.concat(all_meta, ignore_index=True)\n\n# keep rows where histology contains the word \"serous\"\nserous_meta = metadata.loc[metadata[\"histology\"].str.contains(\"serous\", na=False)].copy()\nserous_meta = serous_meta.dropna(subset=[\"age\"])\n\n# ---------------- subtask-4 ---------------------------------------\naverage_age = float(np.round(serous_meta[\"age\"].mean(), 1)) if not serous_meta.empty else np.nan\n\n# ------------------------------------------------------------------\n# print answers for each task\nanswers = {\n    \"subtask-1\": meta_sheets,\n    \"subtask-2\": f\"{len(metadata)} total rows, columns={list(metadata.columns)}\",\n    \"subtask-3\": f\"{len(serous_meta)} serous rows used for averaging\",\n    \"subtask-4\": average_age,\n    \"main-task\": average_age\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 20.3179829120636
      }
    ],
    "runtime": 19.37446665763855
  },
  {
    "task_id": "biomedical-hard-3",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the age of the patient with the lowest APP-Z score?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc7.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Locate the worksheet(s) that contain both an APP-Z score column and an age column.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "answer": {
            "mmc1": [],
            "mmc7": [
              "B-APM subtypes"
            ]
          }
        },
        {
          "id": "subtask-2",
          "query": "Load the identified worksheet(s) and coerce the APP-Z and age columns to numeric, handling missing or non-numeric values.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "answer": "APP-Z and age columns converted to numeric; rows with NaN APP-Z dropped."
        },
        {
          "id": "subtask-3",
          "query": "Across all patients, find the minimum APP-Z score (ignoring missing values).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "answer": -2.15300777
        },
        {
          "id": "subtask-4",
          "query": "Return the age of the patient(s) whose APP-Z score equals the minimum identified in subtask-3.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "answer": "Warning: No answer found in the Python pipeline."
        }
      ],
      "answer": -0.0290411472699644
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\n\n# ------------------------ helper functions ----------------------------------\ndef find_relevant_sheets(xlsx_path, keywords=(\"app\", \"age\")):\n    \"\"\"\n    Return sheet names that contain all required keywords in their column headers.\n    Matching is case-insensitive and ignores non-alphanumeric characters.\n    \"\"\"\n    xl = pd.ExcelFile(xlsx_path)\n    relevant = []\n    for sheet in xl.sheet_names:\n        try:\n            df = xl.parse(sheet, nrows=2)     # just read first rows for speed\n        except Exception:\n            continue\n        cleaned_cols = [\"\".join(re.findall(r\"[a-z0-9]\", str(c).lower()))\n                        for c in df.columns]\n        if all(any(k in col for col in cleaned_cols) for k in keywords):\n            relevant.append(sheet)\n    return relevant\n\n\ndef safe_numeric(series):\n    \"\"\"Convert a Series to numeric, coercing errors to NaN.\"\"\"\n    return pd.to_numeric(series, errors='coerce')\n\n\n# --------------------------- locate sheets (subtask-1) -----------------------\nfiles = {\n    \"mmc1\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"mmc7\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\"\n}\n\nrelevant_sheets = {}\nfor tag, path in files.items():\n    if Path(path).exists():\n        relevant_sheets[tag] = find_relevant_sheets(path)\n\n# --------------------------- load & clean (subtask-2) ------------------------\nframes = []\nfor tag, path in files.items():\n    if not Path(path).exists():\n        continue\n    xl = pd.ExcelFile(path)\n    for sheet in relevant_sheets.get(tag, []):\n        df = xl.parse(sheet)\n        # Standardise column names\n        df.columns = [\"\".join(re.findall(r\"[a-z0-9]\", str(c).lower()))\n                      for c in df.columns]\n        # pick probable columns\n        app_cols = [c for c in df.columns if \"appz\" in c or re.fullmatch(r\"app.*z\", c)]\n        age_cols = [c for c in df.columns if \"age\" in c]\n        if not app_cols or not age_cols:\n            continue\n        # take first matches\n        df = df[[app_cols[0], age_cols[0]]].copy()\n        df.columns = [\"appz\", \"age\"]\n        df[\"appz\"] = safe_numeric(df[\"appz\"])\n        df[\"age\"] = safe_numeric(df[\"age\"])\n        frames.append(df)\n\nif not frames:\n    raise ValueError(\"No worksheet contained both APP-Z and age.\")\n\nall_data = pd.concat(frames, ignore_index=True)\n\n# --------------------------- find min APP-Z (subtask-3) ----------------------\nmin_appz = all_data[\"appz\"].min(skipna=True)\n\n# --------------------------- age for min APP-Z (subtask-4 / main) -----------\nages_at_min = all_data.loc[all_data[\"appz\"] == min_appz, \"age\"].dropna().unique()\n# If multiple ages share the same min APP-Z we report all of them\nanswer_main = ages_at_min.tolist() if len(ages_at_min) > 1 else ages_at_min[0]\n\n# --------------------------- print answers ----------------------------------\nprint(json.dumps(\n    {\n        \"subtask-1\": relevant_sheets,            # sheet names where columns were found\n        \"subtask-2\": \"APP-Z and age columns converted to numeric; rows with NaN APP-Z dropped.\",\n        \"subtask-3\": float(min_appz) if pd.notna(min_appz) else None,\n        \"main-task\": answer_main\n    },\n    indent=4\n))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-3-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What files contain clinical data and data about the APP-Z? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which of the supplemental Excel files contain patient-level clinical information (e.g., age, sex, outcome, comorbidities, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which of the supplemental Excel files contain APP-Z (amyloid precursor protein Z-score) measurements?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": {
            "clinical_files": [
              "1-s2.0-S0092867420301070-mmc1.xlsx"
            ],
            "appz_files": []
          }
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# Location map supplied by the benchmark runner\nbase_files = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\"\n]\n\nclinical_files = []\nappz_files = []\n\n# Helper keywords\nclinical_keywords = {\"age\", \"sex\", \"gender\", \"male\", \"female\", \"outcome\", \"comorbidity\", \"days\", \"patient\"}\nappz_keywords = {\"app-z\", \"app_z\", \"app z\", \"app-z\", \"amyloid precursor\", \"app score\"}\n\nfor file in base_files:\n    try:\n        # read only the first row of each sheet just to inspect column names quickly\n        xls = pd.ExcelFile(file)\n        for sheet in xls.sheet_names:\n            df_head = pd.read_excel(xls, sheet_name=sheet, nrows=1, dtype=str)\n            cols = set(c.lower() for c in df_head.columns.astype(str))\n            # Identify clinical files\n            if cols.intersection(clinical_keywords):\n                clinical_files.append(Path(file).name)\n            # Identify APP-Z files\n            if cols.intersection(appz_keywords):\n                appz_files.append(Path(file).name)\n    except Exception as e:\n        # Some sheets may be strange \u2013 just skip\n        # print(f\"Could not scan {file}: {e}\")\n        pass\n\n# Remove duplicates\nclinical_files = sorted(set(clinical_files))\nappz_files = sorted(set(appz_files))\n\nanswers = {\n    \"subtask-1\": clinical_files,\n    \"subtask-2\": appz_files,\n    \"main-task\": {\n        \"clinical_files\": clinical_files,\n        \"appz_files\": appz_files\n    }\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 20.004453420639038
      },
      {
        "task_id": "biomedical-hard-3-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What are the case ids included in the study with endometrioid or serous histological samples? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Find the sheet(s) inside 1-s2.0-S0092867420301070-mmc1.xlsx that contain both a case identifier column (e.g., \"Case ID\", \"case_id\", \"Case\") and a histology column (e.g., \"Histology\", \"Histologic_Type\").",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "From the sheet(s) identified in subtask-1, extract the unique case ids whose histology entry (case\u2013insensitive) contains either the word \"endometrioid\" or the word \"serous\".",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\n\nFILE = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\nanswers = {}\n\n# ---------- subtask-1 ----------\ncandidate_sheets = []\nwith pd.ExcelFile(FILE) as xl:\n    for sheet in xl.sheet_names:\n        try:\n            # Read a small sample with no header to speed things up\n            df_sample = xl.parse(sheet_name=sheet, dtype=str, nrows=40, header=None)\n        except Exception:\n            continue\n\n        # Detect potential header row (the first row that has \"histolog\" and \"case\")\n        header_row = None\n        for idx, row in df_sample.iterrows():\n            joined = \" | \".join(str(x).lower() for x in row.values if pd.notna(x))\n            if (\"histolog\" in joined) and (\"case\" in joined):\n                header_row = idx\n                break\n\n        if header_row is not None:\n            candidate_sheets.append((sheet, header_row))\n\nanswers[\"subtask-1\"] = [s for s, _ in candidate_sheets]\n\n# ---------- subtask-2 ----------\ncase_ids = set()\n\nwith pd.ExcelFile(FILE) as xl:\n    for sheet, header_row in candidate_sheets:\n        # Read the entire sheet using the detected header row\n        try:\n            df = xl.parse(sheet_name=sheet, dtype=str, header=header_row)\n        except Exception:\n            continue\n\n        # Normalise column names\n        df.columns = [str(c).strip().lower() for c in df.columns]\n\n        # Locate histology column\n        histo_cols = [c for c in df.columns if re.search(r\"histolog\", c)]\n        if not histo_cols:\n            continue\n        histo_col = histo_cols[0]\n\n        # Locate case column\n        case_cols = [c for c in df.columns if re.search(r\"case\", c)]\n        if not case_cols:\n            continue\n        case_col = case_cols[0]\n\n        # Filter rows with non-null histology\n        df_filtered = df[[case_col, histo_col]].dropna()\n\n        # Keep rows containing the keywords\n        mask = df_filtered[histo_col].str.contains(\"endometrioid\", case=False, na=False) | \\\n               df_filtered[histo_col].str.contains(\"serous\", case=False, na=False)\n\n        selected = df_filtered.loc[mask, case_col].dropna().unique()\n        case_ids.update(selected)\n\nanswers[\"subtask-2\"] = sorted(case_ids)\n\n# ---------- main-task ----------\nanswers[\"main-task\"] = sorted(case_ids)        # same as subtask-2 for this question\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 21.767069578170776
      },
      {
        "task_id": "biomedical-hard-3-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What is the value of the 'idx' column corresponding to the row with the minimum 'APP_Z_score'? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Load every worksheet in 1-s2.0-S0092867420301070-mmc7.xlsx and keep only the columns that can possibly contain \u2018idx\u2019 and \u2018APP_Z_score\u2019. Concatenate them into one DataFrame.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "95 rows after concatenation"
            },
            {
              "id": "subtask-2",
              "query": "Clean the concatenated DataFrame:  \n\u2022 rename columns so the two of interest are exactly \u2018idx\u2019 and \u2018APP_Z_score\u2019 (strip spaces, un-hide multi-level headers, lower-case, etc.)  \n\u2022 coerce both columns to numeric and drop rows where either is missing or not convertible.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "0 rows after cleaning"
            },
            {
              "id": "subtask-3",
              "query": "Find the minimum value of APP_Z_score and return the corresponding idx (if several rows tie, return all of their idx values as a Python list).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# subtask-1: read & concatenate all worksheets\n# -------------------------------------------------\nxls_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx')\n\n# read all sheets; keep plausible columns only\nsheets = pd.read_excel(xls_path, sheet_name=None, header=0)\n\nframes = []\nfor name, df in sheets.items():\n    # work on a copy so we do not alter the original\n    tmp = df.copy()\n\n    # lower-case & strip column names for easier matching\n    tmp.columns = [str(c).strip().lower() for c in tmp.columns]\n\n    # keep columns that look like idx or app_z_score\n    wanted = [c for c in tmp.columns\n              if ('idx' in c) or ('app' in c and 'z' in c)]\n    if wanted:\n        frames.append(tmp[wanted])\n\nconcat_df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n\n# -------------------------------------------------\n# subtask-2: clean and harmonise\n# -------------------------------------------------\n# create canonical names\nrename_map = {}\nfor c in concat_df.columns:\n    if 'idx' in c:\n        rename_map[c] = 'idx'\n    elif ('app' in c and 'z' in c):\n        rename_map[c] = 'app_z_score'\nconcat_df = concat_df.rename(columns=rename_map)\n\n# keep only the two columns\nconcat_df = concat_df[['idx', 'app_z_score']]\n\n# convert to numeric, coercing errors\nconcat_df['idx'] = pd.to_numeric(concat_df['idx'], errors='coerce')\nconcat_df['app_z_score'] = pd.to_numeric(concat_df['app_z_score'], errors='coerce')\n\n# drop rows with NA\nclean_df = concat_df.dropna(subset=['idx', 'app_z_score'])\n\n# -------------------------------------------------\n# subtask-3: locate minimum APP_Z_score and return idx\n# -------------------------------------------------\nmin_score = clean_df['app_z_score'].min()\nidx_with_min_score = clean_df.loc[clean_df['app_z_score'] == min_score, 'idx'].tolist()\n\n# if it is a single value, unwrap the list\nidx_answer = idx_with_min_score[0] if len(idx_with_min_score) == 1 else idx_with_min_score\n\nprint(json.dumps(\n    {\n        \"subtask-1\": f\"{len(concat_df)} rows after concatenation\",\n        \"subtask-2\": f\"{len(clean_df)} rows after cleaning\",\n        \"subtask-3\": idx_answer,\n        \"main-task\": idx_answer\n    },\n    indent=4\n))",
        "runtime": 34.1803777217865
      },
      {
        "task_id": "biomedical-hard-3-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the age of the patient with the lowest APP-Z score?, please answer the following question:\n What is the age of the patient with the identifier 'S019'? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Open the Excel file, list all sheet names and load each sheet into a DataFrame (treat the first row as header if possible).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "sheet_names": [
                  "UCEC_CPTAC3_meta_table_V2.1"
                ]
              }
            },
            {
              "id": "subtask-2",
              "query": "Inspect each DataFrame for a column that contains patient identifiers and confirm the row where the identifier equals 'S019' (case-sensitive).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "sheet": "UCEC_CPTAC3_meta_table_V2.1",
                "row_index": 18,
                "row_preview": {
                  "idx": "S019",
                  "Proteomics_Participant_ID": "C3L-00413",
                  "Case_excluded": "No",
                  "Proteomics_TMT_batch": 4,
                  "Proteomics_TMT_plex": 13,
                  "Proteomics_TMT_channel": "130N",
                  "Proteomics_Parent_Sample_IDs": "C3L-00413-01",
                  "Proteomics_Aliquot_ID": "CPT0009980003",
                  "Proteomics_Tumor_Normal": "Tumor",
                  "Proteomics_OCT": "No",
                  "Country": "United States",
                  "Histologic_Grade_FIGO": "FIGO grade 1",
                  "Myometrial_invasion_Specify": "under 50 %",
                  "Histologic_type": "Endometrioid",
                  "Treatment_naive": "YES",
                  "Tumor_purity": "Normal",
                  "Path_Stage_Primary_Tumor-pT": "pT2 (FIGO II)",
                  "Path_Stage_Reg_Lymph_Nodes-pN": "pN0",
                  "Clin_Stage_Dist_Mets-cM": "cM0",
                  "Path_Stage_Dist_Mets-pM": "Staging Incomplete",
                  "tumor_Stage-Pathological": "Stage II",
                  "FIGO_stage": "II",
                  "LVSI": 0.0,
                  "BMI": 42.19,
                  "Age": 60.0,
                  "Diabetes": "No",
                  "Race": "White",
                  "Ethnicity": "Not-Hispanic or Latino",
                  "Gender": "Female",
                  "Tumor_Site": "Other, specify",
                  "Tumor_Site_Other": "Both anterior and posterior",
                  "Tumor_Focality": "Unifocal",
                  "Tumor_Size_cm": 3.2,
                  "Estrogen_Receptor": "Cannot be determined",
                  "Estrogen_Receptor_%": null,
                  "Progesterone_Receptor": "Cannot be determined",
                  "Progesterone_Receptor_%": null,
                  "MLH1": "Intact nuclear expression",
                  "MLH2": "Intact nuclear expression",
                  "MSH6": "Intact nuclear expression",
                  "PMS2": "Intact nuclear expression",
                  "p53": "Cannot be determined",
                  "Other_IHC_specify": null,
                  "MLH1_Promoter_Hypermethylation": "Cannot be determined",
                  "Num_full_term_pregnancies": 3,
                  "EPIC_Bcells": 0.002976646,
                  "EPIC_CAFs": 0.032530637,
                  "EPIC_CD4_Tcells": 0.072912978,
                  "EPIC_CD8_Tcells": 0.076101921,
                  "EPIC_Endothelial": 0.074618826,
                  "EPIC_Macrophages": 0.00116644,
                  "EPIC_NKcells": 1.17e-09,
                  "EPIC_otherCells": 0.739692551,
                  "CIBERSORT_B _cells _naive": 0.012463677,
                  "CIBERSORT_B _cells _memory": 0.0,
                  "CIBERSORT_Plasma _cells": 0.001624051,
                  "CIBERSORT_T _cells _CD8": 0.0,
                  "CIBERSORT_T _cells _CD4 _naive": 0.0,
                  "CIBERSORT_T _cells _CD4 _memory _resting": 0.104674412,
                  "CIBERSORT_T _cells _CD4 _memory _activated": 0.0,
                  "CIBERSORT_T _cells _follicular _helper": 0.060904492,
                  "CIBERSORT_T _cells _regulatory _(Tregs)": 0.0,
                  "CIBERSORT_T _cells _gamma _delta": 0.0,
                  "CIBERSORT_NK _cells _resting": 0.029287744,
                  "CIBERSORT_NK _cells _activated": 0.0,
                  "CIBERSORT_Monocytes": 0.033466742,
                  "CIBERSORT_Macrophages _M0": 0.048188695,
                  "CIBERSORT_Macrophages _M1": 0.0,
                  "CIBERSORT_Macrophages _M2": 0.022231967,
                  "CIBERSORT_Dendritic _cells _resting": 0.0,
                  "CIBERSORT_Dendritic _cells _activated": 0.042889981,
                  "CIBERSORT_Mast _cells _resting": 0.0,
                  "CIBERSORT_Mast _cells _activated": 0.018753335,
                  "CIBERSORT_Eosinophils": 0.018264584,
                  "CIBERSORT_Neutrophils": 0.01725032,
                  "CIBERSORT_Absolute _score": 0.41,
                  "ESTIMATE_StromalScore": 1184.03053,
                  "ESTIMATE_ImmuneScore": 3696.358111,
                  "ESTIMATE_ESTIMATEScore": 4880.388642,
                  "Stemness_score": 0.501202113,
                  "ER_ESR1": -1.63,
                  "PR_PGR": -2.57,
                  "Pathway_activity_EGFR": 1.78,
                  "Pathway_activity_Hypoxia": 1.11,
                  "Pathway_activity_JAK.STAT": -1.4,
                  "Pathway_activity_MAPK": 1.54,
                  "Pathway_activity_NFkB": -0.23,
                  "Pathway_activity_PI3K": 1.19,
                  "Pathway_activity_TGFb": 0.57,
                  "Pathway_activity_TNFa": 0.0,
                  "Pathway_activity_Trail": -1.02,
                  "Pathway_activity_VEGF": 2.24,
                  "Pathway_activity_p53": 1.64,
                  "TP53_ATM": 0.0,
                  "TP53_CHEK2": 0.0,
                  "TP53_MDM4": 0.0,
                  "TP53_RPS6KA3": 0.0,
                  "TP53_TP53": 0.0,
                  "TP53_pathway": 0.0,
                  "PI3K_AKT1": 0.0,
                  "PI3K_AKT2": 0.0,
                  "PI3K_AKT3": 0.0,
                  "PI3K_DEPDC5": 0.0,
                  "PI3K_DEPTOR": 0.0,
                  "PI3K_INPP4B": 0.0,
                  "PI3K_MAPKAP1": 0.0,
                  "PI3K_MLST8": 0.0,
                  "PI3K_MTOR": 0.0,
                  "PI3K_NPRL2": 0.0,
                  "PI3K_NPRL3": 0.0,
                  "PI3K_PDK1": 0.0,
                  "PI3K_PIK3CA": 0.0,
                  "PI3K_PIK3CB": 0.0,
                  "PI3K_PIK3R1": 0.0,
                  "PI3K_PIK3R2": 0.0,
                  "PI3K_PPP2R1A": 0.0,
                  "PI3K_PTEN": 1.0,
                  "PI3K_RHEB": 0.0,
                  "PI3K_RICTOR": 0.0,
                  "PI3K_RPS6": 0.0,
                  "PI3K_RPS6KB1": 0.0,
                  "PI3K_RPTOR": 0.0,
                  "PI3K_STK11": 0.0,
                  "PI3K_TSC1": 0.0,
                  "PI3K_TSC2": 0.0,
                  "PI3K_pathway": 1.0,
                  "HRD_BRCA1": 0.0,
                  "HRD_BRCA2": 0.0,
                  "HRD_BRCA1_or_BRCA2": 0.0,
                  "CNV_DEL": 0.001083995,
                  "CNV_AMP": 0.167850707,
                  "CNV_class": "CNV_LOW",
                  "CNV_idx": 2.92410267,
                  "CNV_1q_DEL": -0.091810653,
                  "CNV_3q_DEL": -0.097520363,
                  "CNV_4q_DEL": -0.103711441,
                  "CNV_1q_AMP": 3.8e-05,
                  "CNV_3q_AMP": 0.000125954,
                  "CNV_4q_AMP": 7.34e-05,
                  "Purity_Immune": 0.158828136,
                  "Purity_Cancer": 0.632079059,
                  "Purity_Stroma": 0.209092805,
                  "MSI_status": "MSS",
                  "POLE_subtype": "No",
                  "JAK1_MS_INDEL": "WT",
                  "JAK1_Mutation": "WT",
                  "Log2_variant_per_Mbp": 1.331721187,
                  "Log2_SNP_per_Mbp": 1.258934249,
                  "Log2_INDEL_per_Mbp": 0.168435383,
                  "Log2_variant_total": 5.64385619,
                  "Log2_SNP_total": 5.523561956,
                  "Log2_INDEL_total": 2.321928095,
                  "Genomics_subtype": "CNV_low",
                  "Mutation_signature_C>A": 15.55555556,
                  "Mutation_signature_C>G": 8.888888889,
                  "Mutation_signature_C>T": 57.77777778,
                  "Mutation_signature_T>C": 8.888888889,
                  "Mutation_signature_T>A": 4.444444444,
                  "Mutation_signature_T>G": 4.444444444,
                  "WXS_normal_sample_type": "Blood_normal",
                  "WXS_normal_filename": "92bebf14-5fd0-453e-99a3-c7041b9cde22_gdc_realn.bam",
                  "WXS_normal_UUID": "6058412c-d366-44dd-a0fc-4aad9f6166dc",
                  "WXS_tumor_sample_type": "Tumor",
                  "WXS_tumor_filename": "ef798c59-f595-45b8-960e-6b36c8169fb9_gdc_realn.bam",
                  "WXS_tumor_UUID": "f9bf7283-eb23-477d-bc36-b928c96ff1f2",
                  "WGS_normal_sample_type": "Blood_normal",
                  "WGS_normal_UUID": "22c5648f-e238-4e98-b2d5-32d41ab94314",
                  "WGS_tumor_sample_type": "Tumor",
                  "WGS_tumor_UUID": "7cd7dd5b-4c04-4f55-88c5-73a813a1b7a8",
                  "RNAseq_R1_sample_type": "Tumor",
                  "RNAseq_R1_filename": "170818_UNC32-K00270_0051_BHLCCHBBXX_GTGAAA_S1_L006_R1_001.fastq.gz",
                  "RNAseq_R1_UUID": "e46aae2a-0162-443c-9f2d-7f95cd2f132e",
                  "RNAseq_R2_sample_type": "Tumor",
                  "RNAseq_R2_filename": "170818_UNC32-K00270_0051_BHLCCHBBXX_GTGAAA_S1_L006_R2_001.fastq.gz",
                  "RNAseq_R2_UUID": "05b8da1e-d495-4023-9aca-b71417350ee1",
                  "miRNAseq_sample_type": "Tumor",
                  "miRNAseq_UUID": "77466cca-387b-4e58-861c-bb2e74085989",
                  "Methylation_available": "YES",
                  "Methylation_quality": "PASS"
                }
              }
            },
            {
              "id": "subtask-3",
              "query": "Within the same row found in subtask-2, locate the column that represents the patient\u2019s age. Ensure the value is numeric (cast to int if necessary, handling missing or non-numeric entries). Return that age.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "age_column": "Age",
                "age_value": 60
              }
            }
          ],
          "answer": 60
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# helper: robustly load every sheet from the excel file\n# -------------------------------------------------\nexcel_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\nxls = pd.ExcelFile(excel_path)\n\nsheets_data = {}\nfor sheet in xls.sheet_names:\n    try:\n        df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\")\n    except Exception:\n        # fallback \u2013 sometimes sheets lack proper header row; try header=None\n        df = pd.read_excel(excel_path, sheet_name=sheet, engine=\"openpyxl\", header=None)\n    sheets_data[sheet] = df\n\n# -------------------------------------------------\n# search for identifier 'S019' across all sheets\n# -------------------------------------------------\nidentifier = \"S019\"\nrow_found = None\nage_value = None\nsheet_with_row = None\n\nfor sheet, df in sheets_data.items():\n    for col in df.columns:\n        # Convert column to string to search for the identifier robustly\n        series_str = df[col].astype(str).str.strip()\n        match_idx = series_str[series_str.eq(identifier)].index\n        if len(match_idx) > 0:\n            idx = match_idx[0]\n            row_found = df.loc[idx]\n            sheet_with_row = sheet\n            break\n    if row_found is not None:\n        break\n\nif row_found is None:\n    raise ValueError(\"Identifier S019 not found in any sheet.\")\n\n# -------------------------------------------------\n# try to determine which column is age\n# -------------------------------------------------\npossible_age_cols = [c for c in row_found.index if str(c).lower() in {\"age\", \"patient age\", \"age (years)\", \"years\", \"year\", \"yrs\"}]\n\nif not possible_age_cols:\n    # fallback: look for any column that contains the word 'age'\n    possible_age_cols = [c for c in row_found.index if \"age\" in str(c).lower()]\n\nif not possible_age_cols:\n    # heuristic: find first numeric value that looks like an age (2 <= x <= 120)\n    for c in row_found.index:\n        try:\n            val = float(row_found[c])\n            if 2 <= val <= 120:\n                possible_age_cols = [c]\n                break\n        except Exception:\n            continue\n\nif not possible_age_cols:\n    raise ValueError(\"Could not identify an age column for the row with identifier S019.\")\n\nage_raw_value = row_found[possible_age_cols[0]]\n\n# clean / cast\ntry:\n    age_value = int(float(age_raw_value))\nexcept Exception:\n    # handle non-numeric (e.g., '45y') by stripping non-digit chars\n    import re\n    digits = re.findall(r\"\\d+\", str(age_raw_value))\n    age_value = int(digits[0]) if digits else None\n\nif age_value is None:\n    raise ValueError(f\"Unable to parse age from value '{age_raw_value}'.\")\n\n# -------------------------------------------------\n# print answers per step\n# -------------------------------------------------\nprint(json.dumps({\n    \"subtask-1\": {\n        \"sheet_names\": xls.sheet_names\n    },\n    \"subtask-2\": {\n        \"sheet\": sheet_with_row,\n        \"row_index\": int(row_found.name) if isinstance(row_found.name, (int, float)) else str(row_found.name),\n        \"row_preview\": row_found.to_dict()\n    },\n    \"subtask-3\": {\n        \"age_column\": possible_age_cols[0],\n        \"age_value\": age_value\n    },\n    \"main-task\": age_value\n}, indent=4))",
        "runtime": 17.104523181915283
      }
    ],
    "runtime": 20.743818998336792
  },
  {
    "task_id": "biomedical-hard-4",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc4.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Locate every occurrence of the peptide sequence \u2018HPKPEVLGSSADGALLVSLDGLR\u2019 in 1-s2.0-S0092867420301070-mmc1.xlsx and collect the (unique) sample identifiers in which that peptide was detected.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": []
        },
        {
          "id": "subtask-2",
          "query": "Using the sample identifiers obtained in subtask-1, find the corresponding histological grade(s) of the tumours in 1-s2.0-S0092867420301070-mmc4.xlsx.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc4.xlsx"
          ],
          "answer": {}
        },
        {
          "id": "subtask-3",
          "query": "Aggregate the histological grade values returned in subtask-2 and output the set of distinct grades for which the peptide HPKPEVLGSSADGALLVSLDGLR was observed.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc4.xlsx"
          ],
          "answer": []
        }
      ],
      "answer": []
    },
    "code": "import json\nimport re\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nfrom warnings import filterwarnings\n\n# ---------------------------------------------------------------------\n# helpers\n# ---------------------------------------------------------------------\ndef load_all_sheets(xlsx_path: Path) -> dict[str, pd.DataFrame]:\n    \"\"\"\n    Read every sheet of an Excel file into a dict of data\u2010frames.\n    Everything is read as *string* so we do not suffer from mixed dtypes.\n    \"\"\"\n    xls = pd.ExcelFile(xlsx_path)\n    sheets: dict[str, pd.DataFrame] = {}\n    for sheet in xls.sheet_names:\n        try:\n            df = xls.parse(sheet, dtype=str)       # raw strings everywhere\n            df.columns = df.columns.map(str)\n            sheets[sheet] = df\n        except Exception as e:                     # pragma: no cover\n            # keep going even if one sheet is malformed\n            print(f\"[warn] could not read sheet {sheet!r}: {e}\")\n    return sheets\n\n\ndef normalise_id(val: str) -> str:\n    \"\"\"\n    Utility that lower-cases, strips and removes non\u2013alphanumeric\n    characters so we can perform fuzzy matches between identifiers\n    originating from different tables.\n    \"\"\"\n    if val is None:\n        return \"\"\n    return re.sub(r\"\\W+\", \"\", str(val)).lower().strip()\n\n\ndef is_numeric_string(val: str) -> bool:\n    \"\"\"\n    Return True if *val* looks like a number.\n    \"\"\"\n    try:\n        float(val)\n        return True\n    except Exception:\n        return False\n\n\n# silence the many \u201cUnknown extension\u201d warnings from openpyxl\nfilterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl\")\n\n# ---------------------------------------------------------------------\n# paths\n# ---------------------------------------------------------------------\nroot = Path(\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input\"\n)  # directory that stores the files\n\nmmc1_path = root / \"1-s2.0-S0092867420301070-mmc1.xlsx\"\nmmc4_path = root / \"1-s2.0-S0092867420301070-mmc4.xlsx\"\n\n# ---------------------------------------------------------------------\n# sub-task-1  \u2013\u2013\u2013\u2013\u2013 find every sample that contains the peptide\n# ---------------------------------------------------------------------\npeptide = \"HPKPEVLGSSADGALLVSLDGLR\"\nsheets1 = load_all_sheets(mmc1_path)\n\nsample_ids: set[str] = set()\n\nfor sheet, df in sheets1.items():\n\n    # -------- \u2776 the \u201cpeptide as column\u201d layout -----------------------\n    # (rows = samples, columns = peptides / measurements)\n    for col in df.columns:\n        series = df[col].dropna().astype(str)\n\n        if not series.str.contains(peptide, case=False, regex=False).any():\n            continue  # peptide not in this column\n\n        # rows in which the peptide occurs (exact or substring)\n        mask = series.str.contains(peptide, case=False, regex=False)\n\n        # try to find a column that looks like sample ID\n        id_cols = [\n            c\n            for c in df.columns\n            if any(k in c.lower() for k in [\"sample\", \"tumor\", \"patient\", \"case\", \"id\"])\n        ]\n        id_col = id_cols[0] if id_cols else None\n\n        if id_col:\n            # the dataframe contains an explicit sample-id column\n            sample_ids.update(df.loc[mask, id_col].dropna().map(str))\n        else:\n            # fallback: the row-index *is* the sample identifier\n            sample_ids.update(df.index[mask].map(str))\n\n    # -------- \u2777 the \u201cpeptide as row\u201d layout -------------------------\n    # first, locate rows whose *any* cell equals the peptide\n    rows_with_peptide = df.apply(\n        lambda r: r.astype(str).str.contains(peptide, case=False, regex=False).any(), axis=1\n    )\n\n    if not rows_with_peptide.any():\n        continue  # peptide not present as a row\n\n    peptide_rows = df.loc[rows_with_peptide]\n\n    # (a) if there is an explicit sample column, grab it\n    id_cols = [\n        c\n        for c in df.columns\n        if any(k in c.lower() for k in [\"sample\", \"tumor\", \"patient\", \"case\", \"id\"])\n    ]\n    for id_col in id_cols:\n        sample_ids.update(peptide_rows[id_col].dropna().map(str))\n\n    # (b) frequently the *columns* beyond the first few metadata\n    #     columns are individual samples containing numeric values.\n    #     We regard such column names as sample IDs if the cell value\n    #     is not missing / zero.\n    for _, row in peptide_rows.iterrows():\n        for col, cell in row.items():\n            cell_str = str(cell).strip()\n            if cell_str in (\"\", \"na\", \"nan\"):\n                continue\n            # ignore clearly non-sample columns\n            if any(\n                kw in col.lower()\n                for kw in [\"peptide\", \"sequence\", \"gene\", \"protein\", \"description\", \"charge\"]\n            ):\n                continue\n            # if the cell looks numeric, it is probably an intensity\n            if is_numeric_string(cell_str):\n                sample_ids.add(col)\n\n# tidy up\nsubtask1_answer = sorted(set(map(str, sample_ids)))\n\n# ---------------------------------------------------------------------\n# sub-task-2  \u2013\u2013\u2013\u2013\u2013 map sample \u2192 histological grade\n# ---------------------------------------------------------------------\nsheets4 = load_all_sheets(mmc4_path)\ngrade_lookup: defaultdict[str, set[str]] = defaultdict(set)\n\nnormalised_samples = {normalise_id(s): s for s in subtask1_answer}\n\nfor sheet, df in sheets4.items():\n    # find suitable columns\n    id_candidates = [\n        c\n        for c in df.columns\n        if any(k in c.lower() for k in [\"sample\", \"tumor\", \"patient\", \"id\", \"case\"])\n    ]\n    grade_candidates = [\n        c\n        for c in df.columns\n        if any(k in c.lower() for k in [\"grade\", \"histologic\", \"histological\"])\n    ]\n    if not id_candidates or not grade_candidates:\n        continue\n\n    for id_col in id_candidates:\n        ids = df[id_col].astype(str).str.strip()\n        ids_norm = ids.map(normalise_id)\n\n        for grade_col in grade_candidates:\n            grades = df[grade_col].astype(str).str.strip()\n\n            for raw_id, norm_id, grade in zip(ids, ids_norm, grades):\n                if norm_id in normalised_samples and grade not in (\"\", \"nan\", \"na\"):\n                    original_id = normalised_samples[norm_id]\n                    grade_lookup[original_id].add(grade)\n\n# aggregate distinct grades\nall_grades = sorted({g for gs in grade_lookup.values() for g in gs})\n\nsubtask2_answer = {k: sorted(v) for k, v in grade_lookup.items()}\nsubtask3_answer = all_grades\n\n# ---------------------------------------------------------------------\n# final \u2013\u2013\u2013\u2013\u2013 print everything\n# ---------------------------------------------------------------------\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": subtask1_answer,\n            \"subtask-2\": subtask2_answer,\n            \"subtask-3\": subtask3_answer,\n            \"main-task\": all_grades,\n        },\n        indent=4,\n    )\n)",
    "subresponses": [
      {
        "task_id": "biomedical-hard-4-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found?, please answer the following question:\n What file(s) contains clinical information and peptides found per tumor samples? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which file(s) contain clinical metadata (e.g., histological grade, stage, age, sex, etc.) for each tumor sample?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which file(s) contain peptide\u2010level information mapped to individual tumor samples (i.e., a sample or patient identifier together with peptide sequences detected in that sample)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx"
              ]
            },
            {
              "id": "subtask-3",
              "query": "Intersect the results of subtask-1 and subtask-2 to list the file(s) that contain BOTH clinical information and peptides per tumor sample.",
              "data_sources": [],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ]
            }
          ],
          "answer": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        },
        "code": "import json\nimport pandas as pd\nimport os\n\n# path list provided by the benchmark\nbase_paths = [\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx',\n    '/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx',\n]\n\nclinical_cols_keywords = ['grade', 'histolog', 'stage', 'age', 'sex', 'gender']\npeptide_cols_keywords  = ['peptide', 'sequence', 'seq', 'peptides']\n\nclinical_files = set()\npeptide_files  = set()\n\ndef contains_keyword(cols, keywords):\n    \"\"\"\n    Check whether any of the provided keywords appears as a substring\n    in any of the column names.\n    \"\"\"\n    cols_lower = [str(col).lower() for col in cols]\n    return any(\n        any(keyword in col for col in cols_lower)\n        for keyword in keywords\n    )\n\nfor f in base_paths:\n    try:\n        xls = pd.ExcelFile(f)\n    except Exception:\n        # some supplementary xlsx files are very large or malformed \u2013 skip if unreadable\n        continue\n\n    for sheet in xls.sheet_names:\n        try:\n            # read just a few rows to inspect the header\n            df = xls.parse(sheet, nrows=5)\n        except Exception:\n            # skip sheets that cannot be parsed\n            continue\n\n        if contains_keyword(df.columns, clinical_cols_keywords):\n            clinical_files.add(os.path.basename(f))\n\n        if contains_keyword(df.columns, peptide_cols_keywords):\n            peptide_files.add(os.path.basename(f))\n\nfiles_with_both = sorted(list(clinical_files & peptide_files))\n\nprint(json.dumps(\n    {\n        \"subtask-1\": sorted(list(clinical_files)),\n        \"subtask-2\": sorted(list(peptide_files)),\n        \"subtask-3\": files_with_both,\n        \"main-task\": files_with_both  # propagates to the main question\n    },\n    indent=4\n))",
        "runtime": 45.44465255737305
      },
      {
        "task_id": "biomedical-hard-4-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What are the histological grades of tumors for which the peptide HPKPEVLGSSADGALLVSLDGLR was found?, please answer the following question:\n What samples are found with the peptide HPKPEVLGSSADGALLVSLDGLR? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate the worksheet(s) in every Excel file that contain peptide-to-sample information. Identify the column names that store the peptide sequence and the sample identifier.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "Identified worksheets by scanning each Excel file \u2013 see code comments."
            },
            {
              "id": "subtask-2",
              "query": "From the identified worksheets, extract every row where the peptide sequence exactly equals \u201cHPKPEVLGSSADGALLVSLDGLR\u201d (case-sensitive match after stripping white-space).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "Matched rows where any peptide column == 'HPKPEVLGSSADGALLVSLDGLR'."
            },
            {
              "id": "subtask-3",
              "query": "Collect the sample identifier(s) associated with the matching peptide rows, cast them to string, drop missing or duplicate values, and return the final list of sample IDs.",
              "data_sources": [],
              "answer": [
                "S028",
                "S034",
                "S040"
              ]
            }
          ],
          "answer": [
            "S028",
            "S034",
            "S040"
          ]
        },
        "code": "import json\nimport re\nimport warnings\nfrom pathlib import Path\n\nimport pandas as pd\n\n# ------------------------------------------------------------------\n# Silence the \u201cUnknown extension is not supported and will be removed\u201d\n# warning that openpyxl emits when it meets custom XML extensions\n# inside the supplementary Excel files.\n# ------------------------------------------------------------------\nwarnings.filterwarnings(\n    \"ignore\",\n    category=UserWarning,\n    module=r\"openpyxl\\.worksheet\\._reader\",\n)\n\n# ------------------------------------------------------------------\n# helper -----------------------------------------------------------\ndef search_peptide_in_excel(excel_path: str | Path, peptide: str) -> set[str]:\n    \"\"\"\n    Scan every worksheet in an Excel file and return the *sample IDs*\n    (format S### \u2026) for rows whose peptide column equals `peptide`.\n\n    The implementation is defensive with respect to:\n        \u2022 unknown / varying column names\n        \u2022 different data-types inside the same column\n        \u2022 cells that contain several comma-separated sample IDs\n    \"\"\"\n    peptide = peptide.strip()          # safety \u2013 remove surrounding blanks\n    sample_ids: set[str] = set()\n\n    # --------------------------------------------------------------\n    # 1) Try opening the workbook\n    # --------------------------------------------------------------\n    try:\n        xls = pd.ExcelFile(excel_path)\n    except Exception as e:             # corrupted / encrypted file etc.\n        print(f\"[WARN] Could not open {excel_path}: {e}\")\n        return sample_ids\n\n    # --------------------------------------------------------------\n    # 2) Iterate over every sheet in the workbook\n    # --------------------------------------------------------------\n    for sheet_name in xls.sheet_names:\n        try:\n            # Read *all* columns as string to prevent dtype troubles\n            df = xls.parse(sheet_name=sheet_name, dtype=str)\n        except Exception as e:         # unreadable sheet (very rare but possible)\n            print(f\"[WARN]   skipped sheet {sheet_name!r} in {excel_path!s} ({e})\")\n            continue\n\n        if df.empty:\n            continue\n\n        # ----------------------------------------------------------\n        # 3) Normalise column names\n        # ----------------------------------------------------------\n        df.columns = df.columns.str.strip().str.lower()\n\n        # ----------------------------------------------------------\n        # 4) Detect peptide columns\n        # ----------------------------------------------------------\n        known_pep_cols = {\n            \"peptide\", \"sequence\", \"peptideseq\",\n            \"peptide sequence\", \"modified sequence\",\n        }\n        peptide_cols = [c for c in df.columns if c in known_pep_cols]\n\n        # If none of the canonical names exist, fall back to a brute-force search:\n        if not peptide_cols:\n            peptide_cols = list(df.columns)\n\n        # ----------------------------------------------------------\n        # 5) Search rows that match the peptide\n        # ----------------------------------------------------------\n        for pcol in peptide_cols:\n            mask = df[pcol].astype(str).str.strip() == peptide\n            if not mask.any():\n                continue\n\n            matched_rows = df.loc[mask]\n\n            # ------------------------------------------------------\n            # 6) Extract sample IDs from the *entire* matched rows\n            #    (safer than relying on a particular column name)\n            # ------------------------------------------------------\n            for cell in matched_rows.values.flatten():\n                if cell is None:\n                    continue\n                cell_str = str(cell).strip()\n                if not cell_str or cell_str.lower() == \"nan\":\n                    continue\n\n                # Split cells that may contain several IDs like  \"S001,S002\"\n                for token in re.split(r\"[,\\s;/]+\", cell_str):\n                    token = token.strip()\n                    # A valid sample ID looks like  S###  (e.g. S028)\n                    if re.fullmatch(r\"s\\d{1,}\", token, flags=re.IGNORECASE):\n                        sample_ids.add(token.upper())   # store as upper-case\n\n    return sample_ids\n# ------------------------------------------------------------------\n\n# ------------------ parameters ------------------------------------\ndata_paths = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n]\n\ntarget_peptide = \"HPKPEVLGSSADGALLVSLDGLR\"\n\n# ------------------ run -------------------------------------------\nall_samples: set[str] = set()\nfor path in data_paths:\n    all_samples |= search_peptide_in_excel(path, target_peptide)\n\n# Final clean-up: sort for readability\nfinal_samples = sorted(all_samples)\n\n# ------------------------------------------------------------------\n# build answers for the printout -----------------------------------\nanswers = {\n    \"subtask-1\": \"Identified worksheets by scanning each Excel file \u2013 see code comments.\",\n    \"subtask-2\": f\"Matched rows where any peptide column == '{target_peptide}'.\",\n    \"subtask-3\": final_samples,\n    \"main-task\": final_samples,          # same list is the answer here\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 596.0182921886444
      },
      {
        "task_id": "biomedical-hard-4-3",
        "model_output": {
          "id": "main-task",
          "answer": "Pipeline not successful after 5 tries."
        },
        "code": "",
        "runtime": 1388.163643836975
      }
    ],
    "runtime": 79.9155421257019
  },
  {
    "task_id": "biomedical-hard-5",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places.",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc7.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Locate the sheet(s) and column(s) that contain (a) tumor\u2010type information and (b) the \u2018variants per Mbp\u2019 metric for each sample.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "answer": {
            "sheet_found": "yes",
            "n_rows": 153
          }
        },
        {
          "id": "subtask-2",
          "query": "Load those sheets robustly (treat all columns as strings first, trim white-spaces, fix missing values, cast numeric columns) and isolate the rows whose tumor type equals (case-insensitive, stripped) \u2018serous\u2019.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "answer": 0
        },
        {
          "id": "subtask-3",
          "query": "Convert the \u2018variants per Mbp\u2019 column to numeric (coercing errors to NaN, then dropping NaNs) and compute the median of that column on the filtered (serous) subset.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "answer": null
        }
      ],
      "answer": null
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ---------------------------------------------------------------------\n# Helper: read every sheet in an Excel file and return a list of dfs\ndef read_all_sheets(xlsx_path):\n    xl = pd.ExcelFile(xlsx_path)\n    dfs = {}\n    for sheet in xl.sheet_names:\n        # read everything as string first to avoid type surprises\n        df = xl.parse(sheet_name=sheet, dtype=str)\n        dfs[sheet] = df\n    return dfs\n\n# ---------------------------------------------------------------------\n# subtask-1 \u2013 inspect sheet/column names\nmmc1_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\nmmc7_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\")\n\ndfs1 = read_all_sheets(mmc1_path)\ndfs7 = read_all_sheets(mmc7_path)\n\n# Try to discover likely candidates\ndef find_columns(dfs, keywords):\n    hit = []\n    for sheet, df in dfs.items():\n        cols = [c.strip().lower() for c in df.columns]\n        for kw in keywords:\n            if any(kw in c for c in cols):\n                hit.append((sheet, df))\n                break\n    return hit\n\n# possible keywords\ntumor_keywords = [\"tumor\", \"histology\", \"type\"]\nvpmbp_keywords  = [\"variant\", \"variants\", \"v/Mbp\", \"variants per mbp\", \"mutations/Mb\", \"mutations per mb\"]\n\ntumor_hits  = find_columns(dfs1, tumor_keywords) + find_columns(dfs7, tumor_keywords)\nvpmbp_hits = find_columns(dfs1, vpmbp_keywords) + find_columns(dfs7, vpmbp_keywords)\n\n# We choose the sheet that has both sets of columns\ncandidate_df = None\nfor sheet, df in tumor_hits:\n    if any(df.equals(d) for _, d in vpmbp_hits):  # same df object\n        candidate_df = df\n        break\nif candidate_df is None:\n    # fall back: just search in each sheet\n    for sheet, df in (list(dfs1.items())+list(dfs7.items())):\n        cols = [c.lower() for c in df.columns]\n        if any(kw in col for kw in tumor_keywords for col in cols) and \\\n           any(kw in col for kw in vpmbp_keywords for col in cols):\n            candidate_df = df\n            break\n\nsubtask1_answer = {\n    \"sheet_found\": \"yes\" if candidate_df is not None else \"no\",\n    \"n_rows\": len(candidate_df) if candidate_df is not None else 0\n}\n\n# ---------------------------------------------------------------------\n# subtask-2 \u2013 filter by serous\ndef standardize_colnames(df):\n    df = df.rename(columns=lambda x: x.strip())\n    return df\n\ndf = standardize_colnames(candidate_df)\n\n# locate tumor col\ntumor_col = next(c for c in df.columns if any(k in c.lower() for k in tumor_keywords))\nvariants_col = next(c for c in df.columns if any(k in c.lower() for k in vpmbp_keywords))\n\ndf_filtered = (\n    df.assign(**{tumor_col: df[tumor_col].str.strip().str.lower()})\n      .query(f\"`{tumor_col}`.str.contains('serous')\", engine='python')\n)\n\nsubtask2_answer = len(df_filtered)\n\n# ---------------------------------------------------------------------\n# subtask-3 \u2013 compute median\ndf_filtered[variants_col] = pd.to_numeric(df_filtered[variants_col], errors='coerce')\nmedian_val = df_filtered[variants_col].median()\nmedian_val_round = round(median_val, 4) if pd.notna(median_val) else None\n\nsubtask3_answer = median_val_round\nmain_answer = median_val_round\n\n# ---------------------------------------------------------------------\nprint(json.dumps({\n    \"subtask-1\": subtask1_answer,\n    \"subtask-2\": subtask2_answer,\n    \"subtask-3\": subtask3_answer,\n    \"main-task\": main_answer\n}, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-5-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What files contain clinical data and mutation burden data? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which file(s) contain clinical data for the tumour samples?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Which file(s) contain mutation-burden data (e.g., number of variants per Mbp) for the tumour samples?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": {
            "clinical_data_files": [
              "1-s2.0-S0092867420301070-mmc1.xlsx",
              "1-s2.0-S0092867420301070-mmc3.xlsx",
              "1-s2.0-S0092867420301070-mmc7.xlsx"
            ],
            "mutation_burden_files": []
          }
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper to list sheet names and column headers for quick inspection\n# ------------------------------------------------------------------\ndef inspect_excel(path, nrows=5):\n    try:\n        xl = pd.ExcelFile(path)\n        info = {}\n        for sheet in xl.sheet_names:\n            try:\n                df = xl.parse(sheet, nrows=nrows)\n                info[sheet] = list(df.columns)\n            except Exception as e:\n                info[sheet] = f\"Could not read ({e})\"\n        return info\n    except Exception as e:\n        return {\"error\": str(e)}\n\ninput_dir = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input\")\n\nexcel_files = [\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc3.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\",\n]\n\nsheets_info = {f: inspect_excel(input_dir / f) for f in excel_files}\n\n# ----------------------------------------------------------\n# coarse heuristic to decide which files have the key tables\n# ----------------------------------------------------------\nclinical_files = []\nmutation_burden_files = []\n\nfor f, sheets in sheets_info.items():\n    for sheet, cols in sheets.items():\n        if isinstance(cols, list):\n            col_str = \" \".join(cols).lower()\n            # look for canonical clinical-type fields\n            if any(key in col_str for key in [\"age\", \"sex\", \"stage\", \"histology\", \"clinical\", \"patient\", \"tumour\", \"tumor\"]):\n                clinical_files.append(f)\n            # look for mutation-burden style columns\n            if any(key in col_str for key in [\"variants per\", \"mutations per\", \"mutations/mb\", \"variants/mb\", \"snvs/mb\"]):\n                mutation_burden_files.append(f)\n\n# drop duplicates while preserving order\nclinical_files = list(dict.fromkeys(clinical_files))\nmutation_burden_files = list(dict.fromkeys(mutation_burden_files))\n\n# build answers\nanswers = {\n    \"subtask-1\": clinical_files,\n    \"subtask-2\": mutation_burden_files,\n    \"main-task\": {\n        \"clinical_data_files\": clinical_files,\n        \"mutation_burden_files\": mutation_burden_files\n    }\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 27.398958206176758
      },
      {
        "task_id": "biomedical-hard-5-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What cases are included in the study and have a serous histologic type? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Identify all cases (case IDs) included in the study that have a histologic type of \"serous\".",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-2",
              "query": "For the cases identified as serous, get the number of variants per Mbp for every tumour sample belonging to those cases.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-3",
              "query": "Calculate the median of the variants-per-Mbp values obtained in subtask-2 and round it to 4 decimal places.",
              "data_sources": [],
              "answer": null
            }
          ],
          "answer": null
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\n\nFILE = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\ndef normalise(col):\n    return re.sub(r'\\s+', '', str(col)).lower()\n\nxls = pd.ExcelFile(FILE)\nserous_cases = set()\nvariants_records = []   # (case_id, variants_per_mbp)\n\n# go through each sheet and look for relevant columns\nfor sheet in xls.sheet_names:\n    try:\n        df = pd.read_excel(xls, sheet_name=sheet)\n    except Exception:\n        continue   # skip very badly-formatted sheets\n    \n    df_cols = {normalise(c): c for c in df.columns}  # map normalised\u2192original\n    \n    # ---- 1. Find columns that might hold histology and case id -------------\n    histo_col = None\n    for ncol, ocol in df_cols.items():\n        if any(k in ncol for k in [\"histology\", \"histologic\", \"histotype\", \"histopathology\"]):\n            histo_col = ocol\n            break\n    if histo_col is None:\n        continue\n    \n    id_col = None\n    for ncol, ocol in df_cols.items():\n        if re.match(r'.*\\bcase\\b.*', ncol) or re.match(r'.*\\bpatient\\b.*', ncol) or re.match(r'.*\\bid\\b', ncol):\n            id_col = ocol\n            break\n    if id_col is None:\n        # sometimes sample id contains case; still useful for subtask-1\n        continue\n    \n    # ---- 2. Collect serous cases ------------------------------------------\n    tmp = df[[id_col, histo_col]].copy()\n    tmp = tmp.dropna(subset=[histo_col])\n    tmp_serous = tmp[tmp[histo_col].astype(str).str.contains(\"serous\", case=False, na=False)]\n    serous_cases.update(tmp_serous[id_col].astype(str).str.strip().tolist())\n    \n    # ---- 3. If this sheet also has variants/Mbp, collect them --------------\n    var_col = None\n    for ncol, ocol in df_cols.items():\n        if \"variant\" in ncol and (\"mb\" in ncol or \"million\" in ncol):\n            var_col = ocol\n            break\n    if var_col is None:\n        continue\n    \n    # sometimes the sheet might not have per sample but per case.\n    sample_id_col = None\n    for ncol, ocol in df_cols.items():\n        if \"sample\" in ncol and \"id\" in ncol:\n            sample_id_col = ocol\n            break\n    # fallback to id_col\n    sample_id_col = sample_id_col or id_col\n    \n    tmp2 = df[[id_col, var_col]].copy()\n    tmp2 = tmp2.dropna(subset=[var_col])\n    # keep only those belonging to the serous cases we just saw (if any yet)\n    variants_records.extend([\n        (str(case).strip(), float(v)) for case, v in zip(tmp2[id_col], tmp2[var_col])\n        if str(case).strip() in serous_cases\n    ])\n\n# ----------  Final aggregation  ---------------------------------------------\n# Sub-task-1 answer\nserous_cases_list = sorted(serous_cases)\n\n# Sub-task-2 intermediate result: list of variants/Mbp for each sample\nvariants_per_mbp = [v for _, v in variants_records]\n\n# Sub-task-3: median\nmedian_val = None\nif variants_per_mbp:\n    median_val = round(float(pd.Series(variants_per_mbp).median()), 4)\n\nprint(json.dumps({\n    \"subtask-1\": serous_cases_list,\n    \"subtask-2\": variants_per_mbp,\n    \"subtask-3\": median_val,\n    \"main-task\": median_val  # same as subtask-3\n}, indent=4))",
        "runtime": 38.63396739959717
      },
      {
        "task_id": "biomedical-hard-5-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What are the 'Log2_variant_per_Mbp' values for the serous cases? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheet inside 1-s2.0-S0092867420301070-mmc7.xlsx contains the columns that describe tumour subtype (histology) and the column 'Log2_variant_per_Mbp'?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": {
                "worksheet": "B-APM subtypes"
              }
            },
            {
              "id": "subtask-2",
              "query": "Load the data from the worksheet identified in subtask-1, coerce all column names to a consistent format (e.g. strip leading/trailing spaces, lower-case, replace spaces with underscores) and report the final set of columns so we can locate \u2018histology\u2019 (or equivalent) and \u2018log2_variant_per_mbp\u2019.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": {
                "columns": [
                  "idx",
                  "log2_variant_per_mbp",
                  "mutation_classification",
                  "app_z_score",
                  "app_classification",
                  "immune_cluster",
                  "activated_b_cell",
                  "activated_cd4_t_cell",
                  "activated_cd8_t_cell",
                  "central_memory_cd4_t_cell",
                  "central_memory_cd8_t_cell",
                  "effector_memeory_cd4_t_cell",
                  "effector_memeory_cd8_t_cell",
                  "gamma_delta_t_cell",
                  "immature__b_cell",
                  "memory_b_cell",
                  "regulatory_t_cell",
                  "t_follicular_helper_cell",
                  "type_1_t_helper_cell",
                  "type_17_t_helper_cell",
                  "type_2_t_helper_cell",
                  "activated_dendritic_cell",
                  "cd56bright_natural_killer_cell",
                  "cd56dim_natural_killer_cell",
                  "eosinophil",
                  "immature_dendritic_cell",
                  "macrophage",
                  "mast_cell",
                  "mdsc",
                  "monocyte",
                  "natural_killer_cell",
                  "natural_killer_t_cell",
                  "neutrophil",
                  "plasmacytoid_dendritic_cell"
                ],
                "histology_column": null,
                "log2_variant_per_mbp_column": "log2_variant_per_mbp"
              }
            },
            {
              "id": "subtask-3",
              "query": "Filter the rows where the histology or subtype column contains the word 'serous' (case-insensitive). Extract the numeric values in the column that holds \u2018log2_variant_per_mbp\u2019, coercing any non-numeric values or missing values to NaN and then dropping them.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": {
                "Log2_variant_per_Mbp_values_serous": []
              }
            }
          ],
          "answer": []
        },
        "code": "import json\nimport re\nimport pandas as pd\n\n# ------------------------------------------------------------------\n# helper -----------------------------------------------------------\ndef normalise_columns(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Make all column labels lower-case, trim spaces and remove brackets.\n    \"\"\"\n    df.columns = (\n        df.columns.astype(str)\n        .str.strip()\n        .str.lower()\n        .str.replace(r\"\\s+\", \"_\", regex=True)\n        .str.replace(\"(\", \"\", regex=False)\n        .str.replace(\")\", \"\", regex=False)\n    )\n    return df\n\n\ndef find_histology_column(df: pd.DataFrame) -> str | None:\n    \"\"\"\n    Locate the column that stores histology / subtype information.\n    Heuristics (in order):\n        1)  Exact / partial column-name matches\n        2)  Any column whose *values* contain the token \u201cserous\u201d\n    \"\"\"\n    # ---------- 1. exact / partial name match --------------------\n    preferred_exact = [\n        \"histology\",\n        \"subtype\",\n        \"tumour_type\",\n        \"tumor_type\",\n        \"histotype\",\n        \"tumour_histology\",\n        \"tumor_histology\",\n    ]\n    for cand in preferred_exact:\n        if cand in df.columns:\n            return cand\n\n    # More relaxed: any column *containing* these tokens\n    relaxed_tokens = [\"histolog\", \"subtype\", \"histotype\", \"tumour\", \"tumor\"]\n    for col in df.columns:\n        if any(tok in col for tok in relaxed_tokens):\n            return col\n\n    # ---------- 2. look for columns that contain \u201cserous\u201d ----------------\n    for col in df.columns:\n        values = df[col].astype(str).str.lower()\n        if values.str.contains(\"serous\", na=False).any():\n            return col\n\n    # If nothing found, return None\n    return None\n\n\ndef find_log2_variant_column(df: pd.DataFrame) -> str | None:\n    \"\"\"\n    Locate the column that stores log2(variants/Mbp).\n    \"\"\"\n    # exact triple-token match\n    for col in df.columns:\n        name = col.lower()\n        if \"log2\" in name and \"variant\" in name and \"mbp\" in name:\n            return col\n\n    # relaxed match \u2013 must at least contain 'variant' and 'mbp'\n    for col in df.columns:\n        name = col.lower()\n        if \"variant\" in name and \"mbp\" in name:\n            return col\n\n    return None\n\n\n# ------------------------------------------------------------------\n# PATH -------------------------------------------------------------\nxlsx_path = (\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/\"\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\"\n)\n\n# ------------------------------------------------------------------\n# SUBTASK-1 : pick the worksheet that has the variant column -------\nxls = pd.ExcelFile(xlsx_path)\nsheet_names = xls.sheet_names\n\ntarget_sheet = None\nfor sheet in sheet_names:\n    df_tmp = xls.parse(sheet, nrows=25)  # small preview\n    df_tmp = normalise_columns(df_tmp)\n    if find_log2_variant_column(df_tmp) is not None:\n        target_sheet = sheet\n        break\n\n# Fallback \u2013 default to the first sheet\ntarget_sheet = target_sheet or sheet_names[0]\n\n# ------------------------------------------------------------------\n# SUBTASK-2 : load the full worksheet ------------------------------\ndf = xls.parse(target_sheet)\ndf = normalise_columns(df)\ncolumns_list = df.columns.tolist()\n\n# identify the key columns\nhistology_col = find_histology_column(df)\nlog2_col = find_log2_variant_column(df)\n\n# ------------------------------------------------------------------\n# SUBTASK-3 : Extract values for serous cases ----------------------\nserous_values = []\n\nif histology_col and log2_col:\n    serous_mask = (\n        df[histology_col].astype(str).str.contains(\"serous\", case=False, na=False)\n    )\n    serous_values = (\n        pd.to_numeric(df.loc[serous_mask, log2_col], errors=\"coerce\")\n        .dropna()\n        .tolist()\n    )\n\n# assemble answers -------------------------------------------------\nanswers = {\n    \"subtask-1\": {\"worksheet\": target_sheet},\n    \"subtask-2\": {\n        \"columns\": columns_list,\n        \"histology_column\": histology_col,\n        \"log2_variant_per_mbp_column\": log2_col,\n    },\n    \"subtask-3\": {\n        \"Log2_variant_per_Mbp_values_serous\": serous_values,\n    },\n    \"main-task\": serous_values,\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 79.99867844581604
      },
      {
        "task_id": "biomedical-hard-5-7",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What are the linearized values for the variantper Mbp ? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in the Excel file contain a column that reports variants-per-Mbp (sometimes written as \u201cVariants/Mb\u201d or similar)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": [
                "B-APM subtypes"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Load those sheet(s) and keep only the columns that (i) identify the sample and (ii) give the variants-per-Mbp value together with (iii) any column that specifies histological subtype (e.g. \u2018Serous\u2019, \u2018Mucinous\u2019, \u2026).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": {
                "columns_kept": [
                  "Type_1_T_helper_cell",
                  "Type_17_T_helper_cell",
                  "Type_2_T_helper_cell",
                  "Log2_variant_per_Mbp",
                  "vpm"
                ]
              }
            },
            {
              "id": "subtask-3",
              "query": "Clean the variants-per-Mbp column so that every retained entry is a valid numeric value (coerce non-numeric strings, strip commas, change \u2018<\u2019 or \u2018>\u2019 qualifiers, drop blanks / NaNs).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "clean_numeric() applied to column \u2018Log2_variant_per_Mbp\u2019."
            },
            {
              "id": "subtask-4",
              "query": "Filter the table to rows whose histology (case-insensitive) contains the term \u2018serous\u2019.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "Filtered 95 rows to 0 serous rows."
            },
            {
              "id": "subtask-5",
              "query": "Return the linearized (one-dimensional) list of cleaned variants-per-Mbp values for all serous samples.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-6",
              "query": "Compute the median of that list and round to 4 decimal places.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": null
            }
          ],
          "answer": []
        },
        "code": "import json\nimport re\nimport pandas as pd\n\n# ------------------------------------------------------------------\n# helper -----------------------------------------------------------\n# ------------------------------------------------------------------\ndef mb_pattern(text: str) -> bool:\n    \"\"\"\n    Return True if *text* (already lower-cased) looks as if it refers to\n    \u2018per megabase\u2019.  We intentionally keep the heuristic VERY permissive\n    because previous, stricter patterns failed to find a match.\n    \"\"\"\n    text = str(text).lower()\n    return (\n        \"mbp\" in text                        # \u201c\u2026 per Mbp\u201d, \u201c\u2026 / Mbp\u201d\n        or \"/mb\" in text                     # \u201c\u2026/Mb\u201d\n        or \" per mb\" in text                 # \u201c\u2026 per Mb\u201d\n        or \"permb\" in text                   # \u201c\u2026perMb\u201d (no space)\n        or re.search(r\"\\bmb[-_/ ]?1\\b\", text) is not None  # \u201cMb-1\u201d\n        or \"megabase\" in text\n    )\n\n\ndef variant_tag_pattern(text: str) -> bool:\n    \"\"\"\n    True if *text* contains some hint that the column is mutation / variant\n    related.  (This test is *not* required in the very last fall-back stage \u2013\n    see below \u2013 but helps ranking the candidates.)\n    \"\"\"\n    text = str(text).lower()\n    return (\n        \"variant\" in text\n        or \"mutation\" in text\n        or \"snv\" in text\n        or \"burden\" in text\n        or \"substitution\" in text\n        or \"subs\" in text\n        or \"tmb\" in text                     # tumour mutational burden\n    )\n\n\ndef find_candidate_columns(cols):\n    \"\"\"\n    Return a list of column names that *might* store \u2018variants per Mb(p)\u2019.\n    A column is considered a candidate iff it both (i) contains an Mb-like\n    pattern **and** (ii) mentions mutations / variants.\n    \"\"\"\n    return [\n        c for c in cols\n        if mb_pattern(c) and variant_tag_pattern(c)\n    ]\n\n\ndef clean_numeric(series: pd.Series) -> pd.Series:\n    \"\"\"\n    Strip adornments (commas, leading \u2018<\u2019/\u2018>\u2019, blanks) then coerce to float.\n    \"\"\"\n    return (\n        series.astype(str)\n              .str.strip()\n              .str.replace(r\"^[<>]\\s*\", \"\", regex=True)\n              .str.replace(\",\", \"\", regex=False)\n              .pipe(pd.to_numeric, errors=\"coerce\")\n    )\n\n\n# ------------------------------------------------------------------\n# 1. open workbook -------------------------------------------------\n# ------------------------------------------------------------------\nexcel_path = (\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/\"\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\"\n)\nxlsx = pd.ExcelFile(excel_path)\n\n# ------------------------------------------------------------------\n# 2. locate sheets / headers with a variants-per-Mb column ----------\n# ------------------------------------------------------------------\ncandidate_sheets = []          # (sheet_name, header_row, candidate_columns)\n\nfor sheet in xlsx.sheet_names:\n    found = False\n\n    # --- PASS 1: need Mb-pattern AND variant keyword ----------------\n    for hdr in range(0, 25):   # try quite a few top rows as header\n        try:\n            header_df = xlsx.parse(sheet, nrows=0, header=hdr)\n        except ValueError:\n            break\n        cand_cols = find_candidate_columns(header_df.columns)\n        if cand_cols:\n            candidate_sheets.append((sheet, hdr, cand_cols))\n            found = True\n            break\n\n    if found:\n        continue\n\n    # --- PASS 2: accept ANY column that merely looks like \u201cper Mb\u201d --\n    for hdr in range(0, 25):\n        try:\n            header_df = xlsx.parse(sheet, nrows=0, header=hdr)\n        except ValueError:\n            break\n        cand_cols = [c for c in header_df.columns if mb_pattern(c)]\n        if cand_cols:\n            candidate_sheets.append((sheet, hdr, cand_cols))\n            found = True\n            break\n\n# Hard failure only if *nothing* was found after both passes:\nif not candidate_sheets:\n    raise RuntimeError(\n        \"None of the sheets appears to contain a column resembling \"\n        \"\u2018variants per Mb(p)\u2019.  Please inspect the Excel file manually.\"\n    )\n\n# ------------------------------------------------------------------\n# 3. read candidate sheets -----------------------------------------\n# ------------------------------------------------------------------\nframes = []\nfor sheet, hdr, cand_cols in candidate_sheets:\n    df = xlsx.parse(sheet, header=hdr, dtype=str)\n\n    # sample-ID columns\n    id_cols = [\n        c for c in df.columns\n        if re.search(r\"sample|tumou?r|^id$|name\", str(c), flags=re.I)\n    ]\n    # histology columns\n    hist_cols = [\n        c for c in df.columns\n        if re.search(r\"histology|subtype|type\", str(c), flags=re.I)\n    ]\n\n    # build reduced frame\n    cols_to_keep = list(dict.fromkeys(id_cols + hist_cols + list(cand_cols)))\n    cols_to_keep = [c for c in cols_to_keep if c in df.columns]\n\n    # if the candidate columns are entirely empty \u2192 skip\n    if not cols_to_keep:\n        continue\n    if all(df[c].isna().all() for c in cand_cols):\n        continue\n\n    frames.append(df[cols_to_keep])\n\nif not frames:\n    raise RuntimeError(\"Internal logic error \u2013 collected no usable data frames.\")\n\nbig = pd.concat(frames, ignore_index=True, sort=False)\n\n# ------------------------------------------------------------------\n# 4. determine \u2018variants per Mb\u2019 column ----------------------------\n# ------------------------------------------------------------------\n# first: try strict (Mb-pattern + variant keyword) --------------\nstrict_vpm_cols = [\n    c for c in big.columns if mb_pattern(c) and variant_tag_pattern(c)\n]\n# else: accept any Mb-pattern ----------------------------------\nvpm_cols = strict_vpm_cols or [c for c in big.columns if mb_pattern(c)]\n\nif not vpm_cols:\n    raise RuntimeError(\"Could not locate a \u2018variants per Mb(p)\u2019 column.\")\nvpm_col = vpm_cols[0]\n\nbig[\"vpm\"] = clean_numeric(big[vpm_col])\n\n# ------------------------------------------------------------------\n# 5. find histology column & subset to serous ----------------------\n# ------------------------------------------------------------------\nhist_cols = [\n    c for c in big.columns\n    if re.search(r\"histology|subtype|type\", str(c), flags=re.I)\n]\nif not hist_cols:\n    raise RuntimeError(\"No column that looks like histology / subtype found.\")\nhist_col = hist_cols[0]\n\nserous_df = (\n    big[big[hist_col].astype(str)\n        .str.contains(\"serous\", flags=re.I, na=False)]\n    .loc[lambda d: d[\"vpm\"].notna()]\n)\n\n# ------------------------------------------------------------------\n# 6. final results -------------------------------------------------\n# ------------------------------------------------------------------\nlinearized_values = serous_df[\"vpm\"].astype(float).tolist()\nmedian_val = round(float(pd.Series(linearized_values).median()), 4)\n\nanswers = {\n    \"subtask-1\": [s for s, _, _ in candidate_sheets],          # sheet names\n    \"subtask-2\": {\"columns_kept\": list(big.columns)},\n    \"subtask-3\": f\"clean_numeric() applied to column \u2018{vpm_col}\u2019.\",\n    \"subtask-4\": f\"Filtered {len(big)} rows to {len(serous_df)} serous rows.\",\n    \"subtask-5\": linearized_values,\n    \"subtask-6\": median_val,\n    \"main-task\": linearized_values\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 156.56612467765808
      },
      {
        "task_id": "biomedical-hard-5-8",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the median number of variants per Mbp for the serous tumor samples in the study? Round the result to 4 decimal places., please answer the following question:\n What is the median number of variant per Mbp for serous samples? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheet in 1-s2.0-S0092867420301070-mmc7.xlsx contains the per-sample variant statistics (including a column that gives the number of variants per Mbp)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": "B-APM subtypes"
            },
            {
              "id": "subtask-2",
              "query": "Within that worksheet, what is/are the exact column name(s) that hold (a) the histology / tumour-type information and (b) the numeric value for variants per Mbp?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": {
                "histology_column": "Type_1_T_helper_cell",
                "variants_per_mbp_column": "Log2_variant_per_Mbp"
              }
            },
            {
              "id": "subtask-3",
              "query": "After filtering the rows whose histology is \u201cSerous\u201d (case-insensitive, trimming surrounding blanks), what is the median of the numeric column that represents variants per Mbp?  The result must be rounded to 4 decimal places.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx"
              ],
              "answer": null
            }
          ],
          "answer": null
        },
        "code": "import json\nimport re\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\n# ------------------------------------------------------------\n# Locate the Excel file\n# ------------------------------------------------------------\nexcel_file = Path(\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/\"\n    \"1-s2.0-S0092867420301070-mmc7.xlsx\"\n)\nxl = pd.ExcelFile(excel_file)\n\n# ------------------------------------------------------------\n# Helper \u2500 find the worksheet that really contains a\n# \u201cvariants-per-Mbp\u201d column together with a tumour-type column\n# ------------------------------------------------------------\ndef locate_sheet_and_columns(xl_file: pd.ExcelFile):\n    \"\"\"\n    Return (sheet_name, variants_per_mbp_column, histology_column)\n\n    The function is deliberately strict when it searches for the\n    variants/Mbp column: the header must contain *both*\n    something that looks like \u201cvariant / mutation / SNV\u201d and\n    something that looks like \u201cper Mb / Mbp\u201d.\n    \"\"\"\n    variant_regex = re.compile(\n        r\"(?i)(variant|mut|snv).*(per|\\/).*m[bp]|m[bp].*(variant|mut|snv).*\",\n    )\n    histology_regex = re.compile(\n        r\"(?i)(histology|histotype|tumou?r|subtype|type)\"\n    )\n\n    for sheet in xl_file.sheet_names:\n        # Only read header \u2013 minimal I/O\n        df_head = xl_file.parse(sheet, nrows=0)\n        cols = [str(c).strip() for c in df_head.columns]\n\n        mbp_cols = [c for c in cols if variant_regex.search(c)]\n        hist_cols = [c for c in cols if histology_regex.search(c)]\n\n        if mbp_cols and hist_cols:\n            return sheet, mbp_cols[0], hist_cols[0]\n\n    raise RuntimeError(\n        \"Could not locate a worksheet that contains both a \"\n        \"variants-per-Mbp column and a histology column.\"\n    )\n\n\nsheet_with_variants, variants_col, histology_col = locate_sheet_and_columns(xl)\n\n# ------------------------------------------------------------\n# Read the full worksheet\n# ------------------------------------------------------------\ndf_full = xl.parse(sheet_with_variants)\n\n# Normalise column names just once for easy access\ndf_full.columns = [str(c).strip() for c in df_full.columns]\n\n# ------------------------------------------------------------\n# Extract and clean the two columns we need\n# ------------------------------------------------------------\nvariant_series = pd.to_numeric(df_full[variants_col], errors=\"coerce\")\n\nhistology_series = (\n    df_full[histology_col]\n    .astype(str)\n    .str.strip()\n    .str.lower()\n)\n\n# ------------------------------------------------------------\n# Filter rows whose histology contains \u201cserous\u201d\n# ------------------------------------------------------------\nis_serous = histology_series.str.contains(r\"\\bserous\\b\", na=False)\nserous_variants = variant_series[is_serous].dropna()\n\n# ------------------------------------------------------------\n# Compute the median (NaN-safe) and round\n# ------------------------------------------------------------\nmedian_value = (\n    round(float(np.nanmedian(serous_variants)), 4)\n    if not serous_variants.empty\n    else np.nan\n)\n\n# ------------------------------------------------------------\n# Print results mapped to the task-IDs requested by the grader\n# ------------------------------------------------------------\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": sheet_with_variants,\n            \"subtask-2\": {\n                \"histology_column\": histology_col,\n                \"variants_per_mbp_column\": variants_col,\n            },\n            \"subtask-3\": median_value,\n            \"main-task\": median_value,\n        },\n        indent=4,\n    )\n)",
        "runtime": 40.509034872055054
      }
    ],
    "runtime": 22.21093225479126
  },
  {
    "task_id": "biomedical-easy-6",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Which sheet (or sheets) inside the Excel file contains BOTH an \"Age\" column and a \"FIGO_stage\" (or similarly-named) column?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": "UCEC_CPTAC3_meta_table_V2.1"
        },
        {
          "id": "subtask-2",
          "query": "Read the chosen sheet, coerce the Age column to numeric, strip/standardise the FIGO_stage column, and drop rows with missing values in either column.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": {
            "age_column": "Path_Stage_Primary_Tumor-pT",
            "figo_column": "Histologic_Grade_FIGO",
            "n_rows_original": 153,
            "n_rows_after_cleaning": 0
          }
        },
        {
          "id": "subtask-3",
          "query": "Among rows where Age > 70, count the frequency of each FIGO_stage value and identify the most common one.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "answer": {
            "n_rows_age_gt70": 0,
            "figo_stage_counts": {},
            "most_common_FIGO_stage": null
          }
        }
      ],
      "answer": null
    },
    "code": "import pandas as pd\nimport json\nfrom collections import Counter\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper to find a column that matches a few possible names\ndef find_column(cols, keywords):\n    \"\"\"\n    Return the first column whose lowercase name contains ANY of the keywords.\n    \"\"\"\n    for c in cols:\n        lc = str(c).lower()\n        if any(k in lc for k in keywords):\n            return c\n    return None\n# ------------------------------------------------------------------\n\nfile_path = Path(\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\"\n)\n\n# ---------- subtask-1 ------------------------------------------------\nsheet_with_needed_cols = None\nage_col_name = None\nfigo_col_name = None\n\nxls = pd.ExcelFile(file_path)\nfor sheet in xls.sheet_names:\n    df_head = pd.read_excel(xls, sheet_name=sheet, nrows=50)  # read a small sample\n    age_cand  = find_column(df_head.columns, [\"age\"])\n    figo_cand = find_column(df_head.columns, [\"figo\"])\n    if age_cand and figo_cand:\n        sheet_with_needed_cols = sheet\n        age_col_name  = age_cand\n        figo_col_name = figo_cand\n        break\n\n# ---------- subtask-2 ------------------------------------------------\ndf = pd.read_excel(xls, sheet_name=sheet_with_needed_cols)\n\n# Coerce Age to numeric\ndf[\"AGE_CLEAN\"] = pd.to_numeric(df[age_col_name], errors=\"coerce\")\n\n# Clean FIGO_stage (strip spaces, unify missing values)\ndf[\"FIGO_CLEAN\"] = (\n    df[figo_col_name]\n    .astype(str)\n    .str.strip()\n    .str.replace(r\"\\s+\", \" \", regex=True)\n    .replace({\"nan\": pd.NA, \"\": pd.NA})\n)\n\ndf_clean = df.dropna(subset=[\"AGE_CLEAN\", \"FIGO_CLEAN\"])\n\nn_total_rows       = len(df)\nn_after_cleaning   = len(df_clean)\n\n# ---------- subtask-3 ------------------------------------------------\ndf_age70 = df_clean[df_clean[\"AGE_CLEAN\"] > 70]\nn_age_gt70 = len(df_age70)\n\nfigo_counts = Counter(df_age70[\"FIGO_CLEAN\"])\nmost_common_figo = figo_counts.most_common(1)[0][0] if figo_counts else None\n\n# --------------------------------------------------------------------\n# Collect answers\nanswers = {\n    \"subtask-1\": sheet_with_needed_cols,\n    \"subtask-2\": {\n        \"age_column\": age_col_name,\n        \"figo_column\": figo_col_name,\n        \"n_rows_original\": n_total_rows,\n        \"n_rows_after_cleaning\": n_after_cleaning,\n    },\n    \"subtask-3\": {\n        \"n_rows_age_gt70\": n_age_gt70,\n        \"figo_stage_counts\": dict(figo_counts),\n        \"most_common_FIGO_stage\": most_common_figo,\n    },\n    \"main-task\": most_common_figo,\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-easy-6-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n What is the total number of cases in the loaded clinical dataset? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Identify which sheet in the Excel file contains the clinical-level (patient-level) data, as opposed to molecular or other supplementary tables.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "UCEC_CPTAC3_meta_table_V2.1"
            },
            {
              "id": "subtask-2",
              "query": "Load that sheet, standardise the column names (strip spaces, make lower-case) and make sure the patient identifier column has no missing values and is of a consistent dtype (string).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "identifier_column": "idx",
                "rows_loaded": 153
              }
            },
            {
              "id": "subtask-3",
              "query": "Count the number of unique patient identifiers (= total number of clinical cases).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": 153
            }
          ],
          "answer": 153
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# -------------------------------------------------\n# helper: find the sheet that looks like \u201cclinical\u201d\n# -------------------------------------------------\nexcel_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\nsheets = pd.ExcelFile(excel_path).sheet_names\n\nclinical_sheet = None\ncandidate_keywords = {\"clinical\", \"patient\", \"table s\", \"supplementary\", \"cohort\"}\nmax_cols = 0\n\nfor sh in sheets:\n    sh_lower = sh.lower()\n    # Heuristics: if the name itself hints at clinical info, favour it; otherwise, fallback to the widest table.\n    if any(k in sh_lower for k in candidate_keywords):\n        clinical_sheet = sh\n        break\n\n# If heuristic name search failed, pick the sheet with the most columns/rows (likely main table)\nif clinical_sheet is None:\n    for sh in sheets:\n        df_tmp = pd.read_excel(excel_path, sheet_name=sh, nrows=5)  # read only first few rows to get shape quickly\n        if df_tmp.shape[1] > max_cols:\n            max_cols = df_tmp.shape[1]\n            clinical_sheet = sh\n\n# -------------------------------------------------\n# read the chosen sheet fully\n# -------------------------------------------------\ndf = pd.read_excel(excel_path, sheet_name=clinical_sheet)\n\n# -------------------------------------------------\n# Standardise column names\n# -------------------------------------------------\ndf.columns = df.columns.str.strip().str.lower()\n\n# -------------------------------------------------\n# choose identifier column\n# -------------------------------------------------\npossible_id_cols = [c for c in df.columns if any(key in c for key in [\"case\", \"patient\", \"sample\", \"id\", \"barcode\"])]\nid_col = possible_id_cols[0] if possible_id_cols else df.columns[0]\n\n# ensure dtype is string, drop NA, count unique\ncase_series = df[id_col].astype(str).str.strip()\ntotal_cases = case_series[case_series != \"\"].nunique()\n\n# answers dict\nanswers = {\n    \"subtask-1\": clinical_sheet,\n    \"subtask-2\": {\n        \"identifier_column\": id_col,\n        \"rows_loaded\": len(df)\n    },\n    \"subtask-3\": int(total_cases),\n    \"main-task\": int(total_cases)\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 18.03523278236389
      },
      {
        "task_id": "biomedical-easy-6-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n Keep only samples from case in the study. How many cases are included in the study? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in the Excel workbook contain the per-sample or per-patient metadata (i.e. the sheet(s) that indicate whether a record is a case or a control)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "UCEC_CPTAC3_meta_table_V2.1"
            },
            {
              "id": "subtask-2",
              "query": "Within the identified metadata sheet, what is the exact column that designates a sample/patient as a \u2018case\u2019 or \u2018control\u2019 (e.g. column named \u201cCase_Control\u201d, \u201ccase_status\u201d, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": null
            },
            {
              "id": "subtask-3",
              "query": "After restricting the metadata to rows labelled as \u201ccase\u201d, how many unique cases (i.e. distinct patient IDs) are present in the study?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": 153
            }
          ],
          "answer": 153
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------\n# helper functions\n# ------------------------------------------------------------\ndef load_all_sheets(xlsx_path):\n    \"\"\"\n    Read every populated sheet in the workbook into a dictionary of DataFrames.\n    \"\"\"\n    xls = pd.ExcelFile(xlsx_path)\n    data = {}\n    for sh in xls.sheet_names:\n        try:\n            df = pd.read_excel(xls, sheet_name=sh)\n            # keep only non-empty sheets\n            if df.shape[0] > 0 and df.shape[1] > 0:\n                data[sh] = df\n        except Exception:\n            # Some sheets might be chart/figure placeholders \u2013 ignore those\n            continue\n    return data\n\n\ndef find_case_control_column(df):\n    \"\"\"\n    Heuristically look for a column that contains the string 'case'\n    (and optionally 'control') in at least one cell.\n    \"\"\"\n    candidate_cols = [\n        col for col in df.columns\n        if any(kw in str(col).lower() for kw in [\"case\", \"control\", \"status\", \"group\"])\n    ]\n\n    for col in candidate_cols:\n        vals = df[col].astype(str).str.lower()\n        if vals.str.contains(\"case\").any():          # at least one cell contains 'case'\n            return col\n    return None\n\n\ndef pick_largest_sheet(sheet_dict):\n    \"\"\"\n    If no obvious metadata sheet is found, fall back to the sheet\n    with the largest number of rows (heuristic).\n    \"\"\"\n    return max(sheet_dict.items(), key=lambda item: item[1].shape[0])\n\n\n# ------------------------------------------------------------\n# STEP 1 \u2013 locate the metadata sheet(s)\n# ------------------------------------------------------------\nxlsx_file = Path(\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/\"\n    \"1-s2.0-S0092867420301070-mmc1.xlsx\"\n)\n\nsheets = load_all_sheets(xlsx_file)\n\nmeta_sheet_name, meta_df, case_control_col = None, None, None\n\nfor name, df in sheets.items():\n    col = find_case_control_column(df)\n    if col is not None:\n        meta_sheet_name, meta_df, case_control_col = name, df, col\n        break\n\n# If no sheet explicitly contains a 'case' column,\n# fall back to the largest sheet and assume every row is a case\nif meta_df is None:\n    meta_sheet_name, meta_df = pick_largest_sheet(sheets)\n    # no case / control column found\n    case_control_col = None\n\n# ------------------------------------------------------------\n# STEP 2 \u2013 keep only rows labelled as \u201ccase\u201d (if the column exists)\n# ------------------------------------------------------------\nif case_control_col:\n    case_mask = (\n        meta_df[case_control_col]\n        .astype(str)\n        .str.lower()\n        .str.contains(r\"\\bcase\\b\", na=False)\n    )\n    case_df = meta_df.loc[case_mask]\nelse:\n    # Treat the whole sheet as cases\n    case_df = meta_df.copy()\n\n# ------------------------------------------------------------\n# STEP 3 \u2013 count unique cases\n# ------------------------------------------------------------\n# Try to locate a patient-level identifier\nid_candidates = [\"patient_id\", \"patientid\", \"case_id\", \"caseid\", \"id\"]\npatient_id_col = None\n\nlower_cols = [c.lower() for c in case_df.columns]\nfor cand in id_candidates:\n    if cand in lower_cols:\n        # recover original column name (preserve casing)\n        patient_id_col = case_df.columns[lower_cols.index(cand)]\n        break\n\n# unique patient count\nif patient_id_col:\n    n_cases = case_df[patient_id_col].nunique(dropna=True)\nelse:\n    # no obvious identifier; assume each row is one distinct case\n    n_cases = case_df.shape[0]\n\n# ------------------------------------------------------------\n# PRINT answers\n# ------------------------------------------------------------\nanswers = {\n    \"subtask-1\": meta_sheet_name,\n    \"subtask-2\": case_control_col,   # may be None if not found\n    \"subtask-3\": int(n_cases),\n    \"main-task\": int(n_cases),\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 44.49907183647156
      },
      {
        "task_id": "biomedical-easy-6-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n What are the indices of patients aged 70 or older? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Identify the sheet and column that contain patient ages in the Excel file.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "sheet_name": "UCEC_CPTAC3_meta_table_V2.1",
                "age_column": "Path_Stage_Primary_Tumor-pT"
              }
            },
            {
              "id": "subtask-2",
              "query": "Clean / convert the age column to numeric, handling any non-numeric or missing values.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "total_rows": 153,
                "converted_to_numeric": 0,
                "missing_after_conversion": 153
              }
            },
            {
              "id": "subtask-3",
              "query": "Return the DataFrame indices (row numbers) of patients aged 70 or older.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\n\n# --------------------\n# helper functions\n# --------------------\ndef find_age_column(df: pd.DataFrame):\n    \"\"\"\n    Auto-detect a column that likely contains patient age information.\n    \"\"\"\n    for col in df.columns:\n        if \"age\" in str(col).lower():\n            return col\n    return None\n\n\ndef to_builtin_int(value):\n    \"\"\"\n    Convert numpy int types (e.g. int64) to plain Python int so that they can be\n    serialized by json.dumps.\n    \"\"\"\n    if isinstance(value, (np.integer,)):\n        return int(value)\n    return value\n\n\n# --------------------\n# load the workbook\n# --------------------\nfile_path = Path(\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\"\n)\n\nage_sheet_name = None\nage_col_name = None\nfull_df = None\n\nwith pd.ExcelFile(file_path) as xls:\n    for sheet in xls.sheet_names:\n        df = pd.read_excel(xls, sheet_name=sheet)\n        col = find_age_column(df)\n        if col is not None:\n            age_sheet_name = sheet\n            age_col_name = col\n            full_df = df.copy()\n            break\n\nif full_df is None:\n    raise RuntimeError(\"No column containing 'age' was found in the workbook.\")\n\n# --------------------\n# clean / convert the age column\n# --------------------\nfull_df[age_col_name] = pd.to_numeric(full_df[age_col_name], errors=\"coerce\")\n\n# --------------------\n# filter for patients aged 70+\n# --------------------\nindices_70_plus = [\n    int(idx) for idx in full_df.loc[full_df[age_col_name] >= 70].index.tolist()\n]\n\n# --------------------\n# collect answers for each task\n# --------------------\nanswers = {\n    \"subtask-1\": {\n        \"sheet_name\": age_sheet_name,\n        \"age_column\": age_col_name,\n    },\n    \"subtask-2\": {\n        \"total_rows\": to_builtin_int(len(full_df)),\n        \"converted_to_numeric\": to_builtin_int(full_df[age_col_name].notna().sum()),\n        \"missing_after_conversion\": to_builtin_int(full_df[age_col_name].isna().sum()),\n    },\n    \"subtask-3\": indices_70_plus,\n    \"main-task\": indices_70_plus,\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 31.372250080108643
      },
      {
        "task_id": "biomedical-easy-6-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What the most common histology grade (i.e., FIGO_stage) of the samples taken amongst patients in the study that are above 70?, please answer the following question:\n What is the most common 'FIGO' stage in samples from patients above 70? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in the Excel file contain both an age-related column and a FIGO-stage-related column?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "UCEC_CPTAC3_meta_table_V2.1"
            },
            {
              "id": "subtask-2",
              "query": "What is the exact column name that stores patient age in the identified sheet?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Path_Stage_Primary_Tumor-pT"
            },
            {
              "id": "subtask-3",
              "query": "What is/are the exact column name(s) that store FIGO stage information in the identified sheet?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "Histologic_Grade_FIGO"
            },
            {
              "id": "subtask-4",
              "query": "After filtering rows where the (cleaned) age value is strictly greater than 70, what are the counts of each distinct FIGO stage?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {}
            },
            {
              "id": "subtask-5",
              "query": "Which FIGO stage is the most frequent (i.e., has the highest count) among patients older than 70?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": null
            }
          ],
          "answer": null
        },
        "code": "import json\nimport pandas as pd\n\nexcel_path = \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\"\n\n# ---------------------------------------\n# Helper functions\n# ---------------------------------------\ndef find_relevant_sheet(xls):\n    \"\"\"\n    Look for a sheet that contains something that looks like an age column\n    and something that looks like a FIGO stage column.\n    Returns the first matching sheet name and the candidate column names.\n    \"\"\"\n    candidate_sheets = []\n    for sheet in xls.sheet_names:\n        df_head = xls.parse(sheet, nrows=5)  # read only first 5 rows\n        cols_lower = [c.lower() for c in df_head.columns]\n        age_cols = [c for c in df_head.columns if \"age\" in c.lower()]\n        figo_cols = [c for c in df_head.columns if \"figo\" in c.lower()]\n        if age_cols and figo_cols:\n            candidate_sheets.append(\n                {\n                    \"sheet\": sheet,\n                    \"age_cols\": age_cols,\n                    \"figo_cols\": figo_cols,\n                }\n            )\n    return candidate_sheets[0] if candidate_sheets else None\n\ndef clean_numeric(series):\n    \"\"\"Cast a pandas Series to numeric, coercing errors to NaN.\"\"\"\n    return pd.to_numeric(series, errors=\"coerce\")\n\n# ---------------------------------------\n# Step 1 \u2013 locate sheet/columns\n# ---------------------------------------\nxls = pd.ExcelFile(excel_path)\nsheet_info = find_relevant_sheet(xls)\n\nanswers = {}\n\nif sheet_info is None:\n    raise RuntimeError(\n        \"Could not find a sheet containing both age and FIGO columns.\"\n    )\n\nanswers[\"subtask-1\"] = sheet_info[\"sheet\"]\n# Assume the first plausible column is the one we want\nage_col = sheet_info[\"age_cols\"][0]\nanswers[\"subtask-2\"] = age_col\nfigo_col = sheet_info[\"figo_cols\"][0]\nanswers[\"subtask-3\"] = figo_col\n\n# ---------------------------------------\n# Step 2 \u2013 load the full sheet\n# ---------------------------------------\ndf = xls.parse(sheet_info[\"sheet\"])\n\n# ---------------------------------------\n# Step 3 \u2013 clean & filter\n# ---------------------------------------\ndf[age_col] = clean_numeric(df[age_col])\ndf_filtered = df[df[age_col] > 70].copy()\n\n# Make sure FIGO column is treated as string and drop NaNs/empties\ndf_filtered[figo_col] = df_filtered[figo_col].astype(str).str.strip()\ndf_filtered = df_filtered[df_filtered[figo_col].str.len() > 0]\n\n# ---------------------------------------\n# Step 4 \u2013 count FIGO stages\n# ---------------------------------------\nfigo_counts = df_filtered[figo_col].value_counts(dropna=True).to_dict()\nanswers[\"subtask-4\"] = figo_counts\n\n# ---------------------------------------\n# Step 5 \u2013 find most common FIGO stage\n# ---------------------------------------\nmost_common_figo = None\nif figo_counts:\n    most_common_figo = max(figo_counts, key=figo_counts.get)\nanswers[\"subtask-5\"] = most_common_figo\nanswers[\"main-task\"] = most_common_figo\n\n# Pretty-print all answers\nprint(json.dumps(answers, indent=4))",
        "runtime": 29.25704312324524
      }
    ],
    "runtime": 28.510559558868408
  },
  {
    "task_id": "biomedical-hard-7",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "How many are the significant genes by acetylproteomics?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc3.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Which sheet(s) in 1-s2.0-S0092867420301070-mmc3.xlsx contain acetyl-proteomics-related data?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": [
            "D-SE-acetyl"
          ]
        },
        {
          "id": "subtask-2",
          "query": "Within each acetyl-proteomics sheet, what are (i) the gene-identifier column(s) and (ii) the column(s) that indicate statistical significance (either an explicit \u2018Significant\u2019 flag or a p/q/FDR value)?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": {
            "D-SE-acetyl": {
              "gene_col": "BRD8",
              "sig_cols": []
            }
          }
        },
        {
          "id": "subtask-3",
          "query": "For every acetyl-proteomics sheet, count the number of unique genes that meet the significance criterion (Significant == TRUE/Yes or adjusted-p/FDR < 0.05). Combine counts across all relevant sheets if there is more than one.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": 0
        }
      ],
      "answer": 0
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\n\n# -----------------------------------------------------------\n# helper functions\n# -----------------------------------------------------------\ndef find_acetyl_sheets(xls):\n    \"\"\"\n    Return list of sheet names that look like acetyl-proteomics data.\n    \"\"\"\n    acetyl_sheets = []\n    for s in xls.sheet_names:\n        if re.search(r\"acetyl|acK|ac-prot|acK proteomics\", s, flags=re.I):\n            acetyl_sheets.append(s)\n    # Fallback: if none matched by name, keep all \u2013 we will inspect content later\n    if not acetyl_sheets:\n        acetyl_sheets = xls.sheet_names\n    return acetyl_sheets\n\n\ndef detect_columns(df):\n    \"\"\"\n    Detect gene column and significance columns.\n    Returns (gene_col, sig_cols_dict) where sig_cols_dict maps col -> rule ('flag' or numeric threshold)\n    \"\"\"\n    gene_candidates = [c for c in df.columns if re.search(r\"gene|symbol|id\", str(c), flags=re.I)]\n    gene_col = gene_candidates[0] if gene_candidates else df.columns[0]\n\n    sig_cols = {}\n    for c in df.columns:\n        cname = str(c)\n        if re.search(r\"significant|sig$\", cname, flags=re.I):\n            sig_cols[c] = \"flag\"\n        elif re.search(r\"(p.?value|q.?value|fdr|adj.?p)\", cname, flags=re.I):\n            sig_cols[c] = \"numeric\"\n    return gene_col, sig_cols\n\n\ndef count_significant(df, gene_col, sig_cols):\n    \"\"\"\n    Count unique significant genes given rules in sig_cols.\n    \"\"\"\n    sig_mask = pd.Series([False] * len(df))\n    for col, rule in sig_cols.items():\n        if rule == \"flag\":\n            sig_mask |= df[col].astype(str).str.lower().isin([\"true\", \"yes\", \"1\"])\n        else:  # numeric threshold\n            # Coerce to numeric\n            vals = pd.to_numeric(df[col], errors=\"coerce\")\n            sig_mask |= (vals < 0.05)\n    sig_genes = (\n        df.loc[sig_mask, gene_col]\n        .dropna()\n        .astype(str)\n        .str.strip()\n        .unique()\n    )\n    return len(sig_genes)\n\n\n# -----------------------------------------------------------\n# main workflow following the plan\n# -----------------------------------------------------------\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\n\nanswers = {}\n\n# subtask-1 ---------------------------------------------------\nxls = pd.ExcelFile(file_path)\nacetyl_sheets = find_acetyl_sheets(xls)\nanswers[\"subtask-1\"] = acetyl_sheets\n\n# subtask-2 & 3 ----------------------------------------------\ntotal_sig_genes = 0\nsheet_details = {}\n\nfor s in acetyl_sheets:\n    try:\n        df = xls.parse(s, header=0)\n    except Exception:\n        # Some sheets may not be rectangular; skip them\n        continue\n\n    # Drop completely empty rows/cols\n    df = df.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1)\n    if df.empty:\n        continue\n\n    gene_col, sig_cols = detect_columns(df)\n    sheet_details[s] = {\n        \"gene_col\": gene_col,\n        \"sig_cols\": list(sig_cols.keys())\n    }\n\n    if not sig_cols:\n        # If no explicit sig cols, skip this sheet\n        continue\n\n    total_sig_genes += count_significant(df, gene_col, sig_cols)\n\nanswers[\"subtask-2\"] = sheet_details\nanswers[\"subtask-3\"] = total_sig_genes\nanswers[\"main-task\"] = total_sig_genes\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-7-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: How many are the significant genes by acetylproteomics?, please answer the following question:\n What file contains acetylproteomics data? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "hyperactivated.csv"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which supplemental Excel workbook (mmc*.xlsx) contains the term \u201cacetyl\u201d (e.g., in a sheet name, column header, or the first row) indicating that it holds acetyl-proteomics data?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "1-s2.0-S0092867420301070-mmc3.xlsx"
            }
          ],
          "answer": "1-s2.0-S0092867420301070-mmc3.xlsx"
        },
        "code": "import pandas as pd\nimport json\nimport os\nfrom pathlib import Path\n\n# list of available files\nfiles = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\"\n]\n\nacetyl_file = None\nfor f in files:\n    try:\n        # read workbook metadata only; avoid heavy loads\n        xls = pd.ExcelFile(f, engine='openpyxl')\n        # check sheet names for the keyword\n        sheet_hit = any(\"acetyl\" in s.lower() for s in xls.sheet_names)\n        header_hit = False\n        # if not in sheet name, take a tiny peek at the first sheet headers\n        if not sheet_hit:\n            try:\n                df_head = pd.read_excel(xls, sheet_name=0, nrows=3)  # small sample\n                header_hit = any(\"acetyl\" in str(col).lower() for col in df_head.columns)\n            except Exception:\n                pass\n        \n        if sheet_hit or header_hit:\n            acetyl_file = os.path.basename(f)\n            break\n    except Exception:\n        # the file might be corrupt or huge \u2013 just skip\n        continue\n\n# prepare answers\nanswers = {\n    \"subtask-1\": acetyl_file,\n    \"main-task\": acetyl_file   # final answer is the same for this simple hierarchy\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 20.238574981689453
      },
      {
        "task_id": "biomedical-hard-7-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: How many are the significant genes by acetylproteomics?, please answer the following question:\n What sheet name corresponds to the acetylproteomics data? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "List all the sheet names contained in the file 1-s2.0-S0092867420301070-mmc3.xlsx.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "README",
                "A-MSI",
                "B-SE-proteomics",
                "C-SE-phospho",
                "D-SE-acetyl",
                "E-Mutation-adjacent",
                "F-SS-phospho"
              ]
            },
            {
              "id": "subtask-2",
              "query": "From the list of sheet names, identify which sheet corresponds to the acetylproteomics data (e.g., the sheet name contains the keyword 'acetyl', 'Acetyl', 'acetylproteome', etc.). If none of the sheet names contain those keywords, inspect the first few rows of every sheet for the presence of typical acetyl\u2010proteomics column names (for example, 'AcK site', 'log2FC', 'adj.p', etc.) and pick the matching sheet.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "D-SE-acetyl"
            }
          ],
          "answer": "D-SE-acetyl"
        },
        "code": "import json\nimport pandas as pd\n\n# Path to the Excel file (already downloaded / mounted inside the execution environment)\nfile_path = \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\"\n\n# ---------- subtask-1: list all sheet names ----------\ntry:\n    xls = pd.ExcelFile(file_path)\n    sheet_names = xls.sheet_names\nexcept Exception as e:\n    raise RuntimeError(f\"Unable to open workbook: {e}\")\n\n# ---------- subtask-2: detect acetylproteomics sheet ----------\nacetyl_sheet = None\n\n# 1) direct keyword search in the sheet names\nkeywords = [\"acetyl\", \"acetylproteome\", \"acetylproteomics\", \"acetylome\"]\nfor name in sheet_names:\n    if any(k.lower() in name.lower() for k in keywords):\n        acetyl_sheet = name\n        break\n\n# 2) if no direct match, fall back on scanning the first rows/columns of each sheet\nif acetyl_sheet is None:\n    for name in sheet_names:\n        try:\n            df_preview = pd.read_excel(file_path, sheet_name=name, nrows=5)\n            cand_cols = \" \".join(str(c) for c in df_preview.columns)\n            if any(k.lower() in cand_cols.lower() for k in keywords):\n                acetyl_sheet = name\n                break\n        except Exception:\n            # Skip sheets that cannot be read as tabular (e.g., figure sheets)\n            continue\n\n# Answers for printing\nanswers = {\n    \"subtask-1\": sheet_names,\n    \"subtask-2\": acetyl_sheet,\n    \"main-task\": acetyl_sheet\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 27.965148210525513
      },
      {
        "task_id": "biomedical-hard-7-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: How many are the significant genes by acetylproteomics?, please answer the following question:\n What is the total number of entries in the acetylproteomics data sheet, accounting for a header row? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which worksheet in the Excel file contains the acetyl-proteomics data (i.e., its sheet name)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "D-SE-acetyl"
            },
            {
              "id": "subtask-2",
              "query": "How many non-empty data rows are in the acetylproteomics worksheet (excluding the header row)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 15
            }
          ],
          "answer": 16
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\nfile_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\n\n# -------- subtask-1: find the worksheet name --------------------------------\nxls = pd.ExcelFile(file_path)\n# look for a sheet whose name contains 'acetyl' (case-insensitive)\nacetyl_sheet_name = None\nfor name in xls.sheet_names:\n    if \"acetyl\" in name.lower():\n        acetyl_sheet_name = name\n        break\n# if nothing matched, just take the first sheet as a fallback\nif acetyl_sheet_name is None:\n    acetyl_sheet_name = xls.sheet_names[0]\n\n# -------- subtask-2: count the data rows (excluding header) ------------------\ndf = pd.read_excel(file_path, sheet_name=acetyl_sheet_name, header=0)\n\n# Drop any rows that are completely empty\ndf_clean = df.dropna(how=\"all\")\ndata_row_count = len(df_clean)\n\n# -------- main-task: total entries INCLUDING header --------------------------\ntotal_entries_with_header = data_row_count + 1   # add the header row\n\n# ---------------- print answers in required JSON format ----------------------\nprint(json.dumps({\n    \"subtask-1\": acetyl_sheet_name,\n    \"subtask-2\": data_row_count,\n    \"main-task\": total_entries_with_header\n}, indent=4))",
        "runtime": 15.318868398666382
      }
    ],
    "runtime": 26.96643042564392
  },
  {
    "task_id": "biomedical-hard-8",
    "model_output": {
      "id": "main-task",
      "query": "Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc1.xlsx",
        "1-s2.0-S0092867420301070-mmc6.xlsx",
        "hyperactivated.csv"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Which sample IDs belong to CNV-high endometroid tumours?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ]
        },
        {
          "id": "subtask-2",
          "query": "Which protein sites are hyper-activated in the CNV-high endometroid samples identified in subtask-1?",
          "data_sources": [
            "hyperactivated.csv"
          ]
        },
        {
          "id": "subtask-3",
          "query": "Among the hyper-activated protein sites from subtask-2, which ones are targets of FDA-approved drugs?",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc6.xlsx"
          ]
        }
      ],
      "answer": "** ERRORS ** decoding output in /home/gridsan/elai/Kramabench/system_scratch/BaselineLLMSystemGPTo3FewShot/biomedical-hard-8/_intermediate/pipeline-1_out.json"
    },
    "code": "import pandas as pd\nimport json\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper 1: read all sheets and find the *first* one whose column\n#           names mention at least ONE of the supplied keywords\ndef find_sheet_containing(excel_file, keywords, nrows=5):\n    xls = pd.ExcelFile(excel_file)\n    for sheet in xls.sheet_names:\n        try:\n            df = xls.parse(sheet, nrows=nrows)\n        except Exception:\n            continue\n        cols = [str(c).lower() for c in df.columns]\n        if any(k.lower() in \" \".join(cols) for k in keywords):\n            return sheet\n    return None\n\n\n# helper 2: given a dataframe, try to guess a column that matches any\n#           of a list of *candidate* names\ndef pick_column(df, candidates):\n    for c in df.columns:\n        c_low = str(c).lower()\n        for key in candidates:\n            if key in c_low:\n                return c\n    return None\n\n\n# ------------------------------------------------------------------\n# paths\nbase = \"/home/gridsan/elai/Kramabench/data/biomedical/input\"\nmmc1_path  = Path(base) / \"1-s2.0-S0092867420301070-mmc1.xlsx\"\nmmc6_path  = Path(base) / \"1-s2.0-S0092867420301070-mmc6.xlsx\"\nhyper_path = Path(base) / \"hyperactivated.csv\"\n\n# ------------------------------------------------------------------\n# ------------------- subtask-1: sample annotation ------------------\nsample_keywords = [\"sample\", \"id\", \"histology\", \"histotype\", \"subtype\", \"tcga\", \"group\"]\nsample_sheet = find_sheet_containing(mmc1_path, sample_keywords)\n\ncnv_high_samples = []  # will be populated below\n\nif sample_sheet is not None:\n    sample_df = pd.read_excel(mmc1_path, sheet_name=sample_sheet, engine=\"openpyxl\")\n    sample_df.columns = sample_df.columns.str.strip()\n\n    # guess column names\n    sample_col  = pick_column(sample_df, [\"sample\", \"id\", \"case\", \"tumor\"])\n    hist_col    = pick_column(sample_df, [\"histology\", \"histotype\", \"histological\"])\n    subtype_col = pick_column(sample_df, [\"subtype\", \"tcga\", \"group\", \"copy number\", \"classification\"])\n\n    # if we have all three columns \u2192 filter normally\n    if sample_col and hist_col and subtype_col:\n        mask = (\n            sample_df[hist_col].astype(str).str.contains(\"endometr\", case=False, na=False)\n            & sample_df[subtype_col].astype(str).str.contains(\"cnv\", case=False, na=False)\n            & sample_df[subtype_col].astype(str).str.contains(\"high\", case=False, na=False)\n        )\n        cnv_high_samples = (\n            sample_df.loc[mask, sample_col]\n            .astype(str)\n            .str.strip()\n            .unique()\n            .tolist()\n        )\n\n# Fallback:  if we failed to detect any CNV-high samples, **assume**\n#            that every sample present in hyperactivated.csv is CNV-high.\nif not cnv_high_samples:\n    print(\n        \"WARNING  : Could not identify CNV-high samples from the annotation file \u2013 \"\n        \"falling back to all samples listed in hyperactivated.csv.\"\n    )\n    # We'll fill this once we load the hyper file.\n\n# ---------------- subtask-2: hyper-activated sites -----------------\nhyper_df = pd.read_csv(hyper_path)\nhyper_df.columns = hyper_df.columns.str.strip().str.lower()\n\nsample_col_h = \"sample_id\" if \"sample_id\" in hyper_df.columns else hyper_df.columns[0]\nprotein_col  = \"protein\"   if \"protein\"   in hyper_df.columns else hyper_df.columns[1]\n\n# fallback fill for cnv_high_samples if still empty\nif not cnv_high_samples:\n    cnv_high_samples = (\n        hyper_df[sample_col_h]\n        .astype(str)\n        .str.strip()\n        .unique()\n        .tolist()\n    )\n\nhyper_sites_cnv = (\n    hyper_df[hyper_df[sample_col_h].isin(cnv_high_samples)][protein_col]\n    .dropna()\n    .astype(str)\n    .str.strip()\n    .unique()\n    .tolist()\n)\n\n# --------------- subtask-3: FDA-approved drug targets --------------\ndrug_keywords = [\"fda\", \"drug\", \"target\"]\ndrug_sheet = find_sheet_containing(mmc6_path, drug_keywords)\n\nfda_targets = []\nif drug_sheet is not None:\n    drug_df = pd.read_excel(mmc6_path, sheet_name=drug_sheet, engine=\"openpyxl\")\n    drug_df.columns = drug_df.columns.str.strip().str.lower()\n\n    prot_cols = [c for c in drug_df.columns if any(k in c for k in [\"gene\", \"protein\", \"target\"])]\n    fda_col   = next((c for c in drug_df.columns if \"fda\" in c and \"approved\" in c), None)\n\n    if prot_cols and fda_col:\n        fda_mask = drug_df[fda_col].astype(str).str.contains(\n            \"yes|true|approved|fda\", case=False, na=False\n        )\n        fda_targets = (\n            drug_df.loc[fda_mask, prot_cols]\n            .melt(value_name=\"protein\")[\"protein\"]\n            .dropna()\n            .astype(str)\n            .str.strip()\n            .str.upper()\n            .unique()\n            .tolist()\n        )\n    else:\n        print(\"WARNING  : Could not locate protein or FDA status columns \u2013 no FDA targets extracted.\")\nelse:\n    print(\"WARNING  : Could not find a sheet containing drug-target information in mmc6.\")\n\n# --------------------------- main answer ---------------------------\nhyper_proteins_upper = [p.upper() for p in hyper_sites_cnv]\nanswer_main = sorted(set(hyper_proteins_upper) & set(fda_targets))\n\n# ------------------------ assemble answers -------------------------\nanswers = {\n    \"subtask-1\": cnv_high_samples,\n    \"subtask-2\": hyper_sites_cnv,\n    \"subtask-3\": answer_main,   # same list as main result\n    \"main-task\": answer_main\n}\n\nprint(json.dumps(answers, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-hard-8-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n Which files contain patient metadata, hyperactivated phosphoproteomics data, and FDA-approved drug target data? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which supplemental Excel file contains patient-level clinical / sample metadata (e.g., sample-id, patient-id, histology, CNV class, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "1-s2.0-S0092867420301070-mmc3.xlsx"
            },
            {
              "id": "subtask-2",
              "query": "Which data file contains the list of hyperactivated protein (phospho)sites for every tumour sample?",
              "data_sources": [
                "hyperactivated.csv"
              ],
              "answer": "hyperactivated.csv"
            },
            {
              "id": "subtask-3",
              "query": "Which supplemental Excel file lists FDA-approved drugs and their molecular protein targets?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx"
              ],
              "answer": "not-found"
            }
          ],
          "answer": {
            "patient-metadata-file": "1-s2.0-S0092867420301070-mmc3.xlsx",
            "hyperactivated-data-file": "hyperactivated.csv",
            "fda-drug-target-file": "not-found"
          }
        },
        "code": "import json\nimport os\nimport pandas as pd\n\n# Paths that were provided by the task runner\nfiles = {\n    \"mmc1\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"mmc2\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"mmc3\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\",\n    \"mmc4\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"mmc5\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"mmc6\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"mmc7\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"hyper\": \"/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv\"\n}\n\n# ------------------------------------------------------------------------------\n# helper that tries to infer the nature of an excel file by peeking at the\n# sheet names and the first few column headers\ndef inspect_excel(path, nrows=5):\n    try:\n        xl = pd.ExcelFile(path)\n        info = []\n        for sheet in xl.sheet_names:\n            df = xl.parse(sheet, nrows=nrows)\n            cols = list(df.columns)\n            info.append({\"sheet\": sheet, \"columns\": cols[:10]})\n        return info\n    except Exception as e:\n        return str(e)\n\n\n# Sub-task 1 \u2013 look for a sheet with patient / sample meta-data like \u2018Sample_ID\u2019\nmeta_candidates = {}\nfor tag in [\"mmc1\", \"mmc2\", \"mmc3\"]:\n    meta_candidates[tag] = inspect_excel(files[tag])\n\n# Heuristic: choose the file that contains columns such as\n# \u2018Sample_ID\u2019, \u2018Patient_ID\u2019, \u2018Histology\u2019, \u2018CNV\u2019, etc.\nmeta_file = None\nfor tag, sheets in meta_candidates.items():\n    if isinstance(sheets, str):      # error while reading\n        continue\n    for sh in sheets:\n        cols_lower = [str(c).lower() for c in sh[\"columns\"]]\n        if any(key in cols_lower for key in [\"sample\", \"patient\", \"cnv\", \"histology\"]):\n            meta_file = os.path.basename(files[tag])\n            break\n    if meta_file:\n        break\n\n# Sub-task 2 \u2013 hyperactivated phosphoproteomics list\nhyper_file = os.path.basename(files[\"hyper\"]) if os.path.exists(files[\"hyper\"]) else None\n\n# Sub-task 3 \u2013 file with FDA-approved drugs & targets.\nfda_candidates = {}\nfor tag in [\"mmc7\", \"mmc6\", \"mmc5\"]:\n    fda_candidates[tag] = inspect_excel(files[tag])\n\nfda_file = None\nfor tag, sheets in fda_candidates.items():\n    if isinstance(sheets, str):\n        continue\n    for sh in sheets:\n        cols_lower = [str(c).lower() for c in sh[\"columns\"]]\n        if any(key in cols_lower for key in [\"drug\", \"fda\", \"target\", \"uniprotswissprot\"]):\n            fda_file = os.path.basename(files[tag])\n            break\n    if fda_file:\n        break\n\nanswers = {\n    \"subtask-1\": meta_file or \"not-found\",\n    \"subtask-2\": hyper_file or \"not-found\",\n    \"subtask-3\": fda_file or \"not-found\",\n    \"main-task\": {\n        \"patient-metadata-file\": meta_file or \"not-found\",\n        \"hyperactivated-data-file\": hyper_file or \"not-found\",\n        \"fda-drug-target-file\": fda_file or \"not-found\"\n    }\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 26.354599952697754
      },
      {
        "task_id": "biomedical-hard-8-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n What endometrioid samples included in the study are CNV-high? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc1.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in 1-s2.0-S0092867420301070-mmc1.xlsx contain sample-level annotations such as histology, subtype and copy-number group?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": [
                "UCEC_CPTAC3_meta_table_V2.1"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Within the sheet identified in subtask-1, what is/are the exact column name(s) that encode CNV or CNA status (e.g. \u201cCNV_group\u201d, \u201cCopy number subtype\u201d, etc.) and the column that encodes tumour histological subtype (e.g. \u201cHistology\u201d, \u201cSubtype\u201d, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": {
                "sheet": "UCEC_CPTAC3_meta_table_V2.1",
                "cnv_col": "CNV_4q_AMP",
                "histology_col": "Genomics_subtype"
              }
            },
            {
              "id": "subtask-3",
              "query": "Using the columns from subtask-2, which rows correspond to samples that are 1) Histology = \u2018Endometrioid\u2019 (case-insensitive match, allow spelling variants) and 2) CNV/CNA group = \u2018CNV-high\u2019 (again, allow spelling variants such as \u201cCN-high\u201d, \u201cCN_HIGH\u201d, etc.)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": "0 rows match both criteria"
            },
            {
              "id": "subtask-4",
              "query": "Return the unique sample identifiers for those rows picked in subtask-3 (e.g. TCGA barcodes or whatever identifier the study uses).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc1.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\n\n# path to the supplementary Excel file\nxls_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\")\n\n# ---------- subtask-1: find candidate sheets ---------- #\nall_sheets = pd.ExcelFile(xls_path).sheet_names\n\n# Heuristic: pick sheets that contain words suggesting sample annotations\ncandidate_keywords = ['clinical', 'sample', 'annotation', 'metadata', 'table', 'patient']\ncandidate_sheets = [s for s in all_sheets if any(k.lower() in s.lower() for k in candidate_keywords)]\n# If no obvious match, fall back to all sheets (will try each)\nif not candidate_sheets:\n    candidate_sheets = all_sheets\n\n# ---------- subtask-2: locate the columns ---------- #\ncnv_col = None\nhistology_col = None\nsheet_with_columns = None\n\nfor sheet in candidate_sheets:\n    try:\n        df_head = pd.read_excel(xls_path, sheet_name=sheet, nrows=200)  # read only first 200 rows\n    except Exception:\n        continue  # skip unreadable sheet\n\n    # clean column names\n    df_head.columns = [str(c).strip() for c in df_head.columns]\n    # possible CNV column names\n    for col in df_head.columns:\n        if re.search(r'(cnv|cna|copy)', col, flags=re.I) and re.search(r'(group|status|subtype|high|low)', col, flags=re.I):\n            cnv_col = col\n        if re.search(r'(histolog|subtype|tumou?r\\s*type)', col, flags=re.I):\n            histology_col = col\n    if cnv_col and histology_col:\n        sheet_with_columns = sheet\n        break\n\n# if still not found, fall back to brute-force search in all sheets\nif sheet_with_columns is None:\n    for sheet in all_sheets:\n        try:\n            df_head = pd.read_excel(xls_path, sheet_name=sheet, nrows=200)\n        except Exception:\n            continue\n        df_head.columns = [str(c).strip() for c in df_head.columns]\n        for col in df_head.columns:\n            if re.search(r'(cnv|cna|copy)', col, flags=re.I):\n                cnv_col = col\n            if re.search(r'(histolog|subtype|tumou?r\\s*type)', col, flags=re.I):\n                histology_col = col\n        if cnv_col and histology_col:\n            sheet_with_columns = sheet\n            break\n\n# ---------- subtask-3: filter for endometrioid & CNV-high ---------- #\ncnv_high_samples = []\nif sheet_with_columns:\n    df = pd.read_excel(xls_path, sheet_name=sheet_with_columns)\n    # strip whitespace in columns again\n    df.columns = [str(c).strip() for c in df.columns]\n\n    # make defensive lowercase copies for matching\n    def to_str(x):\n        return str(x).strip().lower() if pd.notna(x) else ''\n\n    df['_histology_lower'] = df[histology_col].apply(to_str)\n    df['_cnv_lower'] = df[cnv_col].apply(to_str)\n\n    # rows that satisfy both conditions\n    mask = df['_histology_lower'].str.contains('endometrioid') & df['_cnv_lower'].str.contains('high')\n    filtered = df.loc[mask]\n\n    # guess sample identifier column: the leftmost column that looks like an ID\n    sample_id_col = filtered.columns[0]\n    # but if there is a column explicitly named 'Sample', 'Sample ID', 'Tumor', etc., prefer that\n    for col in filtered.columns:\n        if re.search(r'(sample|tumou?r|barcode|patient)', col, flags=re.I):\n            sample_id_col = col\n            break\n\n    cnv_high_samples = sorted(filtered[sample_id_col].dropna().astype(str).unique())\n\n# Assemble answers\nanswers = {\n    \"subtask-1\": candidate_sheets,\n    \"subtask-2\": {\"sheet\": sheet_with_columns, \"cnv_col\": cnv_col, \"histology_col\": histology_col},\n    \"subtask-3\": f\"{len(cnv_high_samples)} rows match both criteria\",\n    \"subtask-4\": cnv_high_samples,\n    \"main-task\": cnv_high_samples\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 34.165748834609985
      },
      {
        "task_id": "biomedical-hard-8-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n What proteins sites are hyperactivated in the endometrioid samples which are CNV-high? \n\n",
          "data_sources": [
            "hyperactivated.csv"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Load the data and make sure the columns `sample_id` and `protein` exist and are treated as strings.",
              "data_sources": [
                "hyperactivated.csv"
              ],
              "answer": "153 rows loaded."
            },
            {
              "id": "subtask-2",
              "query": "Remove rows where `protein` or `sample_id` is missing/null and strip extra spaces from `protein` names.",
              "data_sources": [
                "hyperactivated.csv"
              ],
              "answer": "153 rows remain after cleaning."
            },
            {
              "id": "subtask-3",
              "query": "Return the unique list of hyperactivated protein sites found in the cleaned table.",
              "data_sources": [
                "hyperactivated.csv"
              ],
              "answer": [
                "ABRAXAS2",
                "ACOX1",
                "ADH1B",
                "AGPAT1",
                "ANKRD54",
                "ANP32B",
                "ANXA5",
                "APOBEC3A",
                "ARPC3",
                "BCAS3",
                "CAPNS1",
                "CAVIN2",
                "CCDC127",
                "CCDC88C",
                "CD46",
                "CD58",
                "CDCA5",
                "CDH2",
                "CDK12",
                "CDK7",
                "CHMP1B",
                "CHMP4A",
                "CNOT1",
                "COL6A1",
                "CRP",
                "CRTAP",
                "CSK",
                "CTU1",
                "CUL5",
                "DDO",
                "DERL1",
                "DNAAF5",
                "DPYSL4",
                "DSCC1",
                "DUSP14",
                "EIF3D",
                "ELOF1",
                "F12",
                "FAM114A2",
                "FAM83G",
                "FEZ2",
                "FOSL2",
                "GALNT2",
                "GDA",
                "GLIPR1",
                "GLYATL2",
                "GRAMD2B",
                "GTF2IRD2",
                "GTPBP3",
                "H2AFX",
                "HDDC3",
                "HK2",
                "HMGCR",
                "HSD17B7",
                "IDS",
                "IFT88",
                "IGF1",
                "IRAK1",
                "KHDRBS3",
                "KIF23",
                "KIF7",
                "LILRB4",
                "LIMCH1",
                "LYSMD2",
                "MAN1A1",
                "MAOB",
                "MARK3",
                "MCM3AP",
                "MCTS1",
                "MGARP",
                "MVD",
                "MYL6",
                "MYO5A",
                "NDUFB8",
                "NELFCD",
                "NIFK",
                "NIP7",
                "OCLN",
                "OLFM4",
                "PABPC1",
                "PARG",
                "PATJ",
                "PFN1",
                "PHKG2",
                "PLEKHM1",
                "PNKD",
                "POC5",
                "POLB",
                "PRKAG2",
                "PRR3",
                "PRR5L",
                "PTPN18",
                "PXMP4",
                "QRSL1",
                "RAB2B",
                "RAB8A",
                "RABGAP1",
                "RASAL1",
                "RCSD1",
                "RHOF",
                "RILPL2",
                "RPS15",
                "RRAGC",
                "RTKN",
                "RUNX1T1",
                "S100A14",
                "SAMD9L",
                "SAR1B",
                "SLC25A29",
                "SLC25A4",
                "SLC25A40",
                "SLC30A6",
                "SMARCA4",
                "SNCAIP",
                "SOCS2",
                "SRSF1",
                "SRSF8",
                "SSR4",
                "STAT5B",
                "STK32C",
                "STK36",
                "STOM",
                "SYNE2",
                "SYT17",
                "TADA2A",
                "TBC1D2B",
                "TCHH",
                "TECR",
                "THBS1",
                "TMA7",
                "TMCO4",
                "TMEM109",
                "TRAF5",
                "TRMT10C",
                "TUBA4A",
                "TWISTNB",
                "UNC13D",
                "VPS26C",
                "VRK2",
                "WAC",
                "WARS",
                "WAS",
                "WDR74",
                "WRAP73",
                "WWC2",
                "YKT6",
                "ZBTB21",
                "ZNF131",
                "ZNF480"
              ]
            }
          ],
          "answer": [
            "ABRAXAS2",
            "ACOX1",
            "ADH1B",
            "AGPAT1",
            "ANKRD54",
            "ANP32B",
            "ANXA5",
            "APOBEC3A",
            "ARPC3",
            "BCAS3",
            "CAPNS1",
            "CAVIN2",
            "CCDC127",
            "CCDC88C",
            "CD46",
            "CD58",
            "CDCA5",
            "CDH2",
            "CDK12",
            "CDK7",
            "CHMP1B",
            "CHMP4A",
            "CNOT1",
            "COL6A1",
            "CRP",
            "CRTAP",
            "CSK",
            "CTU1",
            "CUL5",
            "DDO",
            "DERL1",
            "DNAAF5",
            "DPYSL4",
            "DSCC1",
            "DUSP14",
            "EIF3D",
            "ELOF1",
            "F12",
            "FAM114A2",
            "FAM83G",
            "FEZ2",
            "FOSL2",
            "GALNT2",
            "GDA",
            "GLIPR1",
            "GLYATL2",
            "GRAMD2B",
            "GTF2IRD2",
            "GTPBP3",
            "H2AFX",
            "HDDC3",
            "HK2",
            "HMGCR",
            "HSD17B7",
            "IDS",
            "IFT88",
            "IGF1",
            "IRAK1",
            "KHDRBS3",
            "KIF23",
            "KIF7",
            "LILRB4",
            "LIMCH1",
            "LYSMD2",
            "MAN1A1",
            "MAOB",
            "MARK3",
            "MCM3AP",
            "MCTS1",
            "MGARP",
            "MVD",
            "MYL6",
            "MYO5A",
            "NDUFB8",
            "NELFCD",
            "NIFK",
            "NIP7",
            "OCLN",
            "OLFM4",
            "PABPC1",
            "PARG",
            "PATJ",
            "PFN1",
            "PHKG2",
            "PLEKHM1",
            "PNKD",
            "POC5",
            "POLB",
            "PRKAG2",
            "PRR3",
            "PRR5L",
            "PTPN18",
            "PXMP4",
            "QRSL1",
            "RAB2B",
            "RAB8A",
            "RABGAP1",
            "RASAL1",
            "RCSD1",
            "RHOF",
            "RILPL2",
            "RPS15",
            "RRAGC",
            "RTKN",
            "RUNX1T1",
            "S100A14",
            "SAMD9L",
            "SAR1B",
            "SLC25A29",
            "SLC25A4",
            "SLC25A40",
            "SLC30A6",
            "SMARCA4",
            "SNCAIP",
            "SOCS2",
            "SRSF1",
            "SRSF8",
            "SSR4",
            "STAT5B",
            "STK32C",
            "STK36",
            "STOM",
            "SYNE2",
            "SYT17",
            "TADA2A",
            "TBC1D2B",
            "TCHH",
            "TECR",
            "THBS1",
            "TMA7",
            "TMCO4",
            "TMEM109",
            "TRAF5",
            "TRMT10C",
            "TUBA4A",
            "TWISTNB",
            "UNC13D",
            "VPS26C",
            "VRK2",
            "WAC",
            "WARS",
            "WAS",
            "WDR74",
            "WRAP73",
            "WWC2",
            "YKT6",
            "ZBTB21",
            "ZNF131",
            "ZNF480"
          ]
        },
        "code": "import pandas as pd\nimport json\n\n# ---------- subtask-1: load and ensure correct types ----------\ndf = pd.read_csv('/home/gridsan/elai/Kramabench/data/biomedical/input/hyperactivated.csv')\n\n# Make sure the expected columns exist and are strings\nfor col in [\"sample_id\", \"protein\"]:\n    if col not in df.columns:\n        raise ValueError(f\"Expected column `{col}` not found in dataset.\")\ndf[\"sample_id\"] = df[\"sample_id\"].astype(str)\ndf[\"protein\"]   = df[\"protein\"].astype(str)\n\n# ---------- subtask-2: basic cleaning ----------\n# Drop rows with null/empty protein or sample_id\ndf_clean = df.dropna(subset=[\"sample_id\", \"protein\"]).copy()\n\n# Strip extra whitespace and drop rows whose protein field becomes empty after stripping\ndf_clean[\"protein\"] = df_clean[\"protein\"].str.strip()\ndf_clean = df_clean[df_clean[\"protein\"] != \"\"]\n\n# ---------- subtask-3: get list of unique hyperactivated proteins ----------\nhyperactivated_proteins = sorted(df_clean[\"protein\"].unique().tolist())\n\n# Assemble answers for each step\nanswers = {\n    \"subtask-1\": f\"{len(df)} rows loaded.\",\n    \"subtask-2\": f\"{len(df_clean)} rows remain after cleaning.\",\n    \"subtask-3\": hyperactivated_proteins,\n    \"main-task\": hyperactivated_proteins\n}\n\nprint(json.dumps(answers, indent=4))",
        "runtime": 7.26176643371582
      },
      {
        "task_id": "biomedical-hard-8-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: Which protein sites are found to be hyperactivated in CNV-high endometroid samples and are targeted by FDA-approved drugs?, please answer the following question:\n What protein sites are targeted by FDA-approved drugs? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc6.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) in 1-s2.0-S0092867420301070-mmc6.xlsx contain information on FDA-approved drugs and their protein targets?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": []
            },
            {
              "id": "subtask-2",
              "query": "From the identified sheet(s), extract every row that is flagged as an FDA-approved drug (or \u2018Yes\u2019 in an \u201cFDA approved\u201d column).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": 0
            },
            {
              "id": "subtask-3",
              "query": "For those FDA-approved drugs, list the unique protein sites (e.g., \"EGFR_pY1068\") that the drugs target. Handle possible data issues such as missing values, extra white-spaces, different capitalisation, or multiple sites separated by commas / semicolons.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc6.xlsx"
              ],
              "answer": []
            }
          ],
          "answer": []
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper to robustly read every sheet (header row might not be first)\n# ------------------------------------------------------------------\ndef read_excel_safely(path):\n    xl = pd.ExcelFile(path)\n    sheets = {}\n    for sheet_name in xl.sheet_names:\n        # try the most common header style first\n        try:\n            df = xl.parse(sheet_name, header=0)\n        except Exception:\n            df = xl.parse(sheet_name, header=None)\n        sheets[sheet_name] = df\n    return sheets\n\n\nfile_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx')\n\n# -------------------------\n# subtask-1  \u2013 locate sheet\n# -------------------------\nsheets = read_excel_safely(file_path)\ncandidate_sheets = []\nfor name, df in sheets.items():\n    lowered_cols = [str(c).lower() for c in df.columns]\n    if any(\"fda\" in col for col in lowered_cols) and any(\"target\" in col or \"site\" in col for col in lowered_cols):\n        candidate_sheets.append(name)\n\n# -------------------------\n# subtask-2 \u2013 keep FDA rows\n# -------------------------\nfda_rows = []\nfor sheet in candidate_sheets:\n    df = sheets[sheet]\n    # identify column that marks FDA approval\n    fda_col = None\n    for c in df.columns:\n        if \"fda\" in str(c).lower():\n            fda_col = c\n            break\n    if fda_col is None:\n        continue\n    # treat any case-insensitive 'yes', 'approved', 'y', 1 as True\n    fda_mask = df[fda_col].astype(str).str.strip().str.lower().isin(\n        [\"yes\", \"approved\", \"y\", \"true\", \"1\"]\n    )\n    fda_rows.append(df.loc[fda_mask])\n\nif fda_rows:\n    fda_df = pd.concat(fda_rows, ignore_index=True)\nelse:\n    fda_df = pd.DataFrame()\n\n# -------------------------\n# subtask-3 \u2013 extract sites\n# -------------------------\nprotein_site_cols = [c for c in fda_df.columns if any(\n    key in str(c).lower() for key in [\"site\", \"target\", \"substrate\"]\n)]\n\nsites = set()\nfor col in protein_site_cols:\n    fda_df[col] = fda_df[col].astype(str)\n    for cell in fda_df[col]:\n        if cell.lower() in [\"nan\", \"none\", \"\"]:\n            continue\n        # split on common delimiters\n        for piece in re.split(r'[;,/| ]+', cell):\n            piece = piece.strip()\n            if piece:\n                sites.add(piece)\n\nanswer_sites = sorted(sites)\n\n# -------------------------\n# print answers\n# -------------------------\nprint(json.dumps({\n    \"subtask-1\": candidate_sheets,\n    \"subtask-2\": len(fda_df),\n    \"subtask-3\": answer_sites,\n    \"main-task\": answer_sites\n}, indent=4))",
        "runtime": 47.6764931678772
      }
    ],
    "runtime": 66.00242352485657
  },
  {
    "task_id": "biomedical-easy-9",
    "model_output": {
      "system_subtasks_responses": [],
      "id": "main-task",
      "query": "What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?",
      "data_sources": [
        "1-s2.0-S0092867420301070-mmc3.xlsx"
      ],
      "subtasks": [
        {
          "id": "subtask-1",
          "query": "Identify which sheet(s) in the Excel file contain an FDR-like column as well as a gene identifier column.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": {
            "sheet_used": "F-SS-phospho",
            "gene_column": "Gene",
            "fdr_column": "FDR.phos"
          }
        },
        {
          "id": "subtask-2",
          "query": "Within the chosen sheet(s), standardise column names and coerce the FDR column to numeric, coercing errors to NaN and dropping missing values.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": "FDR column coerced to numeric; rows with NaN dropped."
        },
        {
          "id": "subtask-3",
          "query": "Calculate the average FDR for rows where the gene symbol equals 'CBX3'.",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": 0.01684066897229979
        },
        {
          "id": "subtask-4",
          "query": "Calculate the average FDR for all other genes (i.e., gene symbol \u2260 'CBX3').",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": 0.02484464434616611
        },
        {
          "id": "subtask-5",
          "query": "Return the difference between the two averages (CBX3 \u2013 Other).",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "answer": -0.00800397537386632
        }
      ],
      "answer": -0.00800397537386632
    },
    "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\nFILE = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx')\n\n# ------------------------------------------------------------------\n# Helper: find sheet(s) that contain both a gene and an FDR column.\n# ------------------------------------------------------------------\nsheets_with_fdr = []\nxls = pd.ExcelFile(FILE)\nfor sheet_name in xls.sheet_names:\n    df_preview = xls.parse(sheet_name, nrows=10)  # light read\n    cols = {c.lower(): c for c in df_preview.columns}  # map to original\n    gene_column = next((cols[c] for c in cols if 'gene' in c), None)\n    fdr_column  = next((cols[c] for c in cols if 'fdr' in c or 'qval' in c or 'q-val' in c), None)\n    if gene_column and fdr_column:\n        sheets_with_fdr.append((sheet_name, gene_column, fdr_column))\n\n# If multiple sheets qualify, use the first; otherwise, raise an error.\nif not sheets_with_fdr:\n    raise ValueError(\"No sheet containing both gene and FDR-like columns was found.\")\nsheet_name, gene_col, fdr_col = sheets_with_fdr[0]\n\n# ------------------------------------------------------------------\n# Read full sheet and clean.\n# ------------------------------------------------------------------\ndf = xls.parse(sheet_name)\n\n# Standardise & clean column names\ndf_columns_lower = {c: c.lower() for c in df.columns}\ndf.rename(columns=df_columns_lower, inplace=True)\ngene_col_lower = gene_col.lower()\nfdr_col_lower  = fdr_col.lower()\n\n# Coerce FDR column to numeric\ndf[fdr_col_lower] = pd.to_numeric(df[fdr_col_lower], errors='coerce')\n\n# Drop rows without FDR or gene symbol\ndf = df.dropna(subset=[gene_col_lower, fdr_col_lower])\n\n# Ensure gene symbols are treated as strings & strip whitespace\ndf[gene_col_lower] = df[gene_col_lower].astype(str).str.strip()\n\n# ------------------------------------------------------------------\n# Compute averages\n# ------------------------------------------------------------------\ncbx3_mask = df[gene_col_lower].str.upper() == 'CBX3'\navg_fdr_cbx3 = df.loc[cbx3_mask, fdr_col_lower].mean()\navg_fdr_other = df.loc[~cbx3_mask, fdr_col_lower].mean()\ndifference = avg_fdr_cbx3 - avg_fdr_other\n\n# ------------------------------------------------------------------\n# Print answers for each subtask\n# ------------------------------------------------------------------\nprint(json.dumps({\n    \"subtask-1\": {\n        \"sheet_used\": sheet_name,\n        \"gene_column\": gene_col,\n        \"fdr_column\": fdr_col\n    },\n    \"subtask-2\": \"FDR column coerced to numeric; rows with NaN dropped.\",\n    \"subtask-3\": avg_fdr_cbx3,\n    \"subtask-4\": avg_fdr_other,\n    \"subtask-5\": difference,\n    \"main-task\": difference\n}, indent=4))",
    "subresponses": [
      {
        "task_id": "biomedical-easy-9-1",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What file contains false discovery rate (FDR) values? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc7.xlsx",
            "hyperactivated.csv",
            "1-s2.0-S0092867420301070-mmc2.xlsx",
            "1-s2.0-S0092867420301070-mmc4.xlsx",
            "1-s2.0-S0092867420301070-mmc5.xlsx",
            "1-s2.0-S0092867420301070-mmc1.xlsx",
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Scan every provided Excel file for any sheet that contains a column whose header (case\u2013insensitive) includes the string 'fdr' or the phrase 'false discovery'. Return the file-name(s) in which such a column is found.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc7.xlsx",
                "1-s2.0-S0092867420301070-mmc2.xlsx",
                "1-s2.0-S0092867420301070-mmc4.xlsx",
                "1-s2.0-S0092867420301070-mmc5.xlsx",
                "1-s2.0-S0092867420301070-mmc1.xlsx",
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "1-s2.0-S0092867420301070-mmc6.xlsx",
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ]
            }
          ],
          "answer": [
            "1-s2.0-S0092867420301070-mmc6.xlsx",
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ]
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# helper: safely read a (large) Excel file and peek into each sheet\n# ------------------------------------------------------------------\ndef file_contains_fdr(path):\n    \"\"\"\n    Open `path` as an Excel file and return True if, in any sheet,\n    at least one column name contains 'fdr' or 'false discovery'\n    (case\u2013insensitive).  If the file cannot be opened, return False.\n    \"\"\"\n    try:\n        xls = pd.ExcelFile(path)\n    except Exception:\n        # corrupted or not really an Excel workbook\n        return False\n    \n    for sheet in xls.sheet_names:\n        try:\n            # read only the first few rows to avoid loading huge sheets\n            df_sample = xls.parse(sheet, nrows=5)\n        except Exception:\n            continue  # move on to next sheet\n        \n        # normalise to string & lower case\n        cols = [str(c).lower() for c in df_sample.columns]\n        if any((\"fdr\" in c) or (\"false discovery\" in c) for c in cols):\n            return True\n    return False\n\n\nexcel_files = [\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc7.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc2.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc4.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc5.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc1.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc6.xlsx\",\n    \"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\",\n]\n\nfdr_files = [Path(p).name for p in excel_files if file_contains_fdr(p)]\n\nprint(\n    json.dumps(\n        {\n            \"subtask-1\": fdr_files,\n            \"main-task\": fdr_files  # for this particular prompt, it is the same answer\n        },\n        indent=4\n    )\n)",
        "runtime": 22.998211145401
      },
      {
        "task_id": "biomedical-easy-9-2",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n Which columns are necessary from the input data file? \n\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [],
          "answer": {
            "gene_column": [
              "Gene"
            ],
            "fdr_column": [
              "FDR.phos",
              "FDR.prot",
              "logfdr.phos",
              "logfdr.prot",
              "signedfdr.phos",
              "signedfdr.prot"
            ]
          }
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\n\n# ------------------------------------------------------------------\n# 1. Load the Excel file and show the columns in every sheet\n# ------------------------------------------------------------------\nxlsx_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\nall_sheets = pd.read_excel(xlsx_path, sheet_name=None)\n\nsheet_columns = {sheet: list(df.columns) for sheet, df in all_sheets.items()}\n\n# ------------------------------------------------------------------\n# 2. Decide which columns are strictly needed to compare FDRs\n#    \u2013 one identifying the gene (symbol/name)\n#    \u2013 one holding the false-discovery rate (FDR or adjusted-P value)\n# ------------------------------------------------------------------\n# Look for the first column that looks like a gene identifier\ngene_col_candidates = set()\nfdr_col_candidates  = set()\n\nfor cols in sheet_columns.values():\n    for col in cols:\n        c = str(col).lower()\n        if any(key in c for key in [\"gene\", \"symbol\", \"target\"]):\n            gene_col_candidates.add(col)\n        if \"fdr\" in c or (\"p_val_adj\" in c) or (\"padj\" in c):\n            fdr_col_candidates.add(col)\n\n# In most single-cell / differential-expression supplementary tables\n# the relevant columns are typically named exactly:\n#   - 'gene' (or 'Gene')  \u2190 gene symbol\n#   - 'p_val_adj' or 'FDR'\u2190 false discovery rate\n#\n# Those two columns are all that are required for the downstream\n# comparison between CBX3 and the remaining genes.\nnecessary_columns = {\n    \"gene_column\" : sorted(gene_col_candidates),\n    \"fdr_column\"  : sorted(fdr_col_candidates)\n}\n\nprint(json.dumps({\"main-task\": necessary_columns}, indent=4))",
        "runtime": 13.131985902786255
      },
      {
        "task_id": "biomedical-easy-9-3",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What are the average FDR values for each gene?\n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "List all sheet names contained in the workbook so that we know where potential FDR data might reside.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "README",
                "A-MSI",
                "B-SE-proteomics",
                "C-SE-phospho",
                "D-SE-acetyl",
                "E-Mutation-adjacent",
                "F-SS-phospho"
              ]
            },
            {
              "id": "subtask-2",
              "query": "From every sheet, identify columns that refer to genes (column name contains 'gene', 'Gene', 'Symbol', etc.) and columns that refer to false discovery rate (column name contains 'FDR', 'Fdr', 'q-value', 'qvalue', etc.). Return a long-form dataframe with two columns: Gene, FDR.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "Extracted 630 Gene\u2013FDR rows from 1 sheets."
            },
            {
              "id": "subtask-3",
              "query": "Clean the extracted Gene and FDR columns: \u2022 strip white-space, ensure Gene values are strings \u2022 coerce FDR to numeric (float), setting non-numeric or missing entries to NaN \u2022 drop rows where either Gene or FDR is missing.",
              "data_sources": [],
              "answer": "Remaining rows after cleaning: 630"
            },
            {
              "id": "subtask-4",
              "query": "Compute the average FDR for every unique gene symbol found in the cleaned table.",
              "data_sources": [],
              "answer": {
                "AAK1": 0.043956340583415396,
                "ABCF1": 0.0078521613399921,
                "ABLIM1": 0.0496760101814453,
                "ACTL6A": 0.0205708302930587,
                "ADD2": 0.0267724421983287,
                "ADNP": 0.00511262716054596,
                "AFF4": 0.0394362313390499,
                "AGAP2": 0.030202402696061,
                "AGRN": 0.0396716365884564,
                "AHNAK": 0.0300909141488162,
                "AIF1L": 0.0141086530576075,
                "AKAP11": 0.0365433285394853,
                "AKAP9": 0.0247082834366274,
                "AKIRIN2": 0.00575581465720716,
                "AKNA": 0.0180754648214029,
                "AMOT": 0.0429541861987379,
                "ANKLE2": 0.03641930848065587,
                "ANKRD17": 0.01882743304837297,
                "AP4B1": 0.015723209794619,
                "APC": 0.04673785183598855,
                "API5": 0.013905958534478,
                "APOBR": 0.0457711894718315,
                "APTX": 0.0377307000368468,
                "ARFGAP1": 0.0189119245800733,
                "ARHGAP21": 0.0365433285394853,
                "ARHGAP23": 0.0429123695917402,
                "ARHGAP29": 0.03742468300232555,
                "ARHGEF12": 0.024818221188327463,
                "ARHGEF18": 0.0116912247226169,
                "ARID2": 0.0464203504903622,
                "ARID5B": 0.0225928753273997,
                "ARPC5L": 0.011298571216312,
                "ASPSCR1": 0.0468420084479082,
                "ATN1": 0.0297994065230863,
                "ATR": 0.03465631347636565,
                "ATXN2L": 0.02374687882857916,
                "AUP1": 0.0338437319620087,
                "BABAM1": 0.022915051818072064,
                "BAZ1B": 0.00264170698308115,
                "BCL11A": 0.030525886289553023,
                "BICDL2": 0.0433973813975053,
                "BIN2": 0.0121729427813012,
                "BLM": 0.0330686437156121,
                "BOD1L1": 0.01411303457891054,
                "BPTF": 0.0244596186580932,
                "BUB3": 0.0457101882637775,
                "BUD13": 0.00703530023458142,
                "C17orf49": 0.000980881159299924,
                "C18orf25": 0.040056765216411,
                "C2CD5": 0.0102162361818213,
                "C2orf49": 0.000358566570028842,
                "C2orf88": 0.0348398874245796,
                "C5orf30": 3.53760522249561e-05,
                "C6orf106": 0.009443200898048985,
                "C8orf33": 0.0375334621383179,
                "CAP1": 0.0109260628771659,
                "CBLB": 0.0486162909120264,
                "CBX3": 0.016840668972299792,
                "CBX4": 0.0189119245800733,
                "CCDC137": 0.0439488652893517,
                "CCDC78": 0.0307072245607024,
                "CCDC82": 0.0263860865210061,
                "CCDC86": 0.044663410889452845,
                "CCDC92": 0.0263551921581343,
                "CCNL1": 0.024556301830255006,
                "CCNYL1": 0.0257083221557895,
                "CCSAP": 0.0281129797331958,
                "CD2AP": 0.015777524985309314,
                "CD34": 0.0365433285394853,
                "CDAN1": 0.00209151767284925,
                "CDC14A": 0.0335462926741171,
                "CDC42EP4": 0.0258799666476875,
                "CDCA2": 0.0445660045541175,
                "CDCA5": 0.013376677521683325,
                "CENPC": 0.01572743529945242,
                "CEP131": 0.0321323153270959,
                "CEP95": 0.00903663259943753,
                "CGNL1": 0.0352564339325247,
                "CHAMP1": 0.0322946216561759,
                "CHD3": 0.04333206028716445,
                "CHD6": 0.0373202471752503,
                "CHD7": 0.0401441524276685,
                "CHEK2": 0.00881894886244621,
                "CIC": 0.0168133556489441,
                "CIZ1": 0.02093247757915614,
                "CLCC1": 0.027842528865652,
                "CLIP1": 0.0476862206203252,
                "CLK3": 0.0334892594110799,
                "CNOT3": 0.00432965572242424,
                "CRYBG1": 0.0354891195256936,
                "CTNND1": 0.014247697199297258,
                "CTTN": 0.047774896206704,
                "CUL4B": 0.023219697293174066,
                "DACT3": 0.0362867053879639,
                "DBR1": 0.0037774448807903,
                "DCP2": 0.00120989257999747,
                "DDB1": 0.0363634383914052,
                "DDB2": 0.0470860227567865,
                "DDHD1": 0.0164523294921849,
                "DDX20": 0.0211175754635949,
                "DDX50": 0.00696837494906315,
                "DENND4C": 0.0392812998337344,
                "DGCR8": 0.0440804124283028,
                "DLGAP4": 0.0480237851027848,
                "DMXL2": 0.0298664551937986,
                "DNAJA1": 0.0445505646862037,
                "DOCK11": 0.0477951114660338,
                "DOT1L": 0.0436974479354995,
                "DSN1": 0.012139852775975067,
                "DYNC1LI1": 0.0145893733025529,
                "EAF1": 0.0168697641557901,
                "EEF2": 0.00829125996436007,
                "EIF2S2": 0.028334264747518746,
                "EIF3H": 0.00756429895549288,
                "EIF4B": 0.0473670166191686,
                "EIF4G3": 0.0460613348652966,
                "EML4": 0.0345718255898893,
                "EMSY": 0.0381411566387611,
                "ENAH": 0.00819256283991357,
                "ENTPD4": 0.0305027122396485,
                "EPRS": 0.0106618952416762,
                "EXOC7": 0.0333909430904532,
                "EXOSC2": 0.0443090256138802,
                "FAM129A": 0.0281129797331958,
                "FAM193A": 0.0197997969511195,
                "FAM208B": 0.0235703093112661,
                "FAM219A": 0.0304365074448916,
                "FAM83D": 0.000879784873047865,
                "FAM83G": 0.0494723914078752,
                "FAM84B": 0.0304219855097464,
                "FARP2": 0.03115419502385014,
                "FBXO28": 0.000164513068071164,
                "FIP1L1": 0.00870437374226926,
                "FLII": 0.00542256800118294,
                "FLNB": 0.02179416147232348,
                "FLOT1": 0.00935388605974624,
                "FNBP4": 0.00518328925662211,
                "FOXA2": 0.009849521679926395,
                "FOXK1": 0.0003134655198048007,
                "FOXK2": 0.0176422435789253,
                "FOXO3": 0.025577594478051524,
                "FUBP1": 0.0204898876900786,
                "FXR1": 0.0461412011156122,
                "GABARAPL2": 0.0346388028086022,
                "GARNL3": 0.0408201218683491,
                "GART": 0.0360336537657683,
                "GAS2L2": 0.025161375628582273,
                "GCFC2": 0.0290578234220004,
                "GNL3L": 0.0187971767630604,
                "GPALPP1": 0.0382360557777518,
                "GPATCH4": 0.0445646481793812,
                "GPBP1L1": 0.0189981163392183,
                "GPI": 0.0147769221258296,
                "H2AFX": 0.0050204946798786745,
                "H2AFY": 0.0165403467376094,
                "HDGF": 0.02547520559747745,
                "HDLBP": 0.033185880218213,
                "HIP1": 0.0165096012577211,
                "HIST1H1E": 0.0168329439119639,
                "HMBOX1": 0.00961012850632467,
                "HMGA1": 0.018806420933700357,
                "HMGXB4": 0.00996002755398111,
                "HNRNPAB": 0.011827107497728374,
                "HNRNPM": 0.0354122347844906,
                "HP1BP3": 0.0399768129248203,
                "HSF1": 0.0053191829352203,
                "HSP90B1": 0.019531869075253866,
                "HSPH1": 0.00317067607895234,
                "HTATSF1": 0.02671835137353153,
                "HUWE1": 0.0411406623713756,
                "ICA1L": 0.0386466483903568,
                "IGF2BP1": 0.029324466627102,
                "ILF3": 0.00375363666519567,
                "INA": 0.0389763270603149,
                "INCENP": 0.0364253658298323,
                "ING5": 0.0227513131502596,
                "INTS3": 0.0404939378115201,
                "IPCEF1": 0.0478035141775167,
                "IPO5": 0.0365433285394853,
                "IRF2BP1": 0.0284355707777988,
                "IWS1": 0.01837915535348926,
                "JPT1": 0.0353571291519793,
                "KANSL3": 0.0096327620783505,
                "KAT14": 0.0375334621383179,
                "KAT6B": 0.0365484291209722,
                "KCTD3": 0.0496567595583864,
                "KDM1A": 0.0213763830894492,
                "KHSRP": 0.0460934138591996,
                "KIAA0232": 0.022504514565002073,
                "KIAA0556": 0.03502566337261288,
                "KIAA0753": 0.0461422370512207,
                "KIAA1191": 0.0360336537657683,
                "KIDINS220": 0.0427962896047914,
                "KIF13A": 0.0411293078541561,
                "KMT2A": 0.0279546183659566,
                "KMT2B": 0.0147163919366181,
                "KRT16": 0.0385203422302532,
                "KRT17": 0.0327843618335718,
                "LARP1B": 0.027271199482557698,
                "LAS1L": 0.018760216755420642,
                "LASP1": 0.0272872531074229,
                "LDB1": 0.0304219855097464,
                "LIG3": 0.0200499197741112,
                "LIMCH1": 0.0443092834734158,
                "LIN37": 0.0130337399053229,
                "LIN54": 0.0209904919636789,
                "LLGL2": 0.0442151786820634,
                "LMNA": 0.0437639532625381,
                "LMNB1": 0.0322830937335546,
                "LRCH1": 0.0479082393688815,
                "LRRFIP1": 0.00696837494906315,
                "LRWD1": 0.00716930926044789,
                "MAP1S": 0.0463009055794489,
                "MAP4": 0.000490284131252745,
                "MAP7": 0.0214500497215805,
                "MAP7D1": 0.0285307502682166,
                "MAPK8IP3": 0.0291040759630139,
                "MARCKS": 0.030202402696061,
                "MARK1": 0.016782176925974,
                "MARS": 0.0404570693732631,
                "MCC": 0.03266018569266625,
                "MCM3": 0.00646221665138953,
                "MED1": 0.021481848893426733,
                "MEF2C": 0.0277338213611755,
                "MEPCE": 0.0162587503419938,
                "MFHAS1": 0.00956282909468471,
                "MGA": 0.029893238309011153,
                "MIA3": 0.00579881961113552,
                "MICALL1": 0.0274324904452464,
                "MISP3": 0.0382559216084444,
                "MKI67": 0.0289385957956281,
                "MRTFA": 0.0411293078541561,
                "MSL1": 0.00901481031238281,
                "MTA1": 0.0297939975628482,
                "MTCL1": 0.0276381632261513,
                "MYO9A": 0.0399897321430616,
                "NAA10": 0.018634628703567403,
                "NACA": 0.0277338213611755,
                "NASP": 0.0496417849998354,
                "NAV1": 0.00449873675088239,
                "NBEAL2": 0.0324706026414712,
                "NBN": 0.021061738038853594,
                "NCOA5": 0.0397406307173996,
                "NCOR1": 0.0218704519227371,
                "NCOR2": 0.028641975489636852,
                "NEDD1": 0.0245566675369464,
                "NFATC2IP": 0.0160715198530403,
                "NFATC4": 0.03035523705042805,
                "NICN1": 0.016423176158984,
                "NIPBL": 0.0031617303720742037,
                "NKAP": 0.0368116113424702,
                "NKTR": 0.0441420547926059,
                "NME9": 0.0298858210285503,
                "NMT2": 0.01944485426083505,
                "NOC2L": 0.008253048728965404,
                "NOL12": 0.0355555088154915,
                "NOL8": 0.00615640064312127,
                "NOLC1": 0.024612885819526437,
                "NOP58": 0.0182584698459063,
                "NPM1": 0.02259129099789245,
                "NT5C": 0.00192535497831041,
                "NUCKS1": 0.021928099342605575,
                "NUDC": 0.00820832149950257,
                "NUP214": 0.019765663265695167,
                "NUP88": 0.00104413235463156,
                "NUP98": 0.0432475697805802,
                "OCIAD1": 0.0134943067389894,
                "ORC2": 0.00828642708073982,
                "PACS2": 0.0219290747075447,
                "PAK3": 0.0246007874898777,
                "PALLD": 0.0288254566458157,
                "PALM": 0.020542664435670634,
                "PANK4": 0.00881894886244621,
                "PARD3": 0.0460062658000308,
                "PATJ": 0.0164677420111589,
                "PBRM1": 0.044317821258459,
                "PBX2": 0.00320829748025083,
                "PCDH19": 0.0272872531074229,
                "PCF11": 0.0291705443705582,
                "PDS5B": 0.021698767270994696,
                "PELP1": 0.0373202471752503,
                "PEX19": 0.0132248961123523,
                "PFKP": 0.00034823519263003,
                "PHF14": 0.0297580241863299,
                "PHF20": 0.010155514207283065,
                "PHF3": 0.00124168361335943,
                "PI4KB": 0.0375025771584155,
                "PIK3AP1": 0.0252481993101516,
                "PITPNM3": 0.0168133556489441,
                "PKN1": 0.0425037272974999,
                "PLCH1": 0.0290345545706679,
                "PLEKHG2": 0.0303160095576093,
                "PLEKHM1": 0.027169999767462,
                "POP1": 0.0318857971755643,
                "PPIL6": 0.0465605467419352,
                "PPIP5K1": 0.0263869627455006,
                "PPP1R12A": 0.0347477199760802,
                "PPP1R14A": 0.0374144887933437,
                "PPP1R9B": 0.00815232888990476,
                "PPP4R2": 0.00641088493834257,
                "PPP6R2": 0.0332082349479447,
                "PRCC": 0.0110460079305209,
                "PRKCZ": 0.0443854065571069,
                "PRPF3": 0.0167825809464426,
                "PRPF38B": 0.02667985483378142,
                "PRRC2A": 0.0162882856389512,
                "PRRC2B": 0.021999780852726105,
                "PRRC2C": 0.00530469798848326,
                "PSMA2": 0.0101658193319756,
                "PTPN21": 0.03239689119022557,
                "PWWP2A": 0.0432594729442924,
                "RAB13": 0.0494174705629229,
                "RAB3IP": 0.0197997969511195,
                "RAB8A": 0.0186492495913322,
                "RAD50": 0.00626865096280216,
                "RAD9A": 0.0261263934393165,
                "RALGPS2": 0.0353669229204015,
                "RANBP2": 0.01840232892259712,
                "RB1": 0.019894896884840807,
                "RBBP5": 0.000545955530476966,
                "RBM12": 0.0102034242673521,
                "RBM15": 0.00426965673197485,
                "RBM15B": 0.0499983073946662,
                "RICTOR": 0.0272872531074229,
                "RLIM": 0.0302252052808086,
                "RNF168": 0.00682139309494012,
                "RNF4": 0.028399545589546948,
                "RPA1": 0.02427581947495829,
                "RPL30": 0.0355065359477124,
                "RPLP2": 0.00526230309191826,
                "RPS2": 0.0192834209107639,
                "RPS27": 0.00747588714540171,
                "RRBP1": 0.0365844158219992,
                "RREB1": 0.0249362393233076,
                "RSF1": 0.024039027030464438,
                "RSPH3": 0.0151020920805762,
                "RSPH4A": 0.013848228193634555,
                "RUBCN": 0.0365484291209722,
                "S100PBP": 0.03308185336504,
                "SACS": 0.0261263934393165,
                "SASH3": 0.0463806341791065,
                "SCRIB": 0.0374144887933437,
                "SEC16A": 0.0152443679189897,
                "SEL1L": 0.0476862206203252,
                "SET": 0.00340924548957681,
                "SETD1B": 0.0349645382832812,
                "SETD2": 0.02520368878493828,
                "SF3B1": 0.00425583102022784,
                "SF3B2": 0.0257041415329438,
                "SF3B3": 0.0150696287432672,
                "SIPA1L3": 0.0180754648214029,
                "SKA3": 0.00228078600498271,
                "SLBP": 0.0371408674066081,
                "SLC8A1": 0.0232741834932739,
                "SLK": 0.01928110738494836,
                "SMARCA1": 0.0293532485733266,
                "SMARCA4": 0.03214970436771115,
                "SMARCC1": 0.017453824072883,
                "SNX24": 0.034327414579176,
                "SOGA1": 0.0454691231013339,
                "SP140": 0.0234811824231471,
                "SPAG9": 0.020878258460720434,
                "SPATS2L": 0.0374144887933437,
                "SPEG": 0.0399751718574391,
                "SPEN": 0.02010414682979206,
                "SPP1": 0.0370751465629521,
                "SRPK2": 0.0106445729035592,
                "SRRM2": 0.03071484591912444,
                "SRSF11": 0.0492913248497778,
                "SRSF12": 0.029411523087563347,
                "SRSF2": 0.030790189398542653,
                "ST13": 0.0382559216084444,
                "STAU1": 0.0217280527645178,
                "STMN1": 0.00419495709841418,
                "STX17": 0.00148589263873434,
                "SYNGR2": 0.0144359469762761,
                "SYNPO": 0.020369419494287,
                "SYNRG": 0.018077287090468797,
                "SZRD1": 0.0360336537657683,
                "TAF15": 0.0327034020860868,
                "TANGO6": 0.0155898300627306,
                "TBX2": 0.03638005478163332,
                "TCF12": 0.0497992644587145,
                "TCF20": 0.0315773649909543,
                "TCOF1": 0.009315999464395012,
                "TERF2IP": 0.01999275807124975,
                "TFAM": 0.0341716449098999,
                "TGM2": 0.0275304040264136,
                "TLE2": 0.0156871837667427,
                "TMPO": 0.02590155835447288,
                "TNIP1": 0.0163057182734311,
                "TNRC6A": 0.0455837739848171,
                "TNS1": 0.0317740787112154,
                "TNS3": 0.0218235881321484,
                "TOMM70": 0.030900171772955152,
                "TP53BP1": 0.015914743806578344,
                "TRAPPC8": 0.0341716449098999,
                "TRIM28": 0.03068943571831135,
                "TRIT1": 0.0390932758187843,
                "TSC2": 0.026996410406962615,
                "TSHZ3": 0.0336356206979959,
                "TTF2": 0.0486250708765263,
                "TUBGCP3": 0.0274324904452464,
                "TXLNA": 0.0350854516754513,
                "UBA1": 0.014833098717392055,
                "UBAP2L": 0.0466049147436694,
                "UBE2O": 0.0314950361168889,
                "UCKL1": 0.00207247098461645,
                "UFD1": 0.023825581665335202,
                "UGDH": 0.00518328925662211,
                "UNC45A": 0.0496567595583864,
                "USF2": 0.0491924982363911,
                "USP10": 0.0037774448807903,
                "USP32": 0.0395243155467003,
                "USP47": 0.0138565710361594,
                "USP8": 0.0492742541491687,
                "USP9X": 0.0230727213712009,
                "UTP14A": 0.004274746861318775,
                "VIM": 0.0324706026414712,
                "WIPF2": 0.0156657923191347,
                "WNK1": 0.0219399301026229,
                "XRCC1": 0.00253764928159042,
                "XRCC6": 0.0060231290822709,
                "YEATS2": 0.03394187398361015,
                "ZBTB10": 0.0281384896852693,
                "ZBTB21": 0.00701550687582381,
                "ZBTB9": 0.0123832951383463,
                "ZC3H13": 0.027406113169242498,
                "ZC3H8": 0.0399768129248203,
                "ZC3HC1": 0.0004264386291949855,
                "ZFAND2B": 0.0375334621383179,
                "ZFAND3": 0.00115885398086681,
                "ZFAND5": 0.0198740515186142,
                "ZFC3H1": 0.0115115007567177,
                "ZFP91": 0.0218235881321484,
                "ZMYND11": 0.0270620914209947,
                "ZMYND8": 0.0168133556489441,
                "ZNF318": 0.026252497345200288,
                "ZNF326": 0.0173870040853599,
                "ZNF451": 0.01540701734134332,
                "ZNF592": 0.0202793914528402,
                "ZNF638": 0.0120809958371911,
                "ZNF687": 0.0415452010828718
              }
            }
          ],
          "answer": {
            "AAK1": 0.043956340583415396,
            "ABCF1": 0.0078521613399921,
            "ABLIM1": 0.0496760101814453,
            "ACTL6A": 0.0205708302930587,
            "ADD2": 0.0267724421983287,
            "ADNP": 0.00511262716054596,
            "AFF4": 0.0394362313390499,
            "AGAP2": 0.030202402696061,
            "AGRN": 0.0396716365884564,
            "AHNAK": 0.0300909141488162,
            "AIF1L": 0.0141086530576075,
            "AKAP11": 0.0365433285394853,
            "AKAP9": 0.0247082834366274,
            "AKIRIN2": 0.00575581465720716,
            "AKNA": 0.0180754648214029,
            "AMOT": 0.0429541861987379,
            "ANKLE2": 0.03641930848065587,
            "ANKRD17": 0.01882743304837297,
            "AP4B1": 0.015723209794619,
            "APC": 0.04673785183598855,
            "API5": 0.013905958534478,
            "APOBR": 0.0457711894718315,
            "APTX": 0.0377307000368468,
            "ARFGAP1": 0.0189119245800733,
            "ARHGAP21": 0.0365433285394853,
            "ARHGAP23": 0.0429123695917402,
            "ARHGAP29": 0.03742468300232555,
            "ARHGEF12": 0.024818221188327463,
            "ARHGEF18": 0.0116912247226169,
            "ARID2": 0.0464203504903622,
            "ARID5B": 0.0225928753273997,
            "ARPC5L": 0.011298571216312,
            "ASPSCR1": 0.0468420084479082,
            "ATN1": 0.0297994065230863,
            "ATR": 0.03465631347636565,
            "ATXN2L": 0.02374687882857916,
            "AUP1": 0.0338437319620087,
            "BABAM1": 0.022915051818072064,
            "BAZ1B": 0.00264170698308115,
            "BCL11A": 0.030525886289553023,
            "BICDL2": 0.0433973813975053,
            "BIN2": 0.0121729427813012,
            "BLM": 0.0330686437156121,
            "BOD1L1": 0.01411303457891054,
            "BPTF": 0.0244596186580932,
            "BUB3": 0.0457101882637775,
            "BUD13": 0.00703530023458142,
            "C17orf49": 0.000980881159299924,
            "C18orf25": 0.040056765216411,
            "C2CD5": 0.0102162361818213,
            "C2orf49": 0.000358566570028842,
            "C2orf88": 0.0348398874245796,
            "C5orf30": 3.53760522249561e-05,
            "C6orf106": 0.009443200898048985,
            "C8orf33": 0.0375334621383179,
            "CAP1": 0.0109260628771659,
            "CBLB": 0.0486162909120264,
            "CBX3": 0.016840668972299792,
            "CBX4": 0.0189119245800733,
            "CCDC137": 0.0439488652893517,
            "CCDC78": 0.0307072245607024,
            "CCDC82": 0.0263860865210061,
            "CCDC86": 0.044663410889452845,
            "CCDC92": 0.0263551921581343,
            "CCNL1": 0.024556301830255006,
            "CCNYL1": 0.0257083221557895,
            "CCSAP": 0.0281129797331958,
            "CD2AP": 0.015777524985309314,
            "CD34": 0.0365433285394853,
            "CDAN1": 0.00209151767284925,
            "CDC14A": 0.0335462926741171,
            "CDC42EP4": 0.0258799666476875,
            "CDCA2": 0.0445660045541175,
            "CDCA5": 0.013376677521683325,
            "CENPC": 0.01572743529945242,
            "CEP131": 0.0321323153270959,
            "CEP95": 0.00903663259943753,
            "CGNL1": 0.0352564339325247,
            "CHAMP1": 0.0322946216561759,
            "CHD3": 0.04333206028716445,
            "CHD6": 0.0373202471752503,
            "CHD7": 0.0401441524276685,
            "CHEK2": 0.00881894886244621,
            "CIC": 0.0168133556489441,
            "CIZ1": 0.02093247757915614,
            "CLCC1": 0.027842528865652,
            "CLIP1": 0.0476862206203252,
            "CLK3": 0.0334892594110799,
            "CNOT3": 0.00432965572242424,
            "CRYBG1": 0.0354891195256936,
            "CTNND1": 0.014247697199297258,
            "CTTN": 0.047774896206704,
            "CUL4B": 0.023219697293174066,
            "DACT3": 0.0362867053879639,
            "DBR1": 0.0037774448807903,
            "DCP2": 0.00120989257999747,
            "DDB1": 0.0363634383914052,
            "DDB2": 0.0470860227567865,
            "DDHD1": 0.0164523294921849,
            "DDX20": 0.0211175754635949,
            "DDX50": 0.00696837494906315,
            "DENND4C": 0.0392812998337344,
            "DGCR8": 0.0440804124283028,
            "DLGAP4": 0.0480237851027848,
            "DMXL2": 0.0298664551937986,
            "DNAJA1": 0.0445505646862037,
            "DOCK11": 0.0477951114660338,
            "DOT1L": 0.0436974479354995,
            "DSN1": 0.012139852775975067,
            "DYNC1LI1": 0.0145893733025529,
            "EAF1": 0.0168697641557901,
            "EEF2": 0.00829125996436007,
            "EIF2S2": 0.028334264747518746,
            "EIF3H": 0.00756429895549288,
            "EIF4B": 0.0473670166191686,
            "EIF4G3": 0.0460613348652966,
            "EML4": 0.0345718255898893,
            "EMSY": 0.0381411566387611,
            "ENAH": 0.00819256283991357,
            "ENTPD4": 0.0305027122396485,
            "EPRS": 0.0106618952416762,
            "EXOC7": 0.0333909430904532,
            "EXOSC2": 0.0443090256138802,
            "FAM129A": 0.0281129797331958,
            "FAM193A": 0.0197997969511195,
            "FAM208B": 0.0235703093112661,
            "FAM219A": 0.0304365074448916,
            "FAM83D": 0.000879784873047865,
            "FAM83G": 0.0494723914078752,
            "FAM84B": 0.0304219855097464,
            "FARP2": 0.03115419502385014,
            "FBXO28": 0.000164513068071164,
            "FIP1L1": 0.00870437374226926,
            "FLII": 0.00542256800118294,
            "FLNB": 0.02179416147232348,
            "FLOT1": 0.00935388605974624,
            "FNBP4": 0.00518328925662211,
            "FOXA2": 0.009849521679926395,
            "FOXK1": 0.0003134655198048007,
            "FOXK2": 0.0176422435789253,
            "FOXO3": 0.025577594478051524,
            "FUBP1": 0.0204898876900786,
            "FXR1": 0.0461412011156122,
            "GABARAPL2": 0.0346388028086022,
            "GARNL3": 0.0408201218683491,
            "GART": 0.0360336537657683,
            "GAS2L2": 0.025161375628582273,
            "GCFC2": 0.0290578234220004,
            "GNL3L": 0.0187971767630604,
            "GPALPP1": 0.0382360557777518,
            "GPATCH4": 0.0445646481793812,
            "GPBP1L1": 0.0189981163392183,
            "GPI": 0.0147769221258296,
            "H2AFX": 0.0050204946798786745,
            "H2AFY": 0.0165403467376094,
            "HDGF": 0.02547520559747745,
            "HDLBP": 0.033185880218213,
            "HIP1": 0.0165096012577211,
            "HIST1H1E": 0.0168329439119639,
            "HMBOX1": 0.00961012850632467,
            "HMGA1": 0.018806420933700357,
            "HMGXB4": 0.00996002755398111,
            "HNRNPAB": 0.011827107497728374,
            "HNRNPM": 0.0354122347844906,
            "HP1BP3": 0.0399768129248203,
            "HSF1": 0.0053191829352203,
            "HSP90B1": 0.019531869075253866,
            "HSPH1": 0.00317067607895234,
            "HTATSF1": 0.02671835137353153,
            "HUWE1": 0.0411406623713756,
            "ICA1L": 0.0386466483903568,
            "IGF2BP1": 0.029324466627102,
            "ILF3": 0.00375363666519567,
            "INA": 0.0389763270603149,
            "INCENP": 0.0364253658298323,
            "ING5": 0.0227513131502596,
            "INTS3": 0.0404939378115201,
            "IPCEF1": 0.0478035141775167,
            "IPO5": 0.0365433285394853,
            "IRF2BP1": 0.0284355707777988,
            "IWS1": 0.01837915535348926,
            "JPT1": 0.0353571291519793,
            "KANSL3": 0.0096327620783505,
            "KAT14": 0.0375334621383179,
            "KAT6B": 0.0365484291209722,
            "KCTD3": 0.0496567595583864,
            "KDM1A": 0.0213763830894492,
            "KHSRP": 0.0460934138591996,
            "KIAA0232": 0.022504514565002073,
            "KIAA0556": 0.03502566337261288,
            "KIAA0753": 0.0461422370512207,
            "KIAA1191": 0.0360336537657683,
            "KIDINS220": 0.0427962896047914,
            "KIF13A": 0.0411293078541561,
            "KMT2A": 0.0279546183659566,
            "KMT2B": 0.0147163919366181,
            "KRT16": 0.0385203422302532,
            "KRT17": 0.0327843618335718,
            "LARP1B": 0.027271199482557698,
            "LAS1L": 0.018760216755420642,
            "LASP1": 0.0272872531074229,
            "LDB1": 0.0304219855097464,
            "LIG3": 0.0200499197741112,
            "LIMCH1": 0.0443092834734158,
            "LIN37": 0.0130337399053229,
            "LIN54": 0.0209904919636789,
            "LLGL2": 0.0442151786820634,
            "LMNA": 0.0437639532625381,
            "LMNB1": 0.0322830937335546,
            "LRCH1": 0.0479082393688815,
            "LRRFIP1": 0.00696837494906315,
            "LRWD1": 0.00716930926044789,
            "MAP1S": 0.0463009055794489,
            "MAP4": 0.000490284131252745,
            "MAP7": 0.0214500497215805,
            "MAP7D1": 0.0285307502682166,
            "MAPK8IP3": 0.0291040759630139,
            "MARCKS": 0.030202402696061,
            "MARK1": 0.016782176925974,
            "MARS": 0.0404570693732631,
            "MCC": 0.03266018569266625,
            "MCM3": 0.00646221665138953,
            "MED1": 0.021481848893426733,
            "MEF2C": 0.0277338213611755,
            "MEPCE": 0.0162587503419938,
            "MFHAS1": 0.00956282909468471,
            "MGA": 0.029893238309011153,
            "MIA3": 0.00579881961113552,
            "MICALL1": 0.0274324904452464,
            "MISP3": 0.0382559216084444,
            "MKI67": 0.0289385957956281,
            "MRTFA": 0.0411293078541561,
            "MSL1": 0.00901481031238281,
            "MTA1": 0.0297939975628482,
            "MTCL1": 0.0276381632261513,
            "MYO9A": 0.0399897321430616,
            "NAA10": 0.018634628703567403,
            "NACA": 0.0277338213611755,
            "NASP": 0.0496417849998354,
            "NAV1": 0.00449873675088239,
            "NBEAL2": 0.0324706026414712,
            "NBN": 0.021061738038853594,
            "NCOA5": 0.0397406307173996,
            "NCOR1": 0.0218704519227371,
            "NCOR2": 0.028641975489636852,
            "NEDD1": 0.0245566675369464,
            "NFATC2IP": 0.0160715198530403,
            "NFATC4": 0.03035523705042805,
            "NICN1": 0.016423176158984,
            "NIPBL": 0.0031617303720742037,
            "NKAP": 0.0368116113424702,
            "NKTR": 0.0441420547926059,
            "NME9": 0.0298858210285503,
            "NMT2": 0.01944485426083505,
            "NOC2L": 0.008253048728965404,
            "NOL12": 0.0355555088154915,
            "NOL8": 0.00615640064312127,
            "NOLC1": 0.024612885819526437,
            "NOP58": 0.0182584698459063,
            "NPM1": 0.02259129099789245,
            "NT5C": 0.00192535497831041,
            "NUCKS1": 0.021928099342605575,
            "NUDC": 0.00820832149950257,
            "NUP214": 0.019765663265695167,
            "NUP88": 0.00104413235463156,
            "NUP98": 0.0432475697805802,
            "OCIAD1": 0.0134943067389894,
            "ORC2": 0.00828642708073982,
            "PACS2": 0.0219290747075447,
            "PAK3": 0.0246007874898777,
            "PALLD": 0.0288254566458157,
            "PALM": 0.020542664435670634,
            "PANK4": 0.00881894886244621,
            "PARD3": 0.0460062658000308,
            "PATJ": 0.0164677420111589,
            "PBRM1": 0.044317821258459,
            "PBX2": 0.00320829748025083,
            "PCDH19": 0.0272872531074229,
            "PCF11": 0.0291705443705582,
            "PDS5B": 0.021698767270994696,
            "PELP1": 0.0373202471752503,
            "PEX19": 0.0132248961123523,
            "PFKP": 0.00034823519263003,
            "PHF14": 0.0297580241863299,
            "PHF20": 0.010155514207283065,
            "PHF3": 0.00124168361335943,
            "PI4KB": 0.0375025771584155,
            "PIK3AP1": 0.0252481993101516,
            "PITPNM3": 0.0168133556489441,
            "PKN1": 0.0425037272974999,
            "PLCH1": 0.0290345545706679,
            "PLEKHG2": 0.0303160095576093,
            "PLEKHM1": 0.027169999767462,
            "POP1": 0.0318857971755643,
            "PPIL6": 0.0465605467419352,
            "PPIP5K1": 0.0263869627455006,
            "PPP1R12A": 0.0347477199760802,
            "PPP1R14A": 0.0374144887933437,
            "PPP1R9B": 0.00815232888990476,
            "PPP4R2": 0.00641088493834257,
            "PPP6R2": 0.0332082349479447,
            "PRCC": 0.0110460079305209,
            "PRKCZ": 0.0443854065571069,
            "PRPF3": 0.0167825809464426,
            "PRPF38B": 0.02667985483378142,
            "PRRC2A": 0.0162882856389512,
            "PRRC2B": 0.021999780852726105,
            "PRRC2C": 0.00530469798848326,
            "PSMA2": 0.0101658193319756,
            "PTPN21": 0.03239689119022557,
            "PWWP2A": 0.0432594729442924,
            "RAB13": 0.0494174705629229,
            "RAB3IP": 0.0197997969511195,
            "RAB8A": 0.0186492495913322,
            "RAD50": 0.00626865096280216,
            "RAD9A": 0.0261263934393165,
            "RALGPS2": 0.0353669229204015,
            "RANBP2": 0.01840232892259712,
            "RB1": 0.019894896884840807,
            "RBBP5": 0.000545955530476966,
            "RBM12": 0.0102034242673521,
            "RBM15": 0.00426965673197485,
            "RBM15B": 0.0499983073946662,
            "RICTOR": 0.0272872531074229,
            "RLIM": 0.0302252052808086,
            "RNF168": 0.00682139309494012,
            "RNF4": 0.028399545589546948,
            "RPA1": 0.02427581947495829,
            "RPL30": 0.0355065359477124,
            "RPLP2": 0.00526230309191826,
            "RPS2": 0.0192834209107639,
            "RPS27": 0.00747588714540171,
            "RRBP1": 0.0365844158219992,
            "RREB1": 0.0249362393233076,
            "RSF1": 0.024039027030464438,
            "RSPH3": 0.0151020920805762,
            "RSPH4A": 0.013848228193634555,
            "RUBCN": 0.0365484291209722,
            "S100PBP": 0.03308185336504,
            "SACS": 0.0261263934393165,
            "SASH3": 0.0463806341791065,
            "SCRIB": 0.0374144887933437,
            "SEC16A": 0.0152443679189897,
            "SEL1L": 0.0476862206203252,
            "SET": 0.00340924548957681,
            "SETD1B": 0.0349645382832812,
            "SETD2": 0.02520368878493828,
            "SF3B1": 0.00425583102022784,
            "SF3B2": 0.0257041415329438,
            "SF3B3": 0.0150696287432672,
            "SIPA1L3": 0.0180754648214029,
            "SKA3": 0.00228078600498271,
            "SLBP": 0.0371408674066081,
            "SLC8A1": 0.0232741834932739,
            "SLK": 0.01928110738494836,
            "SMARCA1": 0.0293532485733266,
            "SMARCA4": 0.03214970436771115,
            "SMARCC1": 0.017453824072883,
            "SNX24": 0.034327414579176,
            "SOGA1": 0.0454691231013339,
            "SP140": 0.0234811824231471,
            "SPAG9": 0.020878258460720434,
            "SPATS2L": 0.0374144887933437,
            "SPEG": 0.0399751718574391,
            "SPEN": 0.02010414682979206,
            "SPP1": 0.0370751465629521,
            "SRPK2": 0.0106445729035592,
            "SRRM2": 0.03071484591912444,
            "SRSF11": 0.0492913248497778,
            "SRSF12": 0.029411523087563347,
            "SRSF2": 0.030790189398542653,
            "ST13": 0.0382559216084444,
            "STAU1": 0.0217280527645178,
            "STMN1": 0.00419495709841418,
            "STX17": 0.00148589263873434,
            "SYNGR2": 0.0144359469762761,
            "SYNPO": 0.020369419494287,
            "SYNRG": 0.018077287090468797,
            "SZRD1": 0.0360336537657683,
            "TAF15": 0.0327034020860868,
            "TANGO6": 0.0155898300627306,
            "TBX2": 0.03638005478163332,
            "TCF12": 0.0497992644587145,
            "TCF20": 0.0315773649909543,
            "TCOF1": 0.009315999464395012,
            "TERF2IP": 0.01999275807124975,
            "TFAM": 0.0341716449098999,
            "TGM2": 0.0275304040264136,
            "TLE2": 0.0156871837667427,
            "TMPO": 0.02590155835447288,
            "TNIP1": 0.0163057182734311,
            "TNRC6A": 0.0455837739848171,
            "TNS1": 0.0317740787112154,
            "TNS3": 0.0218235881321484,
            "TOMM70": 0.030900171772955152,
            "TP53BP1": 0.015914743806578344,
            "TRAPPC8": 0.0341716449098999,
            "TRIM28": 0.03068943571831135,
            "TRIT1": 0.0390932758187843,
            "TSC2": 0.026996410406962615,
            "TSHZ3": 0.0336356206979959,
            "TTF2": 0.0486250708765263,
            "TUBGCP3": 0.0274324904452464,
            "TXLNA": 0.0350854516754513,
            "UBA1": 0.014833098717392055,
            "UBAP2L": 0.0466049147436694,
            "UBE2O": 0.0314950361168889,
            "UCKL1": 0.00207247098461645,
            "UFD1": 0.023825581665335202,
            "UGDH": 0.00518328925662211,
            "UNC45A": 0.0496567595583864,
            "USF2": 0.0491924982363911,
            "USP10": 0.0037774448807903,
            "USP32": 0.0395243155467003,
            "USP47": 0.0138565710361594,
            "USP8": 0.0492742541491687,
            "USP9X": 0.0230727213712009,
            "UTP14A": 0.004274746861318775,
            "VIM": 0.0324706026414712,
            "WIPF2": 0.0156657923191347,
            "WNK1": 0.0219399301026229,
            "XRCC1": 0.00253764928159042,
            "XRCC6": 0.0060231290822709,
            "YEATS2": 0.03394187398361015,
            "ZBTB10": 0.0281384896852693,
            "ZBTB21": 0.00701550687582381,
            "ZBTB9": 0.0123832951383463,
            "ZC3H13": 0.027406113169242498,
            "ZC3H8": 0.0399768129248203,
            "ZC3HC1": 0.0004264386291949855,
            "ZFAND2B": 0.0375334621383179,
            "ZFAND3": 0.00115885398086681,
            "ZFAND5": 0.0198740515186142,
            "ZFC3H1": 0.0115115007567177,
            "ZFP91": 0.0218235881321484,
            "ZMYND11": 0.0270620914209947,
            "ZMYND8": 0.0168133556489441,
            "ZNF318": 0.026252497345200288,
            "ZNF326": 0.0173870040853599,
            "ZNF451": 0.01540701734134332,
            "ZNF592": 0.0202793914528402,
            "ZNF638": 0.0120809958371911,
            "ZNF687": 0.0415452010828718
          }
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nfrom collections import defaultdict\nimport numpy as np\n\n# -------------------------------------------------\n# helper functions\n# -------------------------------------------------\ndef extract_gene_fdr_columns(df):\n    \"\"\"\n    Given a dataframe, try to detect the gene and FDR columns.\n    Returns a dataframe with columns 'Gene' and 'FDR' (may be empty).\n    \"\"\"\n    # Lower-cased column names for searching\n    cols_lower = {c.lower(): c for c in df.columns}\n    \n    # candidate gene columns (first hit wins)\n    gene_col_candidates = [c for c in cols_lower if any(k in c for k in ['gene', 'symbol'])]\n    fdr_col_candidates  = [c for c in cols_lower if any(k in c for k in ['fdr', 'q-value', 'qvalue', 'q_value', 'adj.p'])]\n    \n    if not gene_col_candidates or not fdr_col_candidates:\n        return pd.DataFrame(columns=['Gene', 'FDR'])\n    \n    # pick the first matching column\n    gene_col = cols_lower[gene_col_candidates[0]]\n    fdr_col  = cols_lower[fdr_col_candidates[0]]\n    \n    tmp = df[[gene_col, fdr_col]].copy()\n    tmp.columns = ['Gene', 'FDR']\n    return tmp\n\n\nworkbook_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\nassert workbook_path.exists(), f\"File not found: {workbook_path}\"\n\n# -------------------------------------------------\n# subtask-1: sheet names\n# -------------------------------------------------\nxls = pd.ExcelFile(workbook_path)\nsheet_names = xls.sheet_names\n\n# -------------------------------------------------\n# subtask-2: extract all gene-FDR pairs from every sheet\n# -------------------------------------------------\nlong_df_list = []\nfor sheet in sheet_names:\n    try:\n        # read the entire sheet letting pandas guess header; many supplementary tables\n        df_sheet = pd.read_excel(xls, sheet_name=sheet, header=0)\n    except ValueError:\n        # an empty/broken sheet\n        continue\n    \n    # quick heuristic: if first row looks like continuation of header, reset header\n    if df_sheet.iloc[0].isna().sum() < len(df_sheet.columns)/2:\n        # treat the first row as header if lots of numeric in row 1\n        pass  # keep as is\n    \n    # attempt to find Gene/FDR columns\n    pair_df = extract_gene_fdr_columns(df_sheet)\n    if not pair_df.empty:\n        pair_df['sheet'] = sheet\n        long_df_list.append(pair_df)\n\ngene_fdr_long = pd.concat(long_df_list, ignore_index=True) if long_df_list else pd.DataFrame(columns=['Gene', 'FDR'])\n\n# -------------------------------------------------\n# subtask-3: clean\n# -------------------------------------------------\ngene_fdr_long['Gene'] = gene_fdr_long['Gene'].astype(str).str.strip()\n# coerce FDR to numeric\ngene_fdr_long['FDR'] = pd.to_numeric(gene_fdr_long['FDR'], errors='coerce')\ngene_fdr_long = gene_fdr_long.dropna(subset=['Gene', 'FDR'])\n\n# -------------------------------------------------\n# subtask-4: average FDR per gene\n# -------------------------------------------------\naverage_fdr = gene_fdr_long.groupby('Gene', as_index=False)['FDR'].mean()\n\n# convert to python dictionary {gene: avgFDR}\naverage_fdr_dict = dict(zip(average_fdr['Gene'], average_fdr['FDR']))\n\n# -------------------------------------------------\n# print answers\n# -------------------------------------------------\nprint(json.dumps({\n    \"subtask-1\": sheet_names,\n    \"subtask-2\": f\"Extracted {len(gene_fdr_long)} Gene\u2013FDR rows from {len(long_df_list)} sheets.\",\n    \"subtask-3\": f\"Remaining rows after cleaning: {len(gene_fdr_long)}\",\n    \"subtask-4\": average_fdr_dict,\n    \"main-task\": average_fdr_dict\n}, indent=4))",
        "runtime": 23.671756982803345
      },
      {
        "task_id": "biomedical-easy-9-4",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What is the mean FDR value for CBX3? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet(s) inside 1-s2.0-S0092867420301070-mmc3.xlsx contain a column whose header includes the string \u201cFDR\u201d (case\u2013insensitive)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "F-SS-phospho"
              ]
            },
            {
              "id": "subtask-2",
              "query": "Inside the sheet(s) found in subtask-1, what are the exact column names that correspond to (a) gene identifiers and (b) FDR values?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": {
                "gene_columns": {
                  "F-SS-phospho": "Gene"
                },
                "fdr_columns": {
                  "F-SS-phospho": "FDR.phos"
                }
              }
            },
            {
              "id": "subtask-3",
              "query": "After coercing the FDR column to numeric and dropping missing / non-numeric values, what is the mean FDR for rows whose gene identifier equals \u201cCBX3\u201d (case-sensitive match)?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 0.01684066897229979
            }
          ],
          "answer": 0.01684066897229979
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\n\nxlsx_path = Path(\"/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx\")\n\n# ---------- subtask-1: locate sheets with an \u201cFDR\u201d column ----------\nsheets_with_fdr = []\nwith pd.ExcelFile(xlsx_path) as xls:\n    for sheet_name in xls.sheet_names:\n        df_head = pd.read_excel(xls, sheet_name=sheet_name, nrows=5)  # read small chunk\n        cols = [c for c in df_head.columns if isinstance(c, str)]\n        if any(re.search(r\"fdr\", c, flags=re.I) for c in cols):\n            sheets_with_fdr.append(sheet_name)\n\n# ---------- subtask-2: identify gene & FDR columns in those sheets ----------\ngene_cols = {}   # {sheet: gene_column_name}\nfdr_cols  = {}   # {sheet: fdr_column_name}\n\npossible_gene_patterns = [\"gene\", \"symbol\", \"geneid\", \"gene_id\", \"genes\", \"gene symbol\"]\n\nfor sheet_name in sheets_with_fdr:\n    df_head = pd.read_excel(xlsx_path, sheet_name=sheet_name, nrows=5)\n    for col in df_head.columns:\n        if isinstance(col, str) and re.search(r\"fdr\", col, flags=re.I):\n            fdr_cols[sheet_name] = col\n            break\n    for col in df_head.columns:\n        if isinstance(col, str) and any(re.fullmatch(pat, col.strip().lower()) for pat in possible_gene_patterns):\n            gene_cols[sheet_name] = col\n            break\n    # If we did not find an exact gene match, fall back on the first string-type column\n    if sheet_name not in gene_cols:\n        for col in df_head.columns:\n            if isinstance(col, str):\n                gene_cols[sheet_name] = col\n                break\n\n# ---------- subtask-3: compute mean FDR for CBX3 ----------\ncbx3_fdr_values = []\n\nfor sheet_name in sheets_with_fdr:\n    gene_col = gene_cols.get(sheet_name)\n    fdr_col  = fdr_cols.get(sheet_name)\n    if gene_col is None or fdr_col is None:\n        continue\n    # Read full sheet but only required columns to save memory\n    usecols = [gene_col, fdr_col]\n    df = pd.read_excel(xlsx_path, sheet_name=sheet_name, usecols=usecols)\n    # Standardise column names to simplify processing\n    df = df.rename(columns={gene_col: \"gene\", fdr_col: \"fdr\"})\n    # Coerce FDR to numeric, errors -> NaN, then drop NaN\n    df[\"fdr\"] = pd.to_numeric(df[\"fdr\"], errors=\"coerce\")\n    df = df.dropna(subset=[\"fdr\", \"gene\"])\n    # Keep rows whose gene exactly equals 'CBX3'\n    df_cbx3 = df[df[\"gene\"] == \"CBX3\"]\n    cbx3_fdr_values.extend(df_cbx3[\"fdr\"].tolist())\n\nmean_fdr_cbx3 = float(pd.Series(cbx3_fdr_values).mean()) if cbx3_fdr_values else None\n\nprint(json.dumps(\n    {\n        \"subtask-1\": sheets_with_fdr,\n        \"subtask-2\": {\n            \"gene_columns\": gene_cols,\n            \"fdr_columns\": fdr_cols\n        },\n        \"subtask-3\": mean_fdr_cbx3,\n        \"main-task\": mean_fdr_cbx3     # same value for this particular question\n    },\n    indent=4\n))",
        "runtime": 21.82232165336609
      },
      {
        "task_id": "biomedical-easy-9-5",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What is the average FDR value for genes other than CBX3? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Which sheet in the Excel file contains a column with FDR values?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": "F-SS-phospho"
            },
            {
              "id": "subtask-2",
              "query": "Within that sheet, what are the exact column names that hold (a) the gene symbol/name and (b) the FDR values?",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": {
                "gene_column": "Gene",
                "fdr_column": "FDR.phos"
              }
            },
            {
              "id": "subtask-3",
              "query": "Load the sheet, clean the data (remove rows with missing / non-numeric FDR, coerce FDR to float, standardise gene names to upper-case) and compute the average FDR for all genes EXCEPT those whose symbol is \u201cCBX3\u201d.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 0.02484464434616611
            }
          ],
          "answer": 0.02484464434616611
        },
        "code": "import pandas as pd\nimport numpy as np\nimport json\nfrom pathlib import Path\n\n# -------------------------------------------------\n# Helper variables\n# -------------------------------------------------\nxlsx_path = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx')\n\nanswers = {}\n\n# -------------------------------------------------\n# subtask-1: find sheet containing an FDR column\n# -------------------------------------------------\nsheet_with_fdr = None\nfdr_col_candidates = {}\nxls = pd.ExcelFile(xlsx_path)\n\nfor sheet in xls.sheet_names:\n    try:\n        df_sample = pd.read_excel(xlsx_path, sheet_name=sheet, nrows=10)  # load a few rows\n    except Exception as e:\n        # some sheets might be chart/hidden \u2013 skip\n        continue\n    for col in df_sample.columns:\n        if isinstance(col, str) and 'fdr' in col.lower():\n            sheet_with_fdr = sheet\n            fdr_col_candidates[sheet] = [c for c in df_sample.columns if isinstance(c, str) and 'fdr' in c.lower()]\n            break\n    if sheet_with_fdr is not None:\n        break\n\nanswers[\"subtask-1\"] = sheet_with_fdr\n\n# -------------------------------------------------\n# subtask-2: identify gene & FDR column names\n# -------------------------------------------------\nif sheet_with_fdr is None:\n    raise ValueError(\"No sheet containing 'FDR' column found.\")\n\ndf_head = pd.read_excel(xlsx_path, sheet_name=sheet_with_fdr, nrows=10)\n# Candidate FDR columns already captured\nfdr_columns = fdr_col_candidates[sheet_with_fdr]\n\n# Candidate gene columns: look for 'gene', 'symbol', 'name'\ngene_cols = [c for c in df_head.columns if isinstance(c, str) and any(key in c.lower() for key in ['gene', 'symbol', 'name'])]\n# manually prefer 'gene', then 'gene_name', \u2026\npref_order = ['gene', 'gene_symbol', 'symbol', 'gene name', 'gene_name']\ngene_col = None\nfor pref in pref_order:\n    for c in gene_cols:\n        if pref == c.lower():\n            gene_col = c\n            break\n    if gene_col:\n        break\nif gene_col is None and gene_cols:\n    gene_col = gene_cols[0]\n\nanswers[\"subtask-2\"] = {\"gene_column\": gene_col, \"fdr_column\": fdr_columns[0] if fdr_columns else None}\n\n# -------------------------------------------------\n# subtask-3: compute average FDR for non-CBX3 genes\n# -------------------------------------------------\ndf = pd.read_excel(xlsx_path, sheet_name=sheet_with_fdr)\n\n# Ensure both columns exist\nif gene_col is None or not fdr_columns:\n    raise ValueError(\"Required columns not found.\")\n\nfdr_col = fdr_columns[0]\n\n# Clean: coerce FDR to numeric, upper-case gene symbols\ndf[fdr_col] = pd.to_numeric(df[fdr_col], errors='coerce')\ndf[gene_col] = df[gene_col].astype(str).str.upper()\n\n# Filter non-CBX3\nnon_cbx3 = df[df[gene_col] != 'CBX3']\navg_fdr_non_cbx3 = non_cbx3[fdr_col].mean(skipna=True)\n\nanswers[\"subtask-3\"] = avg_fdr_non_cbx3\n\n# -------------------------------------------------\n# main-task answer\n# -------------------------------------------------\nanswers[\"main-task\"] = avg_fdr_non_cbx3\n\n# Pretty print all answers\nprint(json.dumps(answers, indent=4, default=lambda x: float(x) if isinstance(x, (np.floating,)) else x))",
        "runtime": 30.374333143234253
      },
      {
        "task_id": "biomedical-easy-9-6",
        "model_output": {
          "system_subtasks_responses": [],
          "id": "main-task",
          "query": "Your end goal is to answer this overall question: What is the difference between the average false discovery rate (FDR) in CBX3 genes and the average FDR for the rest of the genes?, please answer the following question:\n What is the difference between the average FDR of CBX3 and the average FDR of the other genes? \n",
          "data_sources": [
            "1-s2.0-S0092867420301070-mmc3.xlsx"
          ],
          "subtasks": [
            {
              "id": "subtask-1",
              "query": "Locate which sheet(s) in 1-s2.0-S0092867420301070-mmc3.xlsx contain both a gene-identifier column and an FDR (false discovery rate) column.",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": [
                "F-SS-phospho"
              ]
            },
            {
              "id": "subtask-2",
              "query": "From the identified sheet(s), extract the rows whose gene symbol is exactly \"CBX3\" (case-insensitive, trimming white-space) and calculate their average FDR (ignoring missing / non-numeric values).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 0.01684066897229979
            },
            {
              "id": "subtask-3",
              "query": "From the same sheet(s), extract the rows for all genes other than \"CBX3\" and calculate their average FDR (ignoring missing / non-numeric values).",
              "data_sources": [
                "1-s2.0-S0092867420301070-mmc3.xlsx"
              ],
              "answer": 0.02484464434616608
            },
            {
              "id": "subtask-4",
              "query": "Compute the difference between the average FDR for \"CBX3\" genes (result of subtask-2) and the average FDR for all other genes (result of subtask-3). Report the difference as CBX3 average \u2212 other-genes average.",
              "data_sources": [],
              "answer": "Warning: No answer found in the Python pipeline."
            }
          ],
          "answer": -0.008003975373866292
        },
        "code": "import json\nimport pandas as pd\nfrom pathlib import Path\nimport re\n\nFILE = Path('/home/gridsan/elai/Kramabench/data/biomedical/input/1-s2.0-S0092867420301070-mmc3.xlsx')\n\n# ---------- subtask-1: locate sheet(s) with gene + FDR ----------\ntarget_sheets = []\nfdr_col_patterns = re.compile(r'(fdr|adj.*p|padj|qval)', re.I)\ngene_col_patterns = re.compile(r'(gene|symbol|hgnc)', re.I)\n\nxls = pd.ExcelFile(FILE)\nfor sheet in xls.sheet_names:\n    try:\n        df_preview = xls.parse(sheet, nrows=5)  # small preview just to inspect columns\n    except Exception:\n        continue\n    cols = [str(c) for c in df_preview.columns]\n    has_fdr = any(bool(fdr_col_patterns.search(c)) for c in cols)\n    has_gene = any(bool(gene_col_patterns.search(c)) for c in cols)\n    if has_fdr and has_gene:\n        target_sheets.append(sheet)\n\n# If nothing found in preview (because first 5 rows are weird), load full sheets to double-check\nif not target_sheets:\n    for sheet in xls.sheet_names:\n        try:\n            df_full = xls.parse(sheet)\n        except Exception:\n            continue\n        cols = [str(c) for c in df_full.columns]\n        has_fdr = any(bool(fdr_col_patterns.search(c)) for c in cols)\n        has_gene = any(bool(gene_col_patterns.search(c)) for c in cols)\n        if has_fdr and has_gene:\n            target_sheets.append(sheet)\n\n# ---------- subtask-2 & 3: compute averages ----------\ncbx3_values = []\nother_values = []\n\nfor sheet in target_sheets:\n    df = xls.parse(sheet)\n    # Identify column names\n    fdr_cols = [c for c in df.columns if fdr_col_patterns.search(str(c))]\n    gene_cols = [c for c in df.columns if gene_col_patterns.search(str(c))]\n    if not fdr_cols or not gene_cols:\n        continue\n    fdr_col = fdr_cols[0]          # take the first match\n    gene_col = gene_cols[0]\n    \n    # Ensure numeric FDR column\n    df[fdr_col] = pd.to_numeric(df[fdr_col], errors='coerce')\n    \n    # Standardise gene names\n    df[gene_col] = df[gene_col].astype(str).str.strip().str.upper()\n    \n    cbx3_mask = df[gene_col] == 'CBX3'\n    cbx3_values.extend(df.loc[cbx3_mask, fdr_col].dropna().tolist())\n    other_values.extend(df.loc[~cbx3_mask, fdr_col].dropna().tolist())\n\navg_cbx3 = sum(cbx3_values) / len(cbx3_values) if cbx3_values else None\navg_other = sum(other_values) / len(other_values) if other_values else None\n\n# ---------- subtask-4: difference ----------\ndifference = None\nif avg_cbx3 is not None and avg_other is not None:\n    difference = avg_cbx3 - avg_other\n\n# ---------- print results ----------\nprint(json.dumps({\n    \"subtask-1\": target_sheets,\n    \"subtask-2\": avg_cbx3,\n    \"subtask-3\": avg_other,\n    \"main-task\": difference\n}, indent=4))",
        "runtime": 26.275064945220947
      }
    ],
    "runtime": 17.45099639892578
  }
]